
\section{Intro first draft}

The field of Natural Language Processing (NLP), which is concerned with the computational analysis, generation and processing of natural language, has undergone several paradigms shifts during its history. The current paradigm is dominated by the use of \textbf{Deep Neural Networks} (DNNs) to learn general-purpose representations of language that transfer well to downstream tasks. The representations are learned using self-supervised tasks that allow the neural network to learn \textbf{contextual embeddings} for language split into discrete \textbf{tokens}. This learning process is called \textbf{pre-training} and the resulting networks are referred to as \textbf{Pre-trained Language Models}, or often simply as \textbf{Language Models} (LMs). 

This pre-training process is almost entirely end-to-end, except for the very first step: the conversion of language into discrete tokens, which in this thesis is referred to as the \textbf{input representation}. In some of the earliest examples of neural networks being used to study the sequential properties of language, these tokens may have consisted of individual characters or even \textbf{phonemes}, allowing researchers to theorise about human language learning and processing. The input representation now used to train LMs is not motivated by linguistics or cognition, but by efficiency. A major benefit of the pre-training process is that it is far more scalable than previous methods in NLP, not requiring explicitly labelled data. Now, the largest of these networks, known as \textbf{Large Language Models} (LLMs) consist of billions of parameters, are trained on trillions of words, and are remarkably powerful across a broad range of tasks and applications. As a result of this success, the NLP has converged on an input representation that make this scaling feasible. As the easiest means of sourcing trillions of words of data is web-scraping \addcites, tokens predominantly represent \textbf{orthographic} text. Instead of characters or words, the discrete unit used to split this data is the \textbf{subword}, an arbitrary unit which typically lies between between characters and words in length but is not motivated by morphology. Instead, subword tokenisers such as BPE seem to seek a balance between many (often competing) technical constraints, such as compression rate, vocabulary size and the handling of out-of-vocabulary (OOV) items.

This \textbf{subword-based input representation} has become ubiquitous in NLP and although many alternatives to popular subword tokenisation algorithms have been proposed, there have been relatively few studies challenging this ubiquity. One alternative representation consists of individual \emph{phonemes}, the perceptually distinct units of sound in a specified language used to distinguish words in speech. Instead of orthographic subwords, the \textbf{phonemic input representation} consists of individual phonemes, often without word boundaries, as these are not easily available in speech. Although a few studies have explored specific practical and analytical benefits of using this representation to train language models (see \cref{sec:12-phoneval} for a review), phoneme-level modelling is still vastly under-studied. There is a clear need for a thorough examination of this representation in order to identify possible resource constraints %that could be preventing work in this area,
and to establish the feasibility of using the phonemic input representation to train modern LM architectures.% through a comparison with the standard input representation.

%there are limited insights that can be gained when relating these models to phonology (as the representation is orthographic) or human language learning. %Phoneme-level modelling is vastly under-studied, despite the potential benefits for phonological analysis, alternative tokenisation, multilingual modelling, and low-resource modelling. 

The phoneme input representation could also have significant implications for research using LMs to investigate the distributional properties of language, updating legacy `connectionist' models (which often did use phonemes) with much more powerful architectures. In the current paradigm, LMs are frequently compared to human learners; however, this comparison is complicated by the fact that models are typically trained on vastly more linguistic data than humans encounter over their lifetimes \citep{huebner-etal-2021-babyberta}. This concern has motivated the \emph{BabyLM challenge} \citep{warstadt-2023-babylm-findings,conll-2024-babylm}, where so called `BabyLMs' are pre-trained using corpora whose scale and source are considered ``developmentally plausible''. Despite the aim to improve comparisons between language models and human learners, the default input representation has become so entrenched that it is not even challenged here, despite the divergence from the auditory input available to human infants. Reevaluating the input modality is thus a crucial step toward more realistic comparisons with human language acquisition.\zeb{Potentially needs rephrasing to be more targeted towards general study of phonology as done in \cref{chapter:phonology}}

%The choice of input representation—such as phonemes—has significant implications for research using language models (LMs) to investigate the distributional properties of language, particularly as modern architectures build on the legacy of early connectionist models. In the current landscape, LMs are frequently compared to human learners; however, this comparison is complicated by the fact that models are typically trained on vastly more linguistic data than humans encounter over their lifetimes \citep{huebner-etal-2021-babyberta}. This concern has motivated the \emph{BabyLM Challenge} \citep{warstadt-2023-babylm-findings,findings,conll-2024-babylm}, which promotes the development of LMs trained on data that is both limited in scale and developmentally plausible. Despite this promising direction, most BabyLMs still rely on standard input formats—typically text segmented into subword units—even though such representations diverge significantly from the auditory, phonemic input available to human infants. Reevaluating the input modality is thus a crucial step toward more realistic models of human language acquisition.

% Particularly easy to scale compared to past NLP methods since only unlabelled text is required. Instead,  Benefiting from the vast quantities of text available online through procedures like web-scraping, state-of-the-art has been dominated by scaling this pre-training step as much as possible, using particularly large networks trained on especially massive corpora, aptly-named Large Language Models. 


%Standard NLP pipeline uses large data, orthographic, scraped from the web, with subword tokenisation for efficiency and downstream performance. (+high level explanation here) Remarkable power and almost entirely end-to-end, save for a crucial component: the tokeniser, which is often trained in advance and uses a subword representation motivated entirely by efficiency, rather than being motivated by cognition or linguistic boundaries. Call this the ``standard input representation''.

% Possibly for background instead: Now, pre-training is so expensive that very large models are only trained once, and later fine-tuned for specific tasks, however since the tokeniser is `locked-in', this often limits out-of-domain fine-tuning due to the subword vocabulary badly representing the new task. In some cases, pre-training a smaller network is more effective and in fact less computationally expensive than fine-tuning a large network.

%The earliest `LMs' (then called "connectionist" networks) used a completely different input representation - often individual phonemes or characters, in order to best model the brain's processing of language by choosing appropriate units for modelling. These networks were not built to handle noisy data and so `tokeniser' may not be the correct term. Modern LLMs continue to be a useful tool for linguistic analysis. In response to the astounding performance of transformer-based architectures on downstream tasks, one areas of research investigated the linguistic capabilities of these models....

%However, when trained on internet-scraped written text using a subword-based tokeniser, there are limited insights that can be gained when relating these models to phonology (as the representation is orthographic) or human language learning. In recent years, the unrealistic scale of the pre-training data used in the standard NLP pipeline has led to an alternative direction of research, where models trained on developmentally plausible data (not just in scale, but also in domain) have led to new insights... BabyLM challenge... particularly as these experiments are feasible with academic budgets.

%Despite the focus on developmentally plausible data in the BabyLM research community, the subword input representation is so entrenched that the vast majority of entries continued to use subword tokenisers. This is despite the fact that subword tokenisation is not cognitively plausible...etc...

%Phoneme-level modelling is vastly under-studied, despite the potential benefits for phonological analysis, alternative tokenisation, multilingual modelling, and low-resource modelling. 

The focus of this thesis is to establish the benefits and insights that can be gained from using a phonemic input representation to train modern NLP architectures, particularly when considering developmentally plausible training data. First, finding a lack of suitable tools and datasets for phonemic language modelling, I develop and release two open-source resources which also support phonological analysis more broadly. I then carry out a wide range of experiments leveraging these resources in order to assert the feasibility of the phonemic input representation for training transformer-based architectures. These experiments also establish how phonological and syntactic capabilities scale with data and model size. Using these findings and resources, I demonstrate how the phonemic input representation supports phonological analysis by training LMs on child-directed speech across 31 languages and probing the trained models for phonological features. Using a word segmentation task drawn from the acquisition literature, I find that these models implicitly track word boundaries, despite word boundaries not appearing during training. Finally, noting the parallels between the word segmentation task and bottom-up subword tokenisers, I propose a novel method for learning token merges and find that \writemore


\Zeb{maybe end with something like... demonstrating that research into cognitively-motivated modelling can lead to new improvements even in industry-scale use of these models...}
