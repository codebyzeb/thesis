@inproceedings{8683536,
  author    = {Black, Alan W},
  booktitle = {ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {CMU Wilderness Multilingual Speech Dataset},
  year      = {2019},
  volume    = {},
  number    = {},
  pages     = {5971-5975},
  keywords  = {Speech synthesis;Data models;Synthesizers;Speech recognition;Acoustic measurements;found speech data;multilingual;speech synthesis;speech recognition},
  doi       = {10.1109/ICASSP.2019.8683536}
}

@inproceedings{9383459,
  author    = {Valk, Jörgen and Alumäe, Tanel},
  booktitle = {2021 IEEE Spoken Language Technology Workshop (SLT)},
  title     = {VOXLINGUA107: A Dataset for Spoken Language Recognition},
  year      = {2021},
  volume    = {},
  number    = {},
  pages     = {652-658},
  keywords  = {Training;Training data;Speech recognition;Feature extraction;Data models;Task analysis;Videos;Spoken language recognition;web scraping;x-vectors;crowd-sourcing},
  doi       = {10.1109/SLT48900.2021.9383459}
}

@inproceedings{ahn-chodroff-2022-voxcommunis,
  title     = {{V}ox{C}ommunis: A Corpus for Cross-linguistic Phonetic Analysis},
  author    = {Ahn, Emily  and
               Chodroff, Eleanor},
  editor    = {Calzolari, Nicoletta  and
               B{\'e}chet, Fr{\'e}d{\'e}ric  and
               Blache, Philippe  and
               Choukri, Khalid  and
               Cieri, Christopher  and
               Declerck, Thierry  and
               Goggi, Sara  and
               Isahara, Hitoshi  and
               Maegaard, Bente  and
               Mariani, Joseph  and
               Mazo, H{\'e}l{\`e}ne  and
               Odijk, Jan  and
               Piperidis, Stelios},
  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},
  month     = jun,
  year      = {2022},
  address   = {Marseille, France},
  publisher = {European Language Resources Association},
  url       = {https://aclanthology.org/2022.lrec-1.566/},
  pages     = {5286--5294},
  abstract  = {Cross-linguistic phonetic analysis has long been limited by data scarcity and insufficient computational resources. In the past few years, the availability of large-scale cross-linguistic spoken corpora has increased dramatically, but the data still require considerable computational power and processing for downstream phonetic analysis. To facilitate large-scale cross-linguistic phonetic research in the field, we release the VoxCommunis Corpus, which contains acoustic models, pronunciation lexicons, and word- and phone-level alignments, derived from the publicly available Mozilla Common Voice Corpus. The current release includes data from 36 languages. The corpus also contains acoustic-phonetic measurements, which currently consist of formant frequencies (F1{--}F4) from all vowel quartiles. Major advantages of this corpus for phonetic analysis include the number of available languages, the large amount of speech per language, as well as the fact that most language datasets have dozens to hundreds of contributing speakers. We demonstrate the utility of this corpus for downstream phonetic research in a descriptive analysis of language-specific vowel systems, as well as an analysis of {\textquotedblleft}uniformity{\textquotedblright} in vowel realization across languages. The VoxCommunis Corpus is free to download and use under a CC0 license.}
}

@article{al-rfou_character-level_2019,
  title   = {Character-level Language Modeling with Deeper Self-Attention},
  volume  = {33},
  url     = {https://ojs.aaai.org/index.php/AAAI/article/view/4182},
  doi     = {10.1609/aaai.v33i01.33013159},
  number  = {01},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author  = {Al-Rfou, Rami and Choe, Dokook and Constant, Noah and Guo, Mandy and Jones, Llion},
  month   = jul,
  year    = {2019},
  pages   = {3159--3166}
}

@article{algayres_dp-parse_2022,
  title   = {{DP}-{Parse}: {Finding} {Word} {Boundaries} from {Raw} {Speech} with an {Instance} {Lexicon}},
  volume  = {10},
  issn    = {2307-387X},
  url     = {https://doi.org/10.1162/tacl\_a\_00505},
  doi     = {10.1162/tacl_a_00505},
  journal = {Transactions of the Association for Computational Linguistics},
  author  = {Algayres, Robin and Ricoul, Tristan and Karadayi, Julien and Laurençon, Hugo and Zaiem, Salah and Mohamed, Abdelrahman and Sagot, Benoît and Dupoux, Emmanuel},
  month   = sep,
  year    = {2022},
  pages   = {1051--1065}
}

@inproceedings{ardila-etal-2020-common,
  title     = {Common Voice: A Massively-Multilingual Speech Corpus},
  author    = {Ardila, Rosana  and
               Branson, Megan  and
               Davis, Kelly  and
               Kohler, Michael  and
               Meyer, Josh  and
               Henretty, Michael  and
               Morais, Reuben  and
               Saunders, Lindsay  and
               Tyers, Francis  and
               Weber, Gregor},
  editor    = {Calzolari, Nicoletta  and
               B{\'e}chet, Fr{\'e}d{\'e}ric  and
               Blache, Philippe  and
               Choukri, Khalid  and
               Cieri, Christopher  and
               Declerck, Thierry  and
               Goggi, Sara  and
               Isahara, Hitoshi  and
               Maegaard, Bente  and
               Mariani, Joseph  and
               Mazo, H{\'e}l{\`e}ne  and
               Moreno, Asuncion  and
               Odijk, Jan  and
               Piperidis, Stelios},
  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
  month     = may,
  year      = {2020},
  address   = {Marseille, France},
  publisher = {European Language Resources Association},
  url       = {https://aclanthology.org/2020.lrec-1.520},
  pages     = {4218--4222},
  abstract  = {The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identification). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla{'}s DeepSpeech Speech-to-Text toolkit. By applying transfer learning from a source English model, we find an average Character Error Rate improvement of 5.99 {\mbox{$\pm$}} 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the first ever published results on end-to-end Automatic Speech Recognition.},
  language  = {English},
  isbn      = {979-10-95546-34-4}
}

 @incollection{aslin1996models,
  title     = {Models of word segmentation in fluent maternal speech to infants.},
  author    = {Aslin, Richard N and Woodward, Julide Z and LaMendola, Nicholas P and Bever, Thomas G},
  booktitle = {Signal to syntax},
  year      = {1996},
  pages     = {117--134},
  publisher = {Psychology Press}
}

@article{aslin1998computation,
  title     = {Computation of conditional probability statistics by 8-month-old infants},
  author    = {Aslin, Richard N and Saffran, Jenny R and Newport, Elissa L},
  journal   = {Psychological Science},
  volume    = {9},
  number    = {4},
  pages     = {321--324},
  year      = {1998},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA}
}

@inproceedings{bansal-2022-datascaling,
  title        = {Data scaling laws in {NMT}: The effect of noise and architecture},
  author       = {Bansal, Yamini and Ghorbani, Behrooz and Garg, Ankush and Zhang, Biao and Cherry, Colin and Neyshabur, Behnam and Firat, Orhan},
  booktitle    = {International Conference on Machine Learning},
  pages        = {1466--1482},
  year         = {2022},
  organization = {PMLR}
}

@incollection{baroni-2022-proper,
  title     = {On the proper role of linguistically oriented deep net analysis in linguistic theorising},
  author    = {Baroni, Marco},
  booktitle = {Algebraic structures in natural language},
  pages     = {1--16},
  year      = {2022},
  publisher = {CRC Press}
}

@inproceedings{batsuren-etal-2022-sigmorphon,
  title     = {The {SIGMORPHON} 2022 Shared Task on Morpheme Segmentation},
  author    = {Batsuren, Khuyagbaatar  and
               Bella, G{\'a}bor  and
               Arora, Aryaman  and
               Martinovic, Viktor  and
               Gorman, Kyle  and
               {\v{Z}}abokrtsk{\'y}, Zden{\v{e}}k  and
               Ganbold, Amarsanaa  and
               Dohnalov{\'a}, {\v{S}}{\'a}rka  and
               {\v{S}}ev{\v{c}}{\'\i}kov{\'a}, Magda  and
               Pelegrinov{\'a}, Kate{\v{r}}ina  and
               Giunchiglia, Fausto  and
               Cotterell, Ryan  and
               Vylomova, Ekaterina},
  editor    = {Nicolai, Garrett  and
               Chodroff, Eleanor},
  booktitle = {Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology},
  month     = jul,
  year      = {2022},
  address   = {Seattle, Washington},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.sigmorphon-1.11},
  doi       = {10.18653/v1/2022.sigmorphon-1.11},
  pages     = {103--116}
}

@inproceedings{beinborn-pinter-2023-analyzing,
  title     = {Analyzing Cognitive Plausibility of Subword Tokenization},
  author    = {Beinborn, Lisa  and
               Pinter, Yuval},
  editor    = {Bouamor, Houda  and
               Pino, Juan  and
               Bali, Kalika},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  month     = dec,
  year      = {2023},
  address   = {Singapore},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.emnlp-main.272/},
  doi       = {10.18653/v1/2023.emnlp-main.272},
  pages     = {4478--4486},
  abstract  = {Subword tokenization has become the de-facto standard for tokenization although comparative evaluations of their quality across languages are scarce. Existing evaluation studies focus on the effect of a tokenization algorithm on the performance in downstream tasks, or on engineering criteria such as the compression rate. We present a new evaluation paradigm that focuses on the cognitive plausibility of subword tokenization. We analyze the correlation of the tokenizer output with the reading time and accuracy of human responses on a lexical decision task. We compare three tokenization algorithms across several languages and vocabulary sizes. Our results indicate that the Unigram algorithm yields less cognitively plausible tokenization behavior and a worse coverage of derivational morphemes, in contrast with prior work.}
}

@book{beinborn2024cognitive,
  title     = {Cognitive plausibility in natural language processing},
  author    = {Beinborn, Lisa and Hollenstein, Nora},
  year      = {2024},
  publisher = {Springer}
}


@article{bergelson-etal-2023,
  author  = {Elika Bergelson  and Melanie Soderstrom  and Iris-Corinna Schwarz  and Caroline F. Rowland  and Nairán Ramírez-Esparza  and Lisa R. Hamrick  and Ellen Marklund  and Marina Kalashnikova  and Ava Guez  and Marisa Casillas  and Lucia Benetti  and Petra van Alphen  and Alejandrina Cristia },
  title   = {Everyday language input and production in 1,001 children from six continents},
  journal = {Proceedings of the National Academy of Sciences},
  volume  = {120},
  number  = {52},
  pages   = {e2300671120},
  year    = {2023},
  doi     = {10.1073/pnas.2300671120},
  url     = {https://www.pnas.org/doi/abs/10.1073/pnas.2300671120},
  eprint  = {https://www.pnas.org/doi/pdf/10.1073/pnas.2300671120}
}

@article{Bernard2021,
  doi       = {10.21105/joss.03958},
  url       = {https://doi.org/10.21105/joss.03958},
  year      = {2021},
  publisher = {The Open Journal},
  volume    = {6},
  number    = {68},
  pages     = {3958},
  author    = {Mathieu Bernard and Hadrien Titeux},
  title     = {Phonemizer: Text to Phones Transcription for Multiple Languages in Python},
  journal   = {Journal of Open Source Software}
}

@article{bernstein_ratner_augmenting_2024,
  title      = {Augmenting {Clinical} {Insights} with {Computing}: {How} {TalkBank} has {Impacted} {Assessment} and {Treatment} of {Speech} and {Language} {Disorders}},
  volume     = {44},
  issn       = {2667-6753},
  shorttitle = {Augmenting {Clinical} {Insights} with {Computing}},
  url        = {https://www.eurokd.com/doi/10.32038/ltrq.2024.44.05},
  doi        = {10.32038/ltrq.2024.44.05},
  abstract   = {Our purpose is to highlight the contributions of TalkBank initiatives to improved understanding of clinical impairments in adult and child speakers and examine remaining challenges and proposed solutions.We review the origins and development of TalkBank initiatives that have targeted a wide array of typical and atypical child and adult populations. In particular, we discuss how such sets of data have given rise to evaluation and validation of traditional measures used to appraise spoken language performance. The durable contributions of AphasiaBank and CHILDES archives are already evident in a body of published research that has re-evaluated, refined and reconceptualized how we evaluate and set therapeutic goals for speakers with expressive speech and language impairments. More recent archival initiatives, such as PhonBank and FluencyBank, are also making impacts. Beyond improvements in basic and applied science in communication development and disorders, archival data are also being used to test and improve accessibility for communicatively impaired speakers. TalkBank has transformed how research in communication disorders is conducted. It no longer relies on small, unshared research ventures that enable limited clinical impact or follow-up research inquiries. Rather, it has enabled largescale, more generalizable research more likely to spur further research and enable more rapid translation to clinical practice.},
  language   = {en},
  urldate    = {2024-11-18},
  journal    = {Language Teaching Research Quarterly},
  author     = {Bernstein Ratner, Nan},
  month      = oct,
  year       = {2024},
  pages      = {31--40},
  file       = {Bernstein Ratner - 2024 - Augmenting Clinical Insights with Computing How T.pdf:/Users/zebulongoriely/Zotero/storage/MEVBB9WN/Bernstein Ratner - 2024 - Augmenting Clinical Insights with Computing How T.pdf:application/pdf}
}

@article{bernstein1987phonology,
  title     = {The Phonology of Parent-Child Speech},
  author    = {Bernstein-Ratner, Nan},
  journal   = {Children's Language},
  volume    = {6},
  pages     = {159},
  year      = {1987},
  publisher = {Psychology Press}
}

@inproceedings{biderman2023pythia,
  title     = {Pythia: A suite for analyzing large language models across training and scaling},
  author    = {Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O’Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  booktitle = {International Conference on Machine Learning},
  pages     = {2397--2430},
  year      = {2023}
}


@article{bisani-2008-g2p,
  title     = {Joint-sequence models for grapheme-to-phoneme conversion},
  author    = {Bisani, Maximilian and Ney, Hermann},
  journal   = {Speech communication},
  volume    = {50},
  number    = {5},
  pages     = {434--451},
  year      = {2008},
  publisher = {Elsevier}
}


@inproceedings{black2001flite,
  title     = {Flite: a small fast run-time synthesis engine},
  author    = {Black, Alan W and Lenzo, Kevin A},
  booktitle = {4th ISCA Tutorial and Research Workshop (ITRW) on Speech Synthesis},
  year      = {2001}
}


@article{Blanchard2010,
  author    = {Blanchard, Daniel and Heinz, Jeffrey and Golinkoff, Roberta},
  doi       = {10.1017/S030500090999050X},
  issn      = {03050009},
  journal   = {Journal of Child Language},
  number    = {3},
  pages     = {487--511},
  pmid      = {20307346},
  publisher = {Cambridge University Press},
  title     = {{Modeling the contribution of phonotactic cues to the problem of word segmentation}},
  volume    = {37},
  year      = {2010}
}

@misc{bnc2007,
  title     = {British National Corpus, {XML} edition},
  author    = {{BNC} Consortium},
  url       = {http://hdl.handle.net/20.500.14106/2554},
  note      = {Literary and Linguistic Data Service},
  copyright = {Distributed by the University of Oxford under the {BNC} User Licence. Clicking to download implies acceptance of the licence conditions.},
  year      = {2007}
}

@inproceedings{borschinger-etal-2013-joint,
  title     = {A joint model of word segmentation and phonological variation for {E}nglish word-final /t/-deletion},
  author    = {B{\"o}rschinger, Benjamin  and
               Johnson, Mark  and
               Demuth, Katherine},
  editor    = {Schuetze, Hinrich  and
               Fung, Pascale  and
               Poesio, Massimo},
  booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = aug,
  year      = {2013},
  address   = {Sofia, Bulgaria},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/P13-1148},
  pages     = {1508--1516}
}

@inproceedings{borschinger2013joint,
  title     = {A joint model of word segmentation and phonological variation for English word-final/t/-deletion},
  author    = {B{\"o}rschinger, Benjamin and Johnson, Mark and Demuth, Katherine},
  booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages     = {1508--1516},
  year      = {2013}
}

@article{Brent1999,
  archiveprefix = {arXiv},
  arxivid       = {cs/9905007},
  author        = {Brent, Michael R.},
  doi           = {10.1023/a:1007541817488},
  eprint        = {9905007},
  issn          = {08856125},
  journal       = {Machine Learning},
  keywords      = {bayesian grammar induction,language acquisition,minimum description length (MDL),probability models,segmentation,unsupervised learning},
  number        = {1},
  pages         = {71--105},
  primaryclass  = {cs},
  publisher     = {Kluwer Academic Publishers},
  title         = {{Efficient, probabilistically sound algorithm for segmentation and word discovery}},
  url           = {https://link.springer.com/article/10.1023/A:1007541817488},
  volume        = {34},
  year          = {1999}
}

@article{brown-2020-gpt3,
  title   = {Language models are few-shot learners},
  author  = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {33},
  pages   = {1877--1901},
  year    = {2020}
}

@inproceedings{bunzeck-etal-2025-small,
  title     = {Small Language Models Also Work With Small Vocabularies: Probing the Linguistic Abilities of Grapheme- and Phoneme-Based Baby Llamas},
  author    = {Bunzeck, Bastian  and
               Duran, Daniel  and
               Schade, Leonie  and
               Zarrie{\ss}, Sina},
  editor    = {Rambow, Owen  and
               Wanner, Leo  and
               Apidianaki, Marianna  and
               Al-Khalifa, Hend  and
               Eugenio, Barbara Di  and
               Schockaert, Steven},
  booktitle = {Proceedings of the 31st International Conference on Computational Linguistics},
  month     = jan,
  year      = {2025},
  address   = {Abu Dhabi, UAE},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2025.coling-main.404/},
  pages     = {6039--6048},
  abstract  = {Recent work investigates whether LMs learn human-like linguistic generalizations and representations from developmentally plausible amounts of data. Yet, the basic linguistic units processed in these LMs are determined by subword-based tokenization, which limits their validity as models of learning at and below the word level. In this paper, we explore the potential of tokenization-free, phoneme- and grapheme-based language models. We demonstrate that small models based on the Llama architecture can achieve strong linguistic performance on standard syntactic and novel lexical/phonetic benchmarks when trained with character-level vocabularies. We further show that phoneme-based models almost match grapheme-based models in standard tasks and novel evaluations. Our findings suggest a promising direction for creating more linguistically plausible language models that are better suited for computational studies of language acquisition and processing.}
}

@inproceedings{bunzeck2024graphemes,
  title     = {Graphemes vs. phonemes: Battling it out in character-based language models},
  author    = {Bunzeck, Bastian and Duran, Daniel and Schade, Leonie and Zarrie{\ss}, Sina},
  booktitle = {The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning},
  pages     = {54--64},
  year      = {2024}
}

@misc{bunzeck2025subwordmodelsstruggleword,
  title         = {Subword models struggle with word learning, but surprisal hides it},
  author        = {Bastian Bunzeck and Sina Zarrieß},
  year          = {2025},
  eprint        = {2502.12835},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2502.12835}
}

@article{caines2019cross,
  title     = {The cross-linguistic performance of word segmentation models over time},
  author    = {Caines, Andrew and Altmann-Richer, Emma and Buttery, Paula},
  journal   = {Journal of child language},
  volume    = {46},
  number    = {6},
  pages     = {1169--1201},
  year      = {2019},
  publisher = {Cambridge University Press}
}

@inproceedings{capone2024babies,
  title        = {BaBIEs: A Benchmark for the Linguistic Evaluation of Italian Baby Language Models},
  author       = {Capone, Luca and Suozzi, Alice and Lebani, Gianluca E and Lenci, Alessandro and others},
  booktitle    = {CEUR WORKSHOP PROCEEDINGS},
  year         = {2024},
  organization = {CEUR-WS}
}
 
@article{caucheteux-2023-predictive-coding,
  title     = {Evidence of a predictive coding hierarchy in the human brain listening to speech},
  author    = {Caucheteux, Charlotte and Gramfort, Alexandre and King, Jean-R{\'e}mi},
  journal   = {Nature human behaviour},
  volume    = {7},
  number    = {3},
  pages     = {430--441},
  year      = {2023},
  publisher = {Nature Publishing Group UK London}
}
@inproceedings{ccoltekin2014explicit,
  title     = {An explicit statistical model of learning lexical segmentation using multiple cues},
  author    = {{\c{C}}{\"o}ltekin, {\c{C}}a{\u{g}}r{\i} and Nerbonne, John},
  booktitle = {Proceedings of the 5th Workshop on Cognitive Aspects of Computational Language Learning (CogACLL)},
  pages     = {19--28},
  year      = {2014}
}

@article{chan2015listen,
  title   = {Listen, attend and spell},
  author  = {Chan, William and Jaitly, Navdeep and Le, Quoc V and Vinyals, Oriol},
  journal = {arXiv preprint arXiv:1508.01211},
  year    = {2015}
}

@inproceedings{charpentier-samuel-2023-layers,
  title     = {Not all layers are equally as important: Every Layer Counts {BERT}},
  author    = {Charpentier, Lucas Georges Gabriel and
               Samuel, David},
  editor    = {Warstadt, Alex  and
               Mueller, Aaron  and
               Choshen, Leshem  and
               Wilcox, Ethan  and
               Zhuang, Chengxu  and
               Ciro, Juan  and
               Mosquera, Rafael  and
               Paranjabe, Bhargavi  and
               Williams, Adina  and
               Linzen, Tal  and
               Cotterell, Ryan},
  booktitle = {Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning},
  month     = dec,
  year      = {2023},
  address   = {Singapore},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.conll-babylm.20},
  doi       = {10.18653/v1/2023.conll-babylm.20},
  pages     = {238--252}
}

@article{charpentier2024gpt,
  title   = {GPT or BERT: why not both?},
  author  = {Charpentier, Lucas Georges Gabriel and Samuel, David},
  journal = {arXiv preprint arXiv:2410.24159},
  year    = {2024}
}

@article{chowdhery-2023-palm,
  title   = {Palm: Scaling language modeling with pathways},
  author  = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal = {Journal of Machine Learning Research},
  volume  = {24},
  number  = {240},
  pages   = {1--113},
  year    = {2023}
}

@article{christiansen1998learning,
  title     = {Learning to segment speech using multiple cues: A connectionist model},
  author    = {Christiansen, Morten H and Allen, Joseph and Seidenberg, Mark S},
  journal   = {Language and Cognitive Processes},
  volume    = {13},
  number    = {2-3},
  pages     = {221--268},
  year      = {1998},
  publisher = {Taylor \& Francis}
}

@inproceedings{cieri2004fisher,
  title     = {The Fisher corpus: A resource for the next generations of speech-to-text.},
  author    = {Cieri, Christopher and Miller, David and Walker, Kevin},
  booktitle = {LREC},
  volume    = {4},
  pages     = {69--71},
  year      = {2004}
}

@book{clark-2009-first,
  title     = {First Language Acquisition},
  author    = {Clark, Eve V},
  year      = {2009},
  publisher = {Cambridge University Press},
  pages     = {21--50}
}

@article{clark-etal-2022-canine,
  title     = {Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation},
  author    = {Clark, Jonathan H.  and
               Garrette, Dan  and
               Turc, Iulia  and
               Wieting, John},
  editor    = {Roark, Brian  and
               Nenkova, Ani},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {10},
  year      = {2022},
  address   = {Cambridge, MA},
  publisher = {MIT Press},
  url       = {https://aclanthology.org/2022.tacl-1.5},
  doi       = {10.1162/tacl_a_00448},
  pages     = {73--91},
  abstract  = {Pipelined NLP systems have largely been superseded by end-to-end neural modeling, yet nearly all commonly used models still require an explicit tokenization step. While recent tokenization approaches based on data-derived subword lexicons are less brittle than manually engineered tokenizers, these techniques are not equally suited to all languages, and the use of any fixed vocabulary may limit a model{'}s ability to adapt. In this paper, we present Canine, a neural encoder that operates directly on character sequences{---}without explicit tokenization or vocabulary{---}and a pre-training strategy that operates either directly on characters or optionally uses subwords as a soft inductive bias. To use its finer-grained input effectively and efficiently, Canine combines downsampling, which reduces the input sequence length, with a deep transformer stack, which encodes context. Canine outperforms a comparable mBert model by 5.7 F1 on TyDi QA, a challenging multilingual benchmark, despite having fewer model parameters.}
}

@article{cole1980model,
  title   = {A model of speech perception},
  author  = {Cole, Ronald A and Jakimik, Jola},
  journal = {Perception and production of fluent speech},
  volume  = {133},
  number  = {64},
  pages   = {133--42},
  year    = {1980}
}

@article{coleman2011mining,
  title   = {Mining a year of speech},
  author  = {Coleman, John and Liberman, Mark and Kochanski, Greg and Burnard, Lou and Yuan, Jiahong},
  journal = {VLSP 2011: New tools and methods for very-large-scale phonetics research},
  pages   = {16--19},
  year    = {2011}
}

@article{coleman2011miningb,
  title   = {Mining years and years of speech},
  author  = {Coleman, John and Liberman, Mark and Kochanski, Greg and Yuan, Jiahong and Grau, Sergio and Cieri, Chris and Burnard, Lou},
  journal = {Phonetics Laboratory of the University of Oxford},
  pages   = {1--23},
  year    = {2011}
}

@article{coleman2012audio,
  title   = {Audio BNC: the audio edition of the Spoken British National Corpus},
  author  = {Coleman, John and Baghai-Ravary, Ladan and Pybus, John and Grau, Sergio},
  journal = {Phonetics Laboratory, University of Oxford},
  year    = {2012}
}

@article{Coltekin2017,
  author    = {{\c{C}}{\"{o}}ltekin, {\c{C}}ağrı},
  doi       = {10.1111/cogs.12454},
  issn      = {03640213},
  journal   = {Cognitive Science},
  keywords  = {Computational modeling,Language acquisition,Predictability entropy,Word segmentation},
  month     = {sep},
  number    = {7},
  pages     = {1988--2021},
  publisher = {Wiley-Blackwell Publishing},
  title     = {{Using Predictability for Lexical Segmentation}},
  url       = {http://doi.wiley.com/10.1111/cogs.12454},
  volume    = {41},
  year      = {2017}
}

@article{coltheart2001drc,
  title     = {DRC: a dual route cascaded model of visual word recognition and reading aloud.},
  author    = {Coltheart, Max and Rastle, Kathleen and Perry, Conrad and Langdon, Robyn and Ziegler, Johannes},
  journal   = {Psychological review},
  volume    = {108},
  number    = {1},
  pages     = {204},
  year      = {2001},
  publisher = {American Psychological Association}
}

@proceedings{conll-2024-babylm,
  title     = {The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning},
  editor    = {Hu, Michael Y.  and
               Mueller, Aaron  and
               Ross, Candace  and
               Williams, Adina  and
               Linzen, Tal  and
               Zhuang, Chengxu  and
               Choshen, Leshem  and
               Cotterell, Ryan  and
               Warstadt, Alex  and
               Wilcox, Ethan Gotlieb},
  month     = nov,
  year      = {2024},
  address   = {Miami, FL, USA},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.conll-babylm.0/}
}

@inproceedings{conneau2023fleurs,
  author    = {Conneau, Alexis and Ma, Min and Khanuja, Simran and Zhang, Yu and Axelrod, Vera and Dalmia, Siddharth and Riesa, Jason and Rivera, Clara and Bapna, Ankur},
  booktitle = {2022 IEEE Spoken Language Technology Workshop (SLT)},
  title     = {FLEURS: FEW-Shot Learning Evaluation of Universal Representations of Speech},
  year      = {2023},
  volume    = {},
  number    = {},
  pages     = {798-805},
  keywords  = {Conferences;Buildings;Speech recognition;Benchmark testing;Machine translation;Task analysis;Automatic speech recognition;Massively Multilingual Speech Recognition;Low-Resource Language Dataset;Speech Language Identification;Speech Information Retrieval;Few-/Zero- Shot Learning},
  doi       = {10.1109/SLT54892.2023.10023141}
}

@inproceedings{cuervo2024scaling,
  title     = {Scaling Properties of Speech Language Models},
  author    = {Cuervo, Santiago and Marxer, Ricard},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages     = {351--361},
  year      = {2024}
}

@article{Cutler1987,
  author    = {Cutler, Anne and Carter, David M.},
  doi       = {10.1016/0885-2308(87)90004-0},
  issn      = {10958363},
  journal   = {Computer Speech and Language},
  month     = {sep},
  number    = {3-4},
  pages     = {133--142},
  publisher = {Academic Press},
  title     = {{The predominance of strong initial syllables in the English vocabulary}},
  volume    = {2},
  year      = {1987}
}

@book{danet2007multilingual,
  title     = {The multilingual Internet: Language, culture, and communication online},
  author    = {Danet, Brenda and Herring, Susan C},
  year      = {2007},
  publisher = {Oxford University Press}
}

@misc{DataReportal2024,
  author    = {DataReportal and We Are Social and Meltwater},
  title     = {Languages most frequently used for web content as of January 2024, by share of websites [Graph]},
  month     = {January 31},
  year      = {2024},
  publisher = {Statista},
  url       = {https://www.statista.com/statistics/262946/most-common-languages-on-the-internet/},
  note      = {Retrieved November 14, 2024}
}

@article{dautriche2017wordform,
  title     = {Wordform similarity increases with semantic similarity: An analysis of 100 languages},
  author    = {Dautriche, Isabelle and Mahowald, Kyle and Gibson, Edward and Piantadosi, Steven T},
  journal   = {Cognitive science},
  volume    = {41},
  number    = {8},
  pages     = {2149--2169},
  year      = {2017},
  publisher = {Wiley Online Library}
}

@article{dautriche2017words,
  title     = {Words cluster phonetically beyond phonotactic regularities},
  author    = {Dautriche, Isabelle and Mahowald, Kyle and Gibson, Edward and Christophe, Anne and Piantadosi, Steven T},
  journal   = {Cognition},
  volume    = {163},
  pages     = {128--145},
  year      = {2017},
  publisher = {Elsevier}
}

@inproceedings{diehl-martinez-etal-2024-tending,
  title     = {Tending Towards Stability: Convergence Challenges in Small Language Models},
  author    = {Diehl Martinez, Richard  and
               Lesci, Pietro  and
               Buttery, Paula},
  editor    = {Al-Onaizan, Yaser  and
               Bansal, Mohit  and
               Chen, Yun-Nung},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2024},
  month     = nov,
  year      = {2024},
  address   = {Miami, Florida, USA},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.findings-emnlp.187/},
  doi       = {10.18653/v1/2024.findings-emnlp.187},
  pages     = {3275--3286},
  abstract  = {Increasing the number of parameters in language models is a common strategy to enhance their performance. However, smaller language models remain valuable due to their lower operational costs. Despite their advantages, smaller models frequently underperform compared to their larger counterparts, even when provided with equivalent data and computational resources. Specifically, their performance tends to degrade in the late pretraining phase. This is anecdotally attributed to their reduced representational capacity. Yet, the exact causes of this performance degradation remain unclear. We use the Pythia model suite to analyse the training dynamics that underlie this phenomenon. Across different model sizes, we investigate the convergence of the Attention and MLP activations to their final state and examine how the effective rank of their parameters influences this process. We find that nearly all layers in larger models stabilise early in training - within the first 20{\%} - whereas layers in smaller models exhibit slower and less stable convergence, especially when their parameters have lower effective rank. By linking the convergence of layers' activations to their parameters' effective rank, our analyses can guide future work to address inefficiencies in the learning dynamics of small models.}
}

@article{ding-2024-songcomposer,
  title   = {Songcomposer: A large language model for lyric and melody composition in song generation},
  author  = {Ding, Shuangrui and Liu, Zihan and Dong, Xiaoyi and Zhang, Pan and Qian, Rui and He, Conghui and Lin, Dahua and Wang, Jiaqi},
  journal = {arXiv preprint arXiv:2402.17645},
  year    = {2024}
}

@inbook{dixon2002word,
  place     = {Cambridge},
  title     = {Word: a typological framework},
  booktitle = {Word: A Cross-linguistic Typology},
  publisher = {Cambridge University Press},
  author    = {Dixon, R. M. W. and Aikhenvald, Alexandra Y.},
  year      = {2003},
  pages     = {1–41}
}

@book{duanmu2007phonology,
  title     = {The phonology of standard Chinese},
  author    = {Duanmu, San},
  year      = {2007},
  publisher = {Oxford University Press}
}

@article{dunbar_self-supervised_2022,
  title      = {Self-{Supervised} {Language} {Learning} {From} {Raw} {Audio}: {Lessons} {From} the {Zero} {Resource} {Speech} {Challenge}},
  volume     = {16},
  issn       = {1941-0484},
  shorttitle = {Self-{Supervised} {Language} {Learning} {From} {Raw} {Audio}},
  doi        = {10.1109/JSTSP.2022.3206084},
  number     = {6},
  journal    = {IEEE Journal of Selected Topics in Signal Processing},
  author     = {Dunbar, Ewan and Hamilakis, Nicolas and Dupoux, Emmanuel},
  month      = oct,
  year       = {2022},
  note       = {Conference Name: IEEE Journal of Selected Topics in Signal Processing},
  keywords   = {Benchmark testing, representation learning, Representation learning, Self-supervised learning, Speech processing, Textless speech processing, unsupervised and self-supervised learning, Unsupervised learning},
  pages      = {1211--1226}
}

@inproceedings{dunbar2021zero,
  title     = {The Zero Resource Speech Challenge 2021: Spoken Language Modelling},
  author    = {Dunbar, Ewan and Bernard, Mathieu and Hamilakis, Nicolas and Nguyen, Tu Anh and Seyssel, Maureen de and Roz{\'e}, Patricia and Rivi{\`e}re, Morgane and Kharitonov, Eugene and Dupoux, Emmanuel},
  booktitle = {Proc. Interspeech 2021},
  pages     = {1574--1578},
  year      = {2021}
}

@misc{Dunn2019,
  url       = {https://github.com/espeak-ng/espeak-ng},
  publisher = {GitHub},
  year      = {2022},
  title     = {e{S}peak {NG} speech synthesizer. {I}n {G}it{H}ub respository ({V}ersion 1.51)},
  author    = {R.H. Dunn and V. Vitolins}
}

@article{dupoux-2018-cognitive,
  title     = {Cognitive science in the era of artificial intelligence: A roadmap for reverse-engineering the infant language-learner},
  author    = {Dupoux, Emmanuel},
  journal   = {Cognition},
  volume    = {173},
  pages     = {43--59},
  year      = {2018},
  publisher = {Elsevier}
}

@misc{ebbertz2002,
  title  = {Internet Statistics: Distribution of languages on the Internet},
  author = {Ebbertz, Martin},
  year   = {2002},
  url    = {https://netz-tipp.de/languages.html},
  note   = {Retrieved November 14, 2024}
}

@phdthesis{eden-2018-phonological-distance,
  title  = {Measuring phonological distance between languages},
  author = {Eden, S Elizabeth},
  year   = {2018},
  school = {UCL (University College London)}
}
@inproceedings{elazar-2024-redpajama,
  title     = {What's In My Big Data?},
  author    = {Elazar, Yanai and Bhagia, Akshita and Magnusson, Ian Helgi and Ravichander, Abhilasha and Schwenk, Dustin and Suhr, Alane and Walsh, Evan Pete and Groeneveld, Dirk and Soldaini, Luca and Singh, Sameer and others},
  booktitle = {The Twelfth International Conference on Learning Representations},
  year      = {2024}
}

@article{elman-1990-finding,
  title     = {Finding structure in time},
  author    = {Elman, Jeffrey L},
  journal   = {Cognitive science},
  volume    = {14},
  number    = {2},
  pages     = {179--211},
  year      = {1990},
  publisher = {Wiley Online Library}
}

@inproceedings{evanson-2023-language,
  title     = {Language acquisition: do children and language models follow similar learning stages?},
  author    = {Evanson, Linnea and Lakretz, Yair and King, Jean R{\'e}mi},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
  pages     = {12205--12218},
  year      = {2023}
}

@inproceedings{evanson-etal-2023-language,
  title     = {Language acquisition: do children and language models follow similar learning stages?},
  author    = {Evanson, Linnea  and
               Lakretz, Yair  and
               King, Jean R{\'e}mi},
  editor    = {Rogers, Anna  and
               Boyd-Graber, Jordan  and
               Okazaki, Naoaki},
  booktitle = {Findings of the Association For Computational Linguistics: ACL 2023},
  month     = jul,
  year      = {2023},
  address   = {Toronto, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.findings-acl.773.pdf},
  doi       = {10.18653/v1/2023.findings-acl.773},
  pages     = {12205--12218}
}

@inproceedings{fan-sun-2023-constructivist,
  title     = {Constructivist Tokenization for {E}nglish},
  author    = {Fan, Allison  and
               Sun, Weiwei},
  editor    = {Bonial, Claire  and
               Tayyar Madabushi, Harish},
  booktitle = {Proceedings of the First International Workshop on Construction Grammars and NLP (CxGs+NLP, GURT/SyntaxFest 2023)},
  month     = mar,
  year      = {2023},
  address   = {Washington, D.C.},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.cxgsnlp-1.5},
  pages     = {36--40}
}

@article{faulkner2003construction,
  author   = {Cameron-Faulkner, Thea and Lieven, Elena and Tomasello, Michael},
  title    = {A construction based analysis of child directed speech},
  journal  = {Cognitive Science},
  volume   = {27},
  number   = {6},
  pages    = {843-873},
  keywords = {Language development, Input, Constructions, Syntax},
  doi      = {https://doi.org/10.1207/s15516709cog2706\_2},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog2706_2},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog2706_2},
  abstract = {Abstract The child directed speech of twelve English-speaking motherswas analyzed in terms of utterance-level constructions. First, the mothers' utterances were categorized in terms of general constructional categories such as Wh-questions, copulas and transitives. Second, mothers' utterances within these categories were further specified in terms of the initial words that framed the utterance, item-based phrases such as Are you …, I'll …, It's …, Let's …, What did … The findings were: (i) overall, only about 15\% of all maternal utterances had SVO form (most were questions, imperatives, copulas, and fragments); (ii) 51\% of all maternal utterances began with one of 52 item-based phrases, mostly consisting of two words or morphemes (45\% began with one of just 17 words); and (iii) children used many of these same item-based phrases, in some cases at a rate that correlated highly with their own mother's frequency of use. We suggest that analyses of adult–child linguistic interaction should take into account not just general constructional categories, but also the item-based constructions that adults and children use and the frequency with which they use them.},
  year     = {2003}
}


@article{feldman_infants_2021,
  author   = {Feldman, Naomi H. and Goldwater, Sharon and Dupoux, Emmanuel and Schatz, Thomas},
  title    = {{Do Infants Really Learn Phonetic Categories?}},
  journal  = {Open Mind},
  volume   = {5},
  pages    = {113-131},
  year     = {2021},
  month    = {11},
  abstract = {{Early changes in infants’ ability to perceive native and nonnative speech sound contrasts are typically attributed to their developing knowledge of phonetic categories. We critically examine this hypothesis and argue that there is little direct evidence of category knowledge in infancy. We then propose an alternative account in which infants’ perception changes because they are learning a perceptual space that is appropriate to represent speech, without yet carving up that space into phonetic categories. If correct, this new account has substantial implications for understanding early language development.}},
  issn     = {2470-2986},
  doi      = {10.1162/opmi_a_00046},
  url      = {https://doi.org/10.1162/opmi\_a\_00046},
  eprint   = {https://direct.mit.edu/opmi/article-pdf/doi/10.1162/opmi\_a\_00046/1969157/opmi\_a\_00046.pdf}
}

@inproceedings{feliciano-de-faria-2019-utterance-boundaries,
  title     = {The Role of Utterance Boundaries and Word Frequencies for Part-of-speech Learning in {B}razilian {P}ortuguese Through Distributional Analysis},
  author    = {Feliciano de Faria, Pablo Picasso},
  editor    = {Chersoni, Emmanuele  and
               Jacobs, Cassandra  and
               Lenci, Alessandro  and
               Linzen, Tal  and
               Pr{\'e}vot, Laurent  and
               Santus, Enrico},
  booktitle = {Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics},
  month     = jun,
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/W19-2917},
  doi       = {10.18653/v1/W19-2917},
  pages     = {152--159}
}

@inproceedings{feng-2023-language-universal-phonetic,
  title     = {Language-Universal Phonetic Representation in Multilingual Speech Pretraining for Low-Resource Speech Recognition },
  author    = {Feng, Siyuan and Tu, Ming and Xia, Rui and Huang, Chuanzeng and Wang, Yuxuan},
  booktitle = {{INTERSPEECH 2023}},
  address   = {Dublin, Ireland},
  publisher = {{ISCA}},
  year      = {2023}
}

@inproceedings{li2020universal,
  title={Universal phone recognition with a multilingual allophone system},
  author={Li, Xinjian and Dalmia, Siddharth and Li, Juncheng and Lee, Matthew and Littell, Patrick and Yao, Jiali and Anastasopoulos, Antonios and Mortensen, David R and Neubig, Graham and Black, Alan W and others},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={8249--8253},
  year={2020},
  organization={IEEE}
}

@inproceedings{feng-etal-2024-child,
  title     = {Is Child-Directed Speech Effective Training Data for Language Models?},
  author    = {Feng, Steven Y.  and
               Goodman, Noah  and
               Frank, Michael},
  editor    = {Al-Onaizan, Yaser  and
               Bansal, Mohit  and
               Chen, Yun-Nung},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  month     = nov,
  year      = {2024},
  address   = {Miami, Florida, USA},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.emnlp-main.1231/},
  doi       = {10.18653/v1/2024.emnlp-main.1231},
  pages     = {22055--22071},
  abstract  = {While high-performing language models are typically trained on hundreds of billions of words, human children become fluent language users with a much smaller amount of data. What are the features of the data they receive, and how do these features support language modeling objectives? To investigate this question, we train GPT-2 and RoBERTa models on 29M words of English child-directed speech and a new matched, synthetic dataset (TinyDialogues), comparing to OpenSubtitles, Wikipedia, and a heterogeneous blend of datasets from the BabyLM challenge. We evaluate the syntactic and semantic knowledge of these models using developmentally-inspired evaluations. Through pretraining experiments, we test whether the global developmental ordering or the local discourse ordering of children`s training data supports high performance relative to other datasets. The local properties of the data affect model results, but surprisingly, global properties do not. Further, child language input is not uniquely valuable for training language models. These findings support the hypothesis that, rather than proceeding from better data, the child`s learning algorithm is substantially more data-efficient than current language modeling techniques.}
}

@incollection{firth1957synopsis,
  author    = {Firth, J. R.},
  title     = {A Synopsis of Linguistic Theory, 1930--55},
  booktitle = {Studies in Linguistic Analysis},
  pages     = {1--31},
  year      = {1957},
  publisher = {Blackwell},
  address   = {Oxford},
  note      = {Special Volume of the Philological Society. Reprinted as Firth (1968)}
}

@inproceedings{fleck2008lexicalized,
  title     = {Lexicalized phonotactic word segmentation},
  author    = {Fleck, Margaret M},
  booktitle = {Proceedings of ACL-08: HLT},
  pages     = {130--138},
  year      = {2008}
}

@article{francis1979brown,
  title   = {Brown corpus manual},
  author  = {Francis, W Nelson and Kucera, Henry},
  journal = {Letters to the Editor},
  volume  = {5},
  number  = {2},
  pages   = {7},
  year    = {1979}
}

@article{futrell2020lossy,
  title     = {Lossy-context surprisal: An information-theoretic model of memory effects in sentence processing},
  author    = {Futrell, Richard and Gibson, Edward and Levy, Roger P},
  journal   = {Cognitive science},
  volume    = {44},
  number    = {3},
  pages     = {e12814},
  year      = {2020},
  publisher = {Wiley Online Library}
}
@inproceedings{gale-etal-2023-bort,
  title     = {Mixed Orthographic/Phonemic Language Modeling: Beyond Orthographically Restricted Transformers ({BORT})},
  author    = {Gale, Robert  and
               Salem, Alexandra  and
               Fergadiotis, Gerasimos  and
               Bedrick, Steven},
  booktitle = {Proceedings of the 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)},
  month     = jul,
  year      = {2023},
  address   = {Toronto, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.repl4nlp-1.18},
  doi       = {10.18653/v1/2023.repl4nlp-1.18},
  pages     = {212--225}
}

@inproceedings{gales2014speech,
  title        = {Speech recognition and keyword spotting for low-resource languages: Babel project research at cued},
  author       = {Gales, Mark JF and Knill, Kate M and Ragni, Anton and Rath, Shakti P},
  booktitle    = {Fourth International workshop on spoken language technologies for under-resourced languages (SLTU-2014)},
  pages        = {16--23},
  year         = {2014},
  organization = {International Speech Communication Association (ISCA)}
}

@article{garofolo1993darpa,
  title   = {DARPA TIMIT acoustic-phonetic continous speech corpus CD-ROM. NIST speech disc 1-1.1},
  author  = {Garofolo, John S and Lamel, Lori F and Fisher, William M and Fiscus, Jonathan G and Pallett, David S},
  journal = {NASA STI/Recon technical report n},
  volume  = {93},
  pages   = {27403},
  year    = {1993}
}

@inproceedings{georges-gabriel-charpentier-samuel-2023-layers,
  title     = {Not all layers are equally as important: Every Layer Counts {BERT}},
  author    = {Georges Gabriel Charpentier, Lucas  and
               Samuel, David},
  editor    = {Warstadt, Alex  and
               Mueller, Aaron  and
               Choshen, Leshem  and
               Wilcox, Ethan  and
               Zhuang, Chengxu  and
               Ciro, Juan  and
               Mosquera, Rafael  and
               Paranjabe, Bhargavi  and
               Williams, Adina  and
               Linzen, Tal  and
               Cotterell, Ryan},
  booktitle = {Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning},
  month     = dec,
  year      = {2023},
  address   = {Singapore},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.conll-babylm.20/},
  doi       = {10.18653/v1/2023.conll-babylm.20},
  pages     = {238--252}
}

@misc{gerlach2018standardizedprojectgutenbergcorpus,
  title         = {A standardized Project Gutenberg corpus for statistical analysis of natural language and quantitative linguistics},
  author        = {Martin Gerlach and Francesc Font-Clos},
  year          = {2018},
  eprint        = {1812.08092},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1812.08092}
}

@inproceedings{giulianelli-etal-2024-proper,
  title     = {On the Proper Treatment of Tokenization in Psycholinguistics},
  author    = {Giulianelli, Mario  and
               Malagutti, Luca  and
               Gastaldi, Juan Luis  and
               DuSell, Brian  and
               Vieira, Tim  and
               Cotterell, Ryan},
  editor    = {Al-Onaizan, Yaser  and
               Bansal, Mohit  and
               Chen, Yun-Nung},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  month     = nov,
  year      = {2024},
  address   = {Miami, Florida, USA},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.emnlp-main.1032/},
  doi       = {10.18653/v1/2024.emnlp-main.1032},
  pages     = {18556--18572},
  abstract  = {Language models are widely used in computational psycholinguistics to test theories that relate the negative log probability (the surprisal) of a region of interest (a substring of characters) under a language model to its cognitive cost experienced by readers, as operationalized, for example, by gaze duration on the region. However, the application of modern language models to psycholinguistic studies is complicated by the practice of using tokenization as an intermediate step in training a model. Doing so results in a language model over *token* strings rather than one over character strings. Vexingly, regions of interest are generally misaligned with these token strings. The paper argues that token-level language models should be (approximately) marginalized into character-level language models before they are used in psycholinguistic studies to compute the surprisal of a region of interest; then, the marginalized character-level language model can be used to compute the surprisal of an arbitrary character substring, which we term a focal area, that the experimenter may wish to use as a predictor. Our proposal of marginalizing a token-level model into a character-level one solves this misalignment issue independently of the tokenization scheme. Empirically, we discover various focal areas whose surprisal is a better psychometric predictor than the surprisal of the region of interest itself.}
}

@misc{gleitman1972mother,
  title     = {Mother, I'd rather do it myself: Some effects and non-effects of maternal speech style},
  author    = {Gleitman, H and Gleitman, L and Newport, EL},
  year      = {1972},
  publisher = {Talking to children. Cambridge University Press}
}

@article{gleitman1988learning,
  title     = {Where learning begins: Initial representations for language learning.},
  author    = {Gleitman, Lila R and Gleitman, Henry and Landau, Barbara and Wanner, Eric},
  year      = {1988},
  publisher = {Cambridge University Press},
  journal   = {Linguistics: The Cambridge Survey: Volume 3, Language: Psychological and Biological Aspects},
  issn      = {0-521-30835-6},
  pages     = {150--193}
}

@inproceedings{glushchenko_programmatic_2019,
  address   = {Cham},
  title     = {Programmatic {Link} {Grammar} {Induction} for {Unsupervised} {Language} {Learning}},
  isbn      = {978-3-030-27005-6},
  abstract  = {Although natural (i.e. human) languages do not seem to follow a strictly formal grammar, their structure analysis and generation can be approximated by one. Having such a grammar is an important tool for programmatic language understanding. Due to the huge number of natural languages and their variations, processing tools that rely on human intervention are available only for the most popular ones. We explore the problem of unsupervisedly inducing a formal grammar for any language, using the Link Grammar paradigm, from unannotated parses also obtained without supervision from an input corpus. The details of our state-of-the-art grammar induction technology and its evaluation techniques are described, as well as preliminary results of its application on both synthetic and real world text-corpora.},
  booktitle = {Artificial {General} {Intelligence}},
  publisher = {Springer International Publishing},
  author    = {Glushchenko, Alex and Suarez, Andres and Kolonin, Anton and Goertzel, Ben and Baskov, Oleg},
  editor    = {Hammer, Patrick and Agrawal, Pulin and Goertzel, Ben and Iklé, Matthew},
  year      = {2019},
  pages     = {111--120}
}

@inproceedings{godfrey1992switchboard,
  title        = {SWITCHBOARD: Telephone speech corpus for research and development},
  author       = {Godfrey, John J and Holliman, Edward C and McDaniel, Jane},
  booktitle    = {Acoustics, speech, and signal processing, ieee international conference on},
  volume       = {1},
  pages        = {517--520},
  year         = {1992},
  organization = {IEEE Computer Society}
}

@article{Goldwater2009,
  abstract  = {Since the experiments of Saffran et al. [Saffran, J., Aslin, R., & Newport, E. (1996). Statistical learning in 8-month-old infants. Science, 274, 1926-1928], there has been a great deal of interest in the question of how statistical regularities in the speech stream might be used by infants to begin to identify individual words. In this work, we use computational modeling to explore the effects of different assumptions the learner might make regarding the nature of words - in particular, how these assumptions affect the kinds of words that are segmented from a corpus of transcribed child-directed speech. We develop several models within a Bayesian ideal observer framework, and use them to examine the consequences of assuming either that words are independent units, or units that help to predict other units. We show through empirical and theoretical results that the assumption of independence causes the learner to undersegment the corpus, with many two- and three-word sequences (e.g. what's that, do you, in the house) misidentified as individual words. In contrast, when the learner assumes that words are predictive, the resulting segmentation is far more accurate. These results indicate that taking context into account is important for a statistical word segmentation strategy to be successful, and raise the possibility that even young infants may be able to exploit more subtle statistical patterns than have usually been considered. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
  author    = {Goldwater, Sharon and Griffiths, Thomas L. and Johnson, Mark},
  doi       = {10.1016/j.cognition.2009.03.008},
  issn      = {00100277},
  journal   = {Cognition},
  keywords  = {Bayesian,Computational modeling,Language acquisition,Word segmentation},
  month     = {jul},
  number    = {1},
  pages     = {21--54},
  pmid      = {19409539},
  publisher = {Elsevier},
  title     = {{A Bayesian framework for word segmentation: Exploring the effects of context}},
  volume    = {112},
  year      = {2009}
}
@article{goriely2023word,
  title     = {Word segmentation from transcriptions of child-directed speech using lexical and sub-lexical cues},
  author    = {Goriely, Z{\'e}bulon and Caines, Andrew and Buttery, Paula},
  journal   = {Journal of Child Language},
  pages     = {1--41},
  year      = {2023},
  publisher = {Cambridge University Press}
}

@inproceedings{goriely2024babble,
  title     = {From Babble to Words: Pre-Training Language Models on Continuous Streams of Phonemes},
  author    = {Goriely, Z{\'e}bulon  and
               Diehl Martinez, Richard  and
               Caines, Andrew  and
               Buttery, Paula  and
               Beinborn, Lisa},
  editor    = {Hu, Michael Y.  and
               Mueller, Aaron  and
               Ross, Candace  and
               Williams, Adina  and
               Linzen, Tal  and
               Zhuang, Chengxu  and
               Choshen, Leshem  and
               Cotterell, Ryan  and
               Warstadt, Alex  and
               Wilcox, Ethan Gotlieb},
  booktitle = {The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning},
  month     = nov,
  year      = {2024},
  address   = {Miami, FL, USA},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.conll-babylm.4/},
  pages     = {37--53}
}

@unpublished{goriely2025,
  title  = {{IPA CHILDES} \& {G}2{P}+: Feature-Rich Resources for Cross-Lingual Phonology and Phonemic Language Modeling},
  author = {Anonymous},
  note   = {Under review for CoNLL 2025},
  year   = {2025}
}

@inproceedings{greenberg1996insights,
  title     = {Insights into spoken language gleaned from phonetic transcription of the Switchboard corpus},
  author    = {Greenberg, Steven and Hollenback, Joy and Ellis, Dan},
  booktitle = {Proc. ICSLP},
  volume    = {96},
  pages     = {24--27},
  year      = {1996}
}

@book{gussenhoven2017understanding,
  title     = {Understanding phonology},
  author    = {Gussenhoven, Carlos and Jacobs, Haike},
  year      = {2017},
  publisher = {Routledge}
}

@article{hahn-baroni-2019-tabula,
  title     = {Tabula Nearly Rasa: Probing the Linguistic Knowledge of Character-level Neural Language Models Trained on Unsegmented Text},
  author    = {Hahn, Michael  and
               Baroni, Marco},
  editor    = {Lee, Lillian  and
               Johnson, Mark  and
               Roark, Brian  and
               Nenkova, Ani},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {7},
  year      = {2019},
  address   = {Cambridge, MA},
  publisher = {MIT Press},
  url       = {https://aclanthology.org/Q19-1033},
  doi       = {10.1162/tacl_a_00283},
  pages     = {467--484}
}

@misc{harper2011babel,
  title  = {The IARPA Babel multilingual speech database.},
  note   = {Accessed: 2020-05-01},
  author = {Harper, M. P},
  year   = {2011}
}

@article{harris1955,
  issn      = {00978507, 15350665},
  url       = {http://www.jstor.org/stable/411036},
  author    = {Zellig S. Harris},
  journal   = {Language},
  number    = {2},
  pages     = {190--222},
  publisher = {Linguistic Society of America},
  title     = {From Phoneme to Morpheme},
  volume    = {31},
  year      = {1955}
}

@article{haspelmath2023defining,
  title     = {Defining the word},
  author    = {Haspelmath, Martin},
  journal   = {Word},
  volume    = {69},
  number    = {3},
  pages     = {283--297},
  year      = {2023},
  publisher = {Taylor \& Francis}
}

@inproceedings{hendrycks-2020-mmlu,
  title     = {Measuring Massive Multitask Language Understanding},
  author    = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  booktitle = {International Conference on Learning Representations},
  year      = {2020}
}

@inproceedings{hewitt-manning-2019-structural,
  title     = {{A} Structural Probe for Finding Syntax in Word Representations},
  author    = {Hewitt, John  and
               Manning, Christopher D.},
  editor    = {Burstein, Jill  and
               Doran, Christy  and
               Solorio, Thamar},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  month     = jun,
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N19-1419},
  doi       = {10.18653/v1/N19-1419},
  pages     = {4129--4138}
}

@article{hilton2022acoustic,
  title     = {Acoustic regularities in infant-directed speech and song across cultures},
  author    = {Hilton, Courtney B and Moser, Cody J and Bertolo, Mila and Lee-Rubin, Harry and Amir, Dorsa and Bainbridge, Constance M and Simson, Jan and Knox, Dean and Glowacki, Luke and Alemu, Elias and others},
  journal   = {Nature Human Behaviour},
  volume    = {6},
  number    = {11},
  pages     = {1545--1556},
  year      = {2022},
  publisher = {Nature Publishing Group UK London}
}

@inproceedings{hoffmann-2022-chinchilla,
  author    = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Thomas and Noland, Eric and Millican, Katherine and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Kar\'{e}n and Elsen, Erich and Vinyals, Oriol and Rae, Jack and Sifre, Laurent},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  pages     = {30016--30030},
  title     = {An empirical analysis of compute-optimal large language model training},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf},
  volume    = {35},
  year      = {2022}
}

@inproceedings{hollenstein-2021-reading-times,
  title     = {Multilingual Language Models Predict Human Reading Behavior},
  author    = {Hollenstein, Nora and Pirovano, Federico and Zhang, Ce and J{\"a}ger, Lena and Beinborn, Lisa},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages     = {106--123},
  year      = {2021}
}

@inproceedings{hollenstein-etal-2021-multilingual,
  title     = {Multilingual Language Models Predict Human Reading Behavior},
  author    = {Hollenstein, Nora  and
               Pirovano, Federico  and
               Zhang, Ce  and
               J{\"a}ger, Lena  and
               Beinborn, Lisa},
  editor    = {Toutanova, Kristina  and
               Rumshisky, Anna  and
               Zettlemoyer, Luke  and
               Hakkani-Tur, Dilek  and
               Beltagy, Iz  and
               Bethard, Steven  and
               Cotterell, Ryan  and
               Chakraborty, Tanmoy  and
               Zhou, Yichao},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,},
  month     = jun,
  year      = {2021},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.findings-acl.773.pdf},
  doi       = {10.18653/v1/2021.naacl-main.10},
  pages     = {106--123}
}

@article{hsu-2021-hubert,
  title   = {Hubert: Self-supervised speech representation learning by masked prediction of hidden units},
  author  = {Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
  journal = {IEEE/ACM transactions on audio, speech, and language processing},
  volume  = {29},
  pages   = {3451--3460},
  year    = {2021}
} 

@article{hsu2021hubert,
  title     = {Hubert: Self-supervised speech representation learning by masked prediction of hidden units},
  author    = {Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
  journal   = {IEEE/ACM transactions on audio, speech, and language processing},
  volume    = {29},
  pages     = {3451--3460},
  year      = {2021},
  publisher = {IEEE}
}

@inproceedings{hu-etal-2020-systematic,
  title     = {A Systematic Assessment of Syntactic Generalization in Neural Language Models},
  author    = {Hu, Jennifer  and
               Gauthier, Jon  and
               Qian, Peng  and
               Wilcox, Ethan  and
               Levy, Roger},
  editor    = {Jurafsky, Dan  and
               Chai, Joyce  and
               Schluter, Natalie  and
               Tetreault, Joel},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.acl-main.158},
  doi       = {10.18653/v1/2020.acl-main.158},
  pages     = {1725--1744}
}

@inproceedings{hu-etal-2024-findings,
  title     = {Findings of the Second {B}aby{LM} Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora},
  author    = {Hu, Michael Y.  and
               Mueller, Aaron  and
               Ross, Candace  and
               Williams, Adina  and
               Linzen, Tal  and
               Zhuang, Chengxu  and
               Cotterell, Ryan  and
               Choshen, Leshem  and
               Warstadt, Alex  and
               Wilcox, Ethan Gotlieb},
  editor    = {Hu, Michael Y.  and
               Mueller, Aaron  and
               Ross, Candace  and
               Williams, Adina  and
               Linzen, Tal  and
               Zhuang, Chengxu  and
               Choshen, Leshem  and
               Cotterell, Ryan  and
               Warstadt, Alex  and
               Wilcox, Ethan Gotlieb},
  booktitle = {The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning},
  month     = nov,
  year      = {2024},
  address   = {Miami, FL, USA},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.conll-babylm.1/},
  pages     = {1--21}
}


@inproceedings{hu2023prompting,
  title     = {Prompting is not a substitute for probability measurements in large language models},
  author    = {Hu, Jennifer and Levy, Roger P},
  booktitle = {The 2023 Conference on Empirical Methods in Natural Language Processing},
  year      = {2023}
}


@article{huebner_structured_2018,
  title    = {Structured {Semantic} {Knowledge} {Can} {Emerge} {Automatically} from {Predicting} {Word} {Sequences} in {Child}-{Directed} {Speech}},
  volume   = {9},
  issn     = {1664-1078},
  url      = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2018.00133/full},
  doi      = {10.3389/fpsyg.2018.00133},
  abstract = {Previous research has suggested that distributional learning mechanisms may contribute to the acquisition of semantic knowledge. However, distributional learning mechanisms, statistical learning, and contemporary “deep learning” approaches have been criticized for being incapable of learning the kind of abstract and structured knowledge that many think is required for acquisition of semantic knowledge. In this paper, we show that recurrent neural networks, trained on noisy naturalistic speech to children, do in fact learn what appears to be abstract and structured knowledge. We trained two types of recurrent neural networks (Simple Recurrent Network, and Long Short-Term Memory) to predict word sequences in a 5-million-word corpus of speech directed to children ages 0 to 3 years old, and assessed what semantic knowledge they acquired. We found that learned internal representations are encoding various abstract grammatical and semantic features that are useful for predicting word sequences. Assessing the organization of semantic knowledge in terms of the similarity structure, we found evidence of emergent categorical and hierarchical structure in both models. We found that the LSTM and SRN are both learning very similar kinds of representations, but the LSTM achieved higher levels of performance on a quantitative evaluation. We also trained a non-recurrent neural network, Skip-gram, on the same input to compare our results to the state-of-the-art in machine learning. We found that Skip-gram achieves relatively similar performance to the LSTM, but is representing words more in terms of thematic compared to taxonomic relations, and we provide reasons why this might be the case. Our findings show that a learning system that derives abstract, distributed representations for the purpose of predicting sequential dependencies in naturalistic language may provide insight into emergence of many properties of the developing semantic system.},
  language = {English},
  urldate  = {2025-05-26},
  journal  = {Frontiers in Psychology},
  author   = {Huebner, Philip A. and Willits, Jon A.},
  month    = feb,
  year     = {2018},
  note     = {Publisher: Frontiers},
  keywords = {language learning, Naturalistic setting, Neural Networks (Computer), Semantic development, statistical learning},
}



@inproceedings{huebner-etal-2021-babyberta,
  title     = {{B}aby{BERT}a: Learning More Grammar With Small-Scale Child-Directed Language},
  author    = {Huebner, Philip A.  and
               Sulem, Elior  and
               Cynthia, Fisher  and
               Roth, Dan},
  editor    = {Bisazza, Arianna  and
               Abend, Omri},
  booktitle = {Proceedings of the 25th Conference on Computational Natural Language Learning},
  month     = nov,
  year      = {2021},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.conll-1.49},
  doi       = {10.18653/v1/2021.conll-1.49},
  pages     = {624--646}
}


@incollection{huebner2021using,
  title     = {Using lexical context to discover the noun category: Younger children have it easier},
  author    = {Huebner, Philip A and Willits, Jon A},
  booktitle = {Psychology of learning and motivation},
  volume    = {75},
  pages     = {279--331},
  year      = {2021},
  publisher = {Elsevier}
}


@article{ivanova2024elements,
  title   = {Elements of World Knowledge (EWOK): A cognition-inspired framework for evaluating basic world knowledge in language models},
  author  = {Anna A. Ivanova and Aalok Sathe and Benjamin Lipkin and Unnathi Kumar and Setayesh Radkani and Thomas H. Clark and Carina Kauf and Jennifer Hu and R. T. Pramod and Gabriel Grand and Vivian Paulun and Maria Ryskina and Ekin Akyurek and Ethan Wilcox and Nafisa Rashid and Leshem Choshen and Roger Levy and Evelina Fedorenko and Joshua Tenenbaum and Jacob Andreas},
  journal = {arXiv preprint arXiv:2405.09605},
  year    = {2024},
  url     = {https://arxiv.org/abs/2405.09605}
}
@article{jain-2018-fmri-context,
  title   = {Incorporating context into language encoding models for fMRI},
  author  = {Jain, Shailee and Huth, Alexander},
  journal = {Advances in neural information processing systems},
  volume  = {31},
  year    = {2018}
}

@inproceedings{johnson2020g2p,
  author    = {Hasegawa-Johnson, Mark
               and Rolston, Leanne
               and Goudeseune, Camille
               and Levow, Gina-Anne
               and Kirchhoff, Katrin},
  editor    = {Espinosa-Anke, Luis
               and Mart{\'i}n-Vide, Carlos
               and Spasi{\'{c}}, Irena},
  title     = {Grapheme-to-Phoneme Transduction for Cross-Language ASR},
  booktitle = {Statistical Language and Speech Processing},
  year      = {2020},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {3--19},
  abstract  = {Automatic speech recognition (ASR) can be deployed in a previously unknown language, in less than 24 h, given just three resources: an acoustic model trained on other languages, a set of language-model training data, and a grapheme-to-phoneme (G2P) transducer to connect them. The LanguageNet G2Ps were created with the goal of being small, fast, and easy to port to a previously unseen language. Data come from pronunciation lexicons if available, but if there are no pronunciation lexicons in the target language, then data are generated from minimal resources: from a Wikipedia description of the target language, or from a one-hour interview with a native speaker of the language. Using such methods, the LanguageNet G2Ps now include simple models in nearly 150 languages, with trained finite state transducers in 122 languages, 59 of which are sufficiently well-resourced to permit measurement of their phone error rates. This paper proposes a measure of the distance between the G2Ps in different languages, and demonstrates that agglomerative clustering of the LanguageNet languages bears some resemblance to a phylogeographic language family tree. The LanguageNet G2Ps proposed in this paper have already been applied in three cross-language ASRs, using both hybrid and end-to-end neural architectures, and further experiments are ongoing.},
  isbn      = {978-3-030-59430-5}
}

@book{jones2011cambridge,
  title     = {Cambridge English pronouncing dictionary with CD-ROM},
  author    = {Jones, Daniel},
  year      = {2011},
  publisher = {Cambridge University Press}
}


@misc{jozefowicz2016exploringlimitslanguagemodeling,
  title         = {Exploring the Limits of Language Modeling},
  author        = {Rafal Jozefowicz and Oriol Vinyals and Mike Schuster and Noam Shazeer and Yonghui Wu},
  year          = {2016},
  eprint        = {1602.02410},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1602.02410}
}

@article{jusczyk-1999-stress-voice,
  title     = {The beginnings of word segmentation in English-learning infants},
  author    = {Jusczyk, Peter W and Houston, Derek M and Newsome, Mary},
  journal   = {Cognitive psychology},
  volume    = {39},
  number    = {3-4},
  pages     = {159--207},
  year      = {1999},
  publisher = {Elsevier}
}

@article{Jusczyk1993stress,
  abstract  = {One critical aspect of language acquisition is the development of a lexicon that associates sounds and meanings; but developing a lexicon first requires that the infant segment utterances into individual words. How might the infant begin this process? The present study was designed to examine the potential role that sensitivity to predominant stress patterns of words might play in lexical development. In English, by far the majority of words have stressed (strong) initial syllables. Experiment 1 of our study demonstrated that by 9 months of age American infants listen significantly longer to words with strong/weak stress patterns than to words with weak/strong stress patterns. However, Experiment 2 showed that no significant preferences for the predominant stress pattern appear with 6‐month‐old infants, which suggests that the preference develops as a result of increasing familiarity with the prosodic features of the native language. In a third experiment, 9‐month‐olds showed a preference for strong/weak patterns even when the speech input was low‐pass filtered, which suggests that their preference is specifically for the prosodic structure of the words. Together the results suggest that attention to predominant stress patterns in the native language may form an important part of the infant's process of developing a lexicon. Copyright {\textcopyright} 1993, Wiley Blackwell. All rights reserved},
  author    = {Jusczyk, Peter W. and Cutler, Anne and Redanz, Nancy J.},
  doi       = {10.1111/j.1467-8624.1993.tb02935.x},
  issn      = {0009-3920},
  journal   = {Child Development},
  month     = {jun},
  number    = {3},
  pages     = {675--687},
  publisher = {John Wiley & Sons, Ltd},
  title     = {{Infants' Preference for the Predominant Stress Patterns of English Words}},
  url       = {http://doi.wiley.com/10.1111/j.1467-8624.1993.tb02935.x},
  volume    = {64},
  year      = {1993}
}

@article{Jusczyk1999allophonic,
  author    = {Jusczyk, Peter W. and Hohne, Elizabeth A. and Bauman, Angela},
  doi       = {10.3758/BF03213111},
  issn      = {00315117},
  journal   = {Perception and Psychophysics},
  keywords  = {Cognitive Psychology},
  number    = {8},
  pages     = {1465--1476},
  pmid      = {10598463},
  publisher = {Psychonomic Society Inc.},
  title     = {{Infants' sensitivity to allophonic cues for word segmentation}},
  url       = {https://link.springer.com/article/10.3758/BF03213111},
  volume    = {61},
  year      = {1999}
}

@misc{Jusczyk1999infants,
  abstract  = {A crucial step for acquiring a native language vocabulary is the ability to segment words from fluent speech. English-learning infants first display some ability to segment words at about 7.5 months of age. However, their initial attempts at segmenting words only approximate those of fluent speakers of the language. In particular, 7.5-month-old infants are able to segment words that conform to the predominant stress patterns of English words. The ability to segment words with other stress patterns appears to require the use of other sources of information about word boundaries. By 10.5 months, English learners display sensitivity to additional cues to word boundaries such as statistical regularities, allophonic cues and phonotactic patterns. Infants' word segmentation abilities undergo further development during their second year when they begin to link sound patterns with particular meanings. By 24 months, the speed and accuracy with which infants recognize words in fluent speech is similar to that of native adult listeners. This review describes how infants use multiple sources of information to locate word boundaries in fluent speech, thereby laying the foundations for language understanding.},
  author    = {Jusczyk, Peter W.},
  booktitle = {Trends in Cognitive Sciences},
  doi       = {10.1016/S1364-6613(99)01363-7},
  issn      = {13646613},
  month     = {sep},
  number    = {9},
  pages     = {323--328},
  publisher = {Elsevier Current Trends},
  title     = {{How infants begin to extract words from speech}},
  volume    = {3},
  year      = {1999}
}
@inproceedings{Kahn_2020,
  title     = {Libri-Light: A Benchmark for ASR with Limited or No Supervision},
  url       = {http://dx.doi.org/10.1109/ICASSP40776.2020.9052942},
  doi       = {10.1109/icassp40776.2020.9052942},
  booktitle = {ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  publisher = {IEEE},
  author    = {Kahn, J. and Riviere, M. and Zheng, W. and Kharitonov, E. and Xu, Q. and Mazare, P.E. and Karadayi, J. and Liptchinsky, V. and Collobert, R. and Fuegen, C. and Likhomanenko, T. and Synnaeve, G. and Joulin, A. and Mohamed, A. and Dupoux, E.},
  year      = {2020},
  month     = may,
  pages     = {7669–7673}
}

@article{kamper2017segmental,
  title     = {A segmental framework for fully-unsupervised large-vocabulary speech recognition},
  author    = {Kamper, Herman and Jansen, Aren and Goldwater, Sharon},
  journal   = {Computer Speech \& Language},
  volume    = {46},
  pages     = {154--174},
  year      = {2017},
  publisher = {Elsevier}
}
@article{kaplan2020scaling,
  title   = {Scaling laws for neural language models},
  author  = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal = {arXiv preprint arXiv:2001.08361},
  year    = {2020}
}
@article{kazanina2018phonemes,
  title     = {Phonemes: Lexical access and beyond},
  author    = {Kazanina, Nina and Bowers, Jeffrey S and Idsardi, William},
  journal   = {Psychonomic bulletin \& review},
  volume    = {25},
  number    = {2},
  pages     = {560--585},
  year      = {2018},
  publisher = {Springer}
}

@article{keuleers2010wuggy,
  title     = {Wuggy: A multilingual pseudoword generator},
  author    = {Keuleers, Emmanuel and Brysbaert, Marc},
  journal   = {Behavior research methods},
  volume    = {42},
  pages     = {627--633},
  year      = {2010},
  publisher = {Springer}
}


@article{kidd2014goldilocks,
  title     = {The Goldilocks effect in infant auditory attention},
  author    = {Kidd, Celeste and Piantadosi, Steven T and Aslin, Richard N},
  journal   = {Child development},
  volume    = {85},
  number    = {5},
  pages     = {1795--1804},
  year      = {2014},
  publisher = {Wiley Online Library}
}

@inproceedings{kim2016character,
  title     = {Character-aware neural language models},
  author    = {Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander},
  booktitle = {Proceedings of the AAAI conference on artificial intelligence},
  volume    = {30},
  year      = {2016}
}

@article{kirov-2018-recurrent,
  author     = {Kirov, Christo and Cotterell, Ryan},
  doi        = {10.1162/tacl_a_00247},
  eprint     = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00247/1567668/tacl\_a\_00247.pdf},
  issn       = {2307-387X},
  journal    = {Transactions of the Association for Computational Linguistics},
  month      = {12},
  pages      = {651-665},
  title      = {{Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate}},
  url        = {https://doi.org/10.1162/tacl\_a\_00247},
  volume     = {6},
  year       = {2018},
  bdsk-url-1 = {https://doi.org/10.1162/tacl%5C_a%5C_00247},
  bdsk-url-2 = {https://doi.org/10.1162/tacl_a_00247}
}

@article{koplenig2017statistical,
  title     = {The statistical trade-off between word order and word structure--Large-scale evidence for the principle of least effort},
  author    = {Koplenig, Alexander and Meyer, Peter and Wolfer, Sascha and M{\"u}ller-Spitzer, Carolin},
  journal   = {PloS one},
  volume    = {12},
  number    = {3},
  pages     = {e0173614},
  year      = {2017},
  publisher = {Public Library of Science San Francisco, CA USA}
}

@inproceedings{kudo-2018-unigram,
  title     = {Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates},
  author    = {Kudo, Taku},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages     = {66--75},
  year      = {2018}
}

@inproceedings{kudo-richardson-2018-sentencepiece,
  title     = {{S}entence{P}iece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing},
  author    = {Kudo, Taku  and
               Richardson, John},
  editor    = {Blanco, Eduardo  and
               Lu, Wei},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  month     = nov,
  year      = {2018},
  address   = {Brussels, Belgium},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/D18-2012},
  doi       = {10.18653/v1/D18-2012},
  pages     = {66--71}
}
@inproceedings{lamel1989speech,
  title     = {Speech database development: Design and analysis of the acoustic-phonetic corpus},
  author    = {Lamel, Lori F and Kassel, Robert H and Seneff, Stephanie},
  booktitle = {Proc. SIOA 1989},
  pages     = {Vol--2},
  year      = {1989}
}

@inproceedings{lavechin,
  title       = {{BabySLM: language-acquisition-friendly benchmark of self-supervised spoken language models}},
  author      = {Lavechin, Marvin and Sy, Yaya and Titeux, Hadrien and Bland{\'o}n, Mar{\'i}a Andrea Cruz and R{\"a}s{\"a}nen, Okko and Bredin, Herv{\'e} and Dupoux, Emmanuel and Cristia, Alejandrina},
  url         = {https://hal.science/hal-04247612},
  booktitle   = {{INTERSPEECH 2023}},
  address     = {Dublin, Ireland},
  publisher   = {{ISCA}},
  pages       = {4588-4592},
  year        = {2023},
  month       = Aug,
  doi         = {10.21437/Interspeech.2023-978},
  keywords    = {Spoken language modeling ; Language acquisition ; Self-supervised learning ; Child language},
  pdf         = {https://hal.science/hal-04247612/file/_Interspeech_2023__BabySLM.pdf},
  hal_id      = {hal-04247612},
  hal_version = {v1}
}

@misc{lavechin2022can,
  title     = {Can statistical learning bootstrap early language acquisition? A modeling investigation},
  author    = {Lavechin, Marvin and De Seyssel, Maureen and Titeux, Hadrien and Bredin, Herv{\'e} and Wisniewski, Guillaume and Cristia, Alejandrina and Dupoux, Emmanuel},
  year      = {2022},
  publisher = {PsyArXiv}
}

@book{leon-cristia-2024,
  author    = { Mathilde Léon and Alejandrina Cristia },
  title     = {Data Protection Handbook for Long-Form Recording Research: Navigating Data Protection Laws across the Globe},
  year      = {2024},
  publisher = {OSF},
  url       = {https://doi.org/10.31219/osf.io/dy4wt}
}

@inproceedings{leong-2022-phone,
  title     = {Phone-ing it in: Towards flexible multi-modal language model training by phonetic representations of data},
  author    = {Leong, Colin and Whitenack, Daniel},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages     = {5306--5315},
  year      = {2022}
}

@inproceedings{leong-whitenack-2022-phone,
  title     = {Phone-ing it in: Towards Flexible Multi-Modal Language Model Training by Phonetic Representations of Data},
  author    = {Leong, Colin  and
               Whitenack, Daniel},
  editor    = {Muresan, Smaranda  and
               Nakov, Preslav  and
               Villavicencio, Aline},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = may,
  year      = {2022},
  address   = {Dublin, Ireland},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.acl-long.364},
  doi       = {10.18653/v1/2022.acl-long.364},
  pages     = {5306--5315},
  abstract  = {Multi-modal techniques offer significant untapped potential to unlock improved NLP technology for local languages. However, many advances in language model pre-training are focused on text, a fact that only increases systematic inequalities in the performance of NLP tasks across the world{'}s languages. In this work, we propose a multi-modal approach to train language models using whatever text and/or audio data might be available in a language. Initial experiments using Swahili and Kinyarwanda data suggest the viability of the approach for downstream Named Entity Recognition (NER) tasks, with models pre-trained on phone data showing an improvement of up to 6{\%} F1-score above models that are trained from scratch. Preprocessing and training code will be uploaded to \url{https://github.com/sil-ai/phone-it-in}.}
}

@inproceedings{lhoest-etal-2021-datasets,
  title         = {Datasets: A Community Library for Natural Language Processing},
  author        = {Lhoest, Quentin  and
                   Villanova del Moral, Albert  and
                   Jernite, Yacine  and
                   Thakur, Abhishek  and
                   von Platen, Patrick  and
                   Patil, Suraj  and
                   Chaumond, Julien  and
                   Drame, Mariama  and
                   Plu, Julien  and
                   Tunstall, Lewis  and
                   Davison, Joe  and
                   {\v{S}}a{\v{s}}ko, Mario  and
                   Chhablani, Gunjan  and
                   Malik, Bhavitvya  and
                   Brandeis, Simon  and
                   Le Scao, Teven  and
                   Sanh, Victor  and
                   Xu, Canwen  and
                   Patry, Nicolas  and
                   McMillan-Major, Angelina  and
                   Schmid, Philipp  and
                   Gugger, Sylvain  and
                   Delangue, Cl{\'e}ment  and
                   Matussi{\`e}re, Th{\'e}o  and
                   Debut, Lysandre  and
                   Bekman, Stas  and
                   Cistac, Pierric  and
                   Goehringer, Thibault  and
                   Mustar, Victor  and
                   Lagunas, Fran{\c{c}}ois  and
                   Rush, Alexander  and
                   Wolf, Thomas},
  booktitle     = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  month         = nov,
  year          = {2021},
  address       = {Online and Punta Cana, Dominican Republic},
  publisher     = {Association for Computational Linguistics},
  url           = {https://aclanthology.org/2021.emnlp-demo.21},
  pages         = {175--184},
  abstract      = {The scale, variety, and quantity of publicly-available NLP datasets has grown rapidly as researchers propose new tasks, larger models, and novel benchmarks. Datasets is a community library for contemporary NLP designed to support this ecosystem. Datasets aims to standardize end-user interfaces, versioning, and documentation, while providing a lightweight front-end that behaves similarly for small datasets as for internet-scale corpora. The design of the library incorporates a distributed, community-driven approach to adding datasets and documenting usage. After a year of development, the library now includes more than 650 unique datasets, has more than 250 contributors, and has helped support a variety of novel cross-dataset research projects and shared tasks. The library is available at https://github.com/huggingface/datasets.},
  eprint        = {2109.02846},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@inproceedings{li-2023-phoneme-level-bert,
  title     = {Phoneme-level {BERT} for enhanced prosody of text-to-speech with grapheme predictions},
  author    = {Li, Yinghao Aaron and Han, Cong and Jiang, Xilin and Mesgarani, Nima},
  booktitle = {ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages     = {1--5},
  year      = {2023}
}

@inproceedings{li2023phoneme,
  title     = {Phoneme-level bert for enhanced prosody of text-to-speech with grapheme predictions},
  author    = {Li, Yinghao Aaron and Han, Cong and Jiang, Xilin and Mesgarani, Nima},
  booktitle = {ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages     = {1--5},
  year      = {2023}
}

@inproceedings{libovicky-fraser-2020-towards,
  title     = {Towards Reasonably-Sized Character-Level Transformer {NMT} by Finetuning Subword Systems},
  author    = {Libovick{\'y}, Jind{\v{r}}ich  and
               Fraser, Alexander},
  editor    = {Webber, Bonnie  and
               Cohn, Trevor  and
               He, Yulan  and
               Liu, Yang},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  month     = nov,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.emnlp-main.203},
  doi       = {10.18653/v1/2020.emnlp-main.203},
  pages     = {2572--2579}
}

@book{lin2007sounds,
  title     = {The Sounds of Chinese},
  author    = {Lin, Yen-Hwei},
  year      = {2007},
  publisher = {Cambridge University Press}
}

@inproceedings{ma-etal-2020-charbert,
  title     = {{C}har{BERT}: Character-aware Pre-trained Language Model},
  author    = {Ma, Wentao  and
               Cui, Yiming  and
               Si, Chenglei  and
               Liu, Ting  and
               Wang, Shijin  and
               Hu, Guoping},
  editor    = {Scott, Donia  and
               Bel, Nuria  and
               Zong, Chengqing},
  booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
  month     = dec,
  year      = {2020},
  address   = {Barcelona, Spain (Online)},
  publisher = {International Committee on Computational Linguistics},
  url       = {https://aclanthology.org/2020.coling-main.4},
  doi       = {10.18653/v1/2020.coling-main.4},
  pages     = {39--50}
}

@inproceedings{ma2016learning,
  title     = {Learning phone embeddings for word segmentation of child-directed speech},
  author    = {Ma, Jianqiang and {\c{C}}{\"o}ltekin, {\c{C}}a{\u{g}}r{\i} and Hinrichs, Erhard},
  booktitle = {Proceedings of the 7th Workshop on Cognitive Aspects of Computational Language Learning},
  pages     = {53--63},
  year      = {2016}
}

@article{macwhinney_understanding_2019,
  title    = {Understanding spoken language through {TalkBank}},
  volume   = {51},
  issn     = {1554-3528},
  url      = {https://doi.org/10.3758/s13428-018-1174-9},
  doi      = {10.3758/s13428-018-1174-9},
  abstract = {Ongoing advances in computer technology have opened up a deluge of new datasets for understanding human behavior (Goldstone \& Lupyan, 2016). Many of these datasets provide information on the use of written language. However, data on naturally occurring spoken-language conversations are much more difficult to obtain. A major exception to this is the TalkBank system, which provides online multimedia data for 14 types of spoken-language data: language in aphasia, child language, stuttering, child phonology, autism spectrum disorder, bilingualism, Conversation Analysis, classroom discourse, dementia, right hemisphere damage, Danish conversation, second language learning, traumatic brain injury, and daylong recordings in the home. The present report reviews these resources and describes the ways they are being used to further our understanding of human language and communication.},
  number   = {4},
  journal  = {Behavior Research Methods},
  author   = {MacWhinney, Brian},
  month    = aug,
  year     = {2019},
  pages    = {1919--1927}
}

@article{macwhinney1978,
  issn      = {0037976X, 15405834},
  url       = {http://www.jstor.org/stable/1166047},
  author    = {Brian MacWhinney},
  journal   = {Monographs of the Society for Research in Child Development},
  number    = {1/2},
  pages     = {1--123},
  publisher = {[Society for Research in Child Development, Wiley]},
  title     = {The Acquisition of Morphophonology},
  urldate   = {2025-03-13},
  volume    = {43},
  year      = {1978}
}
@article{macwhinney1985child,
  title     = {{The Child Language Data Exchange System}},
  author    = {MacWhinney, Brian and Snow, Catherine},
  journal   = {Journal of Child Language},
  volume    = {12},
  number    = {2},
  pages     = {271--295},
  year      = {1985},
  publisher = {Cambridge University Press}
}

@article{mahowald2013info,
  title     = {Info/information theory: Speakers choose shorter words in predictive contexts},
  author    = {Mahowald, Kyle and Fedorenko, Evelina and Piantadosi, Steven T and Gibson, Edward},
  journal   = {Cognition},
  volume    = {126},
  number    = {2},
  pages     = {313--318},
  year      = {2013},
  publisher = {Elsevier}
}
@article{manning-2020-emergent,
  title     = {Emergent linguistic structure in artificial neural networks trained by self-supervision},
  author    = {Manning, Christopher D and Clark, Kevin and Hewitt, John and Khandelwal, Urvashi and Levy, Omer},
  journal   = {Proceedings of the National Academy of Sciences},
  volume    = {117},
  number    = {48},
  pages     = {30046--30054},
  year      = {2020},
  publisher = {National Acad Sciences}
}

@book{manning2009introduction,
  title     = {An introduction to information retrieval},
  author    = {Manning, Christopher D},
  year      = {2009},
  publisher = {Cambridge university press}
}

@inproceedings{martinez-etal-2023-climb,
  title     = {{CLIMB} {--} Curriculum Learning for Infant-inspired Model Building},
  author    = {Diehl Martinez, Richard   and
               McGovern, Hope  and
               Goriely, Zebulon  and
               Davis, Christopher  and
               Caines, Andrew  and
               Buttery, Paula  and
               Beinborn, Lisa},
  editor    = {Warstadt, Alex  and
               Mueller, Aaron  and
               Choshen, Leshem  and
               Wilcox, Ethan  and
               Zhuang, Chengxu  and
               Ciro, Juan  and
               Mosquera, Rafael  and
               Paranjabe, Bhargavi  and
               Williams, Adina  and
               Linzen, Tal  and
               Cotterell, Ryan},
  booktitle = {Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning},
  month     = dec,
  year      = {2023},
  address   = {Singapore},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.conll-babylm.10},
  pages     = {84--99}
}


@article{matsuhira-2023-ipaclip,
  title   = {IPA-CLIP: Integrating phonetic priors into vision and language pretraining},
  author  = {Matsuhira, Chihaya and Kastner, Marc A and Komamizu, Takahiro and Hirayama, Takatsugu and Doman, Keisuke and Kawanishi, Yasutomo and Ide, Ichiro},
  journal = {arXiv preprint arXiv:2303.03144},
  year    = {2023}
}

@article{matusevych2023infant,
  title     = {Infant phonetic learning as perceptual space learning: A crosslinguistic evaluation of computational models},
  author    = {Matusevych, Yevgen and Schatz, Thomas and Kamper, Herman and Feldman, Naomi H and Goldwater, Sharon},
  journal   = {Cognitive Science},
  volume    = {47},
  number    = {7},
  pages     = {e13314},
  year      = {2023},
  publisher = {Wiley Online Library}
}


@article{mayer-2020-phonology-distribution,
  title     = {An algorithm for learning phonological classes from distributional similarity},
  author    = {Mayer, Connor},
  journal   = {Phonology},
  volume    = {37},
  number    = {1},
  pages     = {91--131},
  year      = {2020},
  publisher = {Cambridge University Press}
}

@inproceedings{mazumder2021multilingual,
  title     = {Multilingual Spoken Words Corpus},
  author    = {Mark Mazumder and Sharad Chitlangia and Colby Banbury and Yiping Kang and Juan Manuel Ciro and Keith Achorn and Daniel Galvez and Mark Sabini and Peter Mattson and David Kanter and Greg Diamos and Pete Warden and Josh Meyer and Vijay Janapa Reddi},
  booktitle = {Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
  year      = {2021},
  url       = {https://openreview.net/forum?id=c20jiJ5K2H}
}

@inproceedings{mcauliffe2017montreal,
  title     = {Montreal forced aligner: Trainable text-speech alignment using kaldi.},
  author    = {McAuliffe, Michael and Socolof, Michaela and Mihuc, Sarah and Wagner, Michael and Sonderegger, Morgan},
  booktitle = {Interspeech},
  volume    = {2017},
  pages     = {498--502},
  year      = {2017}
}

@article{mcmurray_myth_2022,
  abstract = {Categorical perception (CP) is likely the single finding from speech perception with the biggest impact on cognitive science. However, within speech perception, it is widely known to be an artifact of task demands. CP is empirically defined as a relationship between phoneme identification and discrimination. As discrimination tasks do not appear to require categorization, this was thought to support the claim that listeners perceive speech solely in terms of linguistic categories. However, 50 years of work using discrimination tasks, priming, the visual world paradigm, and event related potentials has rejected the strongest forms of CP and provided little strong evidence for any form of it. This paper reviews the origins and impact of this scientific meme and the work challenging it. It discusses work showing that the encoding of auditory input is largely continuous, not categorical, and describes the modern theoretical synthesis in which listeners preserve fine-grained detail to enable more flexible processing. This synthesis is fundamentally inconsistent with CP. This leads to a different understanding of how to use and interpret the most basic paradigms in speech perception---phoneme identification along a continuum---and has implications for understanding language and hearing disorders, development, and multilingualism.},
  author   = {McMurray, Bob},
  doi      = {10.1121/10.0016614},
  issn     = {0001-4966},
  journal  = {The Journal of the Acoustical Society of America},
  month    = dec,
  note     = {Publisher: Acoustical Society of America},
  number   = {6},
  pages    = {3819--3842},
  title    = {The myth of categorical perception},
  url      = {https://asa.scitation.org/doi/full/10.1121/10.0016614},
  urldate  = {2023-01-10},
  volume   = {152},
  year     = {2022}
}

@article{McNemar_1947,
  title   = {Note on the Sampling Error of the Difference Between Correlated Proportions or Percentages},
  volume  = {12},
  doi     = {10.1007/BF02295996},
  number  = {2},
  journal = {Psychometrika},
  author  = {McNemar, Quinn},
  year    = {1947},
  pages   = {153–157}
}

@article{misc,
  title   = {Standard computer-compatible transcription},
  author  = {Wells, John and Barry, William and Grice, Martine and Fourcin, Adrian and Gibbon, Dafydd},
  journal = {Esprit project 2589 (SAM), Doc. no. SAM-UCL},
  volume  = {37},
  pages   = {64},
  year    = {1992}
}

@inproceedings{Mortensen-et-al:2018,
  author    = {Mortensen, David R.  and Dalmia, Siddharth and Littell, Patrick},
  title     = {Epitran: Precision {G2P} for Many Languages},
  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},
  year      = {2018},
  month     = {May},
  date      = {7--12},
  location  = {Miyazaki, Japan},
  editor    = {Nicoletta Calzolari (Conference chair) and Khalid Choukri and Christopher Cieri and Thierry Declerck and Sara Goggi and Koiti Hasida and Hitoshi Isahara and Bente Maegaard and Joseph Mariani and H\'el\`ene Mazo and Asuncion Moreno and Jan Odijk and Stelios Piperidis and Takenobu Tokunaga},
  publisher = {European Language Resources Association (ELRA)},
  address   = {Paris, France},
  isbn      = {979-10-95546-00-9},
  language  = {english}
}

@inproceedings{mosteiro2025word,
  title     = {Word boundaries and the morphology-syntax trade-off},
  author    = {Mosteiro, Pablo and Blasi, Dami{\'a}n},
  booktitle = {Proceedings of the New Horizons in Computational Linguistics for Religious Texts},
  pages     = {86--93},
  year      = {2025}
}

@inproceedings{nayeem-rafiei-2024-kidlm,
  title     = {{K}id{LM}: Advancing Language Models for Children {--} Early Insights and Future Directions},
  author    = {Nayeem, Mir Tafseer  and
               Rafiei, Davood},
  editor    = {Al-Onaizan, Yaser  and
               Bansal, Mohit  and
               Chen, Yun-Nung},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  month     = nov,
  year      = {2024},
  address   = {Miami, Florida, USA},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.emnlp-main.277/},
  doi       = {10.18653/v1/2024.emnlp-main.277},
  pages     = {4813--4836},
  abstract  = {Recent studies highlight the potential of large language models in creating educational tools for children, yet significant challenges remain in maintaining key child-specific properties such as linguistic nuances, cognitive needs, and safety standards. In this paper, we explore foundational steps toward the development of child-specific language models, emphasizing the necessity of high-quality pre-training data. We introduce a novel user-centric data collection pipeline that involves gathering and validating a corpus specifically written for and sometimes by children. Additionally, we propose a new training objective, Stratified Masking, which dynamically adjusts masking probabilities based on our domain-specific child language data, enabling models to prioritize vocabulary and concepts more suitable for children. Experimental evaluations demonstrate that our model excels in understanding lower grade-level text, maintains safety by avoiding stereotypes, and captures children`s unique preferences. Furthermore, we provide actionable insights for future research and development in child-specific language modeling.}
}

@article{newport1990maturational,
  title     = {Maturational constraints on language learning},
  author    = {Newport, Elissa L},
  journal   = {Cognitive science},
  volume    = {14},
  number    = {1},
  pages     = {11--28},
  year      = {1990},
  publisher = {Elsevier}
}

@article{nguyen-2022-word-boundaries,
  title   = {Are word boundaries useful for unsupervised language learning?},
  author  = {Nguyen, Tu Anh and De Seyssel, Maureen and Algayres, Robin and Roze, Patricia and Dunbar, Ewan and Dupoux, Emmanuel},
  journal = {arXiv preprint arXiv:2210.02956},
  year    = {2022}
}


@inproceedings{nguyen2020zero,
  title     = {The Zero Resource Speech Benchmark 2021: Metrics and baselines for unsupervised spoken language modeling},
  author    = {Nguyen, Tu Anh and de Seyssel, Maureen and Roz{\'e}, Patricia and Rivi{\`e}re, Morgane and Kharitonov, Evgeny and Baevski, Alexei and Dunbar, Ewan and Dupoux, Emmanuel},
  booktitle = {NeuRIPS Workshop on Self-Supervised Learning for Speech and Audio Processing},
  year      = {2020}
}


@article{norris1994shortlist,
  title     = {Shortlist: A connectionist model of continuous speech recognition},
  author    = {Norris, Dennis},
  journal   = {Cognition},
  volume    = {52},
  number    = {3},
  pages     = {189--234},
  year      = {1994},
  publisher = {Elsevier}
}



@article{NOVAK_MINEMATSU_HIROSE_2016,
  title   = {Phonetisaurus: Exploring grapheme-to-phoneme conversion with joint n-gram models in the WFST framework},
  volume  = {22},
  doi     = {10.1017/S1351324915000315},
  number  = {6},
  journal = {Natural Language Engineering},
  author  = {Novak, Josef Robert and Minematsu, Nobuaki and Hirose, Keikichi},
  year    = {2016},
  pages   = {907–938}
}




@inproceedings{nzeyimana-niyongabo-rubungo-2022-kinyabert,
  title     = {{K}inya{BERT}: a Morphology-aware {K}inyarwanda Language Model},
  author    = {Nzeyimana, Antoine  and
               Niyongabo Rubungo, Andre},
  editor    = {Muresan, Smaranda  and
               Nakov, Preslav  and
               Villavicencio, Aline},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = may,
  year      = {2022},
  address   = {Dublin, Ireland},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.acl-long.367},
  doi       = {10.18653/v1/2022.acl-long.367},
  pages     = {5347--5363},
  abstract  = {Pre-trained language models such as BERT have been successful at tackling many natural language processing tasks. However, the unsupervised sub-word tokenization methods commonly used in these models (e.g., byte-pair encoding - BPE) are sub-optimal at handling morphologically rich languages. Even given a morphological analyzer, naive sequencing of morphemes into a standard BERT architecture is inefficient at capturing morphological compositionality and expressing word-relative syntactic regularities. We address these challenges by proposing a simple yet effective two-tier BERT architecture that leverages a morphological analyzer and explicitly represents morphological compositionality.Despite the success of BERT, most of its evaluations have been conducted on high-resource languages, obscuring its applicability on low-resource languages. We evaluate our proposed method on the low-resource morphologically rich Kinyarwanda language, naming the proposed model architecture KinyaBERT. A robust set of experimental results reveal that KinyaBERT outperforms solid baselines by 2{\%} in F1 score on a named entity recognition task and by 4.3{\%} in average score of a machine-translated GLUE benchmark. KinyaBERT fine-tuning has better convergence and achieves more robust results on multiple tasks even in the presence of translation noise.}
}


@article{pagnoni2024byte,
  title   = {Byte latent transformer: Patches scale better than tokens},
  author  = {Pagnoni, Artidoro and Pasunuru, Ram and Rodriguez, Pedro and Nguyen, John and Muller, Benjamin and Li, Margaret and Zhou, Chunting and Yu, Lili and Weston, Jason and Zettlemoyer, Luke and others},
  journal = {arXiv preprint arXiv:2412.09871},
  year    = {2024}
}

@inproceedings{panayotov2015librispeech,
  title     = {Librispeech: an {ASR} corpus based on public domain audio books},
  author    = {Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle = {2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages     = {5206--5210},
  year      = {2015}
}


@article{pasad-2024-know-about-words,
  title     = {What do self-supervised speech models know about words?},
  author    = {Pasad, Ankita and Chien, Chung-Ming and Settle, Shane and Livescu, Karen},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {12},
  pages     = {372--391},
  year      = {2024},
  publisher = {MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@inproceedings{paszke-etal-2019-pytorch,
  title     = {{{PyTorch}}: {A}n Imperative Style, High-Performance Deep Learning Library},
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year      = 2019,
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  volume    = 32,
  url       = {https://proceedings.neurips.cc/paper\_files/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html}
}


@book{phoible,
  address   = {Jena},
  editor    = {Steven Moran and Daniel McCloy},
  publisher = {Max Planck Institute for the Science of Human History},
  title     = {PHOIBLE 2.0},
  url       = {https://phoible.org/},
  year      = {2019}
}



@article{piantadosi2011word,
  title     = {Word lengths are optimized for efficient communication},
  author    = {Piantadosi, Steven T and Tily, Harry and Gibson, Edward},
  journal   = {Proceedings of the National Academy of Sciences},
  volume    = {108},
  number    = {9},
  pages     = {3526--3529},
  year      = {2011},
  publisher = {National Academy of Sciences}
}

@article{pile,
  title   = {The {P}ile: An 800GB Dataset of Diverse Text for Language Modeling},
  author  = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  journal = {arXiv preprint arXiv:2101.00027},
  year    = {2020}
}

@article{pimentel2020phonotactic,
  title     = {Phonotactic complexity and its trade-offs},
  author    = {Pimentel, Tiago and Roark, Brian and Cotterell, Ryan},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {8},
  pages     = {1--18},
  year      = {2020},
  publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{PITT200589,
  title    = {The Buckeye corpus of conversational speech: labeling conventions and a test of transcriber reliability},
  journal  = {Speech Communication},
  volume   = {45},
  number   = {1},
  pages    = {89-95},
  year     = {2005},
  issn     = {0167-6393},
  doi      = {https://doi.org/10.1016/j.specom.2004.09.001},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167639304000974},
  author   = {Mark A. Pitt and Keith Johnson and Elizabeth Hume and Scott Kiesling and William Raymond},
  keywords = {Spontaneous speech corpus, Transcription, Labeling, American English},
  abstract = {This paper describes the Buckeye corpus of spontaneous American English speech, a 307,000-word corpus containing the speech of 40 talkers from central Ohio, USA. The method used to elicit and record the speech is described, followed by a description of the protocol that was developed to phonemically label what talkers said. The results of a test of labeling consistency are then presented. The corpus will be made available to the scientific community when labeling is completed.}
}

@article{pitt2007buckeye,
  title   = {Buckeye corpus of conversational speech (2nd release)},
  author  = {Pitt, Mark A and Dilley, Laura and Johnson, Keith and Kiesling, Scott and Raymond, William and Hume, Elizabeth and Fosler-Lussier, Eric},
  journal = {Columbus, OH: Department of Psychology, Ohio State University},
  pages   = {265--270},
  year    = {2007}
}

@inproceedings{poli2024improving,
  title     = {Improving Spoken Language Modeling with Phoneme Classification: A Simple Fine-tuning Approach},
  author    = {Poli, Maxime and Chemla, Emmanuel and Dupoux, Emmanuel},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages     = {5284--5292},
  year      = {2024}
}

@article{povinelli-1997-pointing,
  title   = {Exploitation of pointing as a referential gesture in young children, but not adolescent chimpanzees},
  journal = {Cognitive Development},
  volume  = {12},
  number  = {4},
  pages   = {423-461},
  year    = {1997},
  issn    = {0885-2014},
  doi     = {https://doi.org/10.1016/S0885-2014(97)90017-4},
  url     = {https://www.sciencedirect.com/science/article/pii/S0885201497900174},
  author  = {Daniel J. Povinelli and James E. Reaux and Donna T. Bierschwale and Ashley D. Allain and Bridgett B. Simon}
}

@article{pratap2020mls,
  title   = {Mls: A large-scale multilingual dataset for speech research},
  author  = {Pratap, Vineel and Xu, Qiantong and Sriram, Anuroop and Synnaeve, Gabriel and Collobert, Ronan},
  journal = {arXiv preprint arXiv:2012.03411},
  year    = {2020}
}

@article{prince-1997-optimality,
  title     = {Optimality: From neural networks to universal grammar},
  author    = {Prince, Alan and Smolensky, Paul},
  journal   = {Science},
  volume    = {275},
  number    = {5306},
  pages     = {1604--1610},
  year      = {1997},
  publisher = {American Association for the Advancement of Science}
}

@article{radford-2019-gpt2,
  title   = {Language models are unsupervised multitask learners},
  author  = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal = {OpenAI blog},
  volume  = {1},
  number  = {8},
  pages   = {9},
  year    = {2019}
}

@article{raffel2020exploring,
  title   = {Exploring the limits of transfer learning with a unified text-to-text transformer},
  author  = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal = {Journal of machine learning research},
  volume  = {21},
  number  = {140},
  pages   = {1--67},
  year    = {2020}
}

@article{ramirez2017look,
  title     = {Look who’s talking {NOW}! {P}arentese speech, social context, and language development across time},
  author    = {Ram{\'\i}rez-Esparza, Nair{\'a}n and Garc{\'\i}a-Sierra, Adri{\'a}n and Kuhl, Patricia K},
  journal   = {Frontiers in psychology},
  volume    = {8},
  pages     = {1008},
  year      = {2017},
  publisher = {Frontiers Media SA}
}



@article{Ratner_1984,
  title   = {Patterns of vowel modification in mother–child speech},
  volume  = {11},
  doi     = {10.1017/S030500090000595X},
  number  = {3},
  journal = {Journal of Child Language},
  author  = {Bernstein Ratner, Nan},
  year    = {1984},
  pages   = {557–578}
}

@misc{robert_forkel_2019_3549784,
  author    = {Robert Forkel and
               Steven Moran and
               Johann-Mattis List and
               Simon J Greenhill and
               Lucas Ashby and
               Kyle Gorman and
               Gereon Kaiping},
  title     = {cldf/segments: Unicode Standard tokenization},
  month     = nov,
  year      = 2019,
  publisher = {Zenodo},
  version   = {v2.1.3},
  doi       = {10.5281/zenodo.3549784},
  url       = {https://doi.org/10.5281/zenodo.3549784}
}

@article{rowe2008child,
  title     = {Child-directed speech: Relation to socioeconomic status, knowledge of child development and child vocabulary skill},
  author    = {Rowe, Meredith L},
  journal   = {Journal of child language},
  volume    = {35},
  number    = {1},
  pages     = {185--205},
  year      = {2008},
  publisher = {Cambridge University Press}
}

@article{Saffran1996distributional,
  abstract  = {One of the infant's first tasks in language acquisition is to discover the words embedded in a mostly continuous speech stream. This learning problem might be solved by using distributional cues to word boundaries - for example, by computing the transitional probabilities between sounds in the language input and using the relative strengths of these probabilities to hypothesize word boundaries. The learner might be further aidedby language-specific prosodic cues correlated with word boundaries. As a first step in testing these hypotheses, we briefly exposed adults to an artificial language in which the only cues available for word segmentation were the transitional probabilities between syllables. Subjects were able to learn the words of this language. Furthermore, the addition of certain prosodic cues served to enhance performance. These results suggest that distributional cues may play an important role in the initial word segmentation of language learners. {\textcopyright} 1996 Academic Press, Inc.},
  author    = {Saffran, Jenny R. and Newport, Elissa L. and Aslin, Richard N.},
  doi       = {10.1006/jmla.1996.0032},
  issn      = {0749596X},
  journal   = {Journal of Memory and Language},
  month     = {aug},
  number    = {4},
  pages     = {606--621},
  publisher = {Academic Press Inc.},
  title     = {{Word segmentation: The role of distributional cues}},
  volume    = {35},
  year      = {1996}
}

@article{Saffran1996learning,
  abstract  = {Learners rely on a combination of experience independent and experience- dependent mechanisms to extract information from the environment. Language acquisition involves both types of mechanisms, but most theorists emphasize the relative importance of experience-independent mechanisms. The present study shows that a fundamental task of language acquisition, segmentation of words from fluent speech, can be accomplished by 8-month-old infants based solely on the statistical relationships between neighboring speech sounds. Moreover, this word segmentation was based on statistical learning from only 2 minutes of exposure, suggesting that infants have access to a powerful mechanism for the computation of statistical properties of the language input.},
  author    = {Saffran, Jenny R. and Aslin, Richard N. and Newport, Elissa L.},
  doi       = {10.1126/science.274.5294.1926},
  issn      = {00368075},
  journal   = {Science},
  month     = {dec},
  number    = {5294},
  pages     = {1926--1928},
  pmid      = {8943209},
  publisher = {American Association for the Advancement of Science},
  title     = {{Statistical learning by 8-month-old infants}},
  url       = {http://science.sciencemag.org/},
  volume    = {274},
  year      = {1996}
}

@inproceedings{saffran1996statistical,
  title        = {Statistical cues in language acquisition: Word segmentation by infants},
  author       = {Saffran, Jenny R and Aslin, Richard N and Newport, Elissa L},
  booktitle    = {Proceedings of the Eighteenth Annual Conference of the Cognitive Science Society},
  pages        = {376--380},
  year         = {1996},
  organization = {Lawrence Erlbaum Associates Mawah, NJ}
}

@inproceedings{salesky-etal-2020-corpus,
  title     = {A Corpus for Large-Scale Phonetic Typology},
  author    = {Salesky, Elizabeth  and
               Chodroff, Eleanor  and
               Pimentel, Tiago  and
               Wiesner, Matthew  and
               Cotterell, Ryan  and
               Black, Alan W  and
               Eisner, Jason},
  editor    = {Jurafsky, Dan  and
               Chai, Joyce  and
               Schluter, Natalie  and
               Tetreault, Joel},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.acl-main.415/},
  doi       = {10.18653/v1/2020.acl-main.415},
  pages     = {4526--4546},
  abstract  = {A major hurdle in data-driven research on typology is having sufficient data in many languages to draw meaningful conclusions. We present VoxClamantis v1.0, the first large-scale corpus for phonetic typology, with aligned segments and estimated phoneme-level labels in 690 readings spanning 635 languages, along with acoustic-phonetic measures of vowels and sibilants. Access to such data can greatly facilitate investigation of phonetic typology at a large scale and across many languages. However, it is non-trivial and computationally intensive to obtain such alignments for hundreds of languages, many of which have few to no resources presently available. We describe the methodology to create our corpus, discuss caveats with current methods and their impact on the utility of this data, and illustrate possible research directions through a series of case studies on the 48 highest-quality readings. Our corpus and scripts are publicly available for non-commercial use at \url{https://voxclamantisproject.github.io}.}
}

@inproceedings{salhan-etal-2024-less,
  title     = {Less is More: Pre-Training Cross-Lingual Small-Scale Language Models with Cognitively-Plausible Curriculum Learning Strategies},
  author    = {Salhan, Suchir  and
               Diehl Martinez, Richard  and
               Goriely, Z{\'e}bulon  and
               Buttery, Paula},
  editor    = {Hu, Michael Y.  and
               Mueller, Aaron  and
               Ross, Candace  and
               Williams, Adina  and
               Linzen, Tal  and
               Zhuang, Chengxu  and
               Choshen, Leshem  and
               Cotterell, Ryan  and
               Warstadt, Alex  and
               Wilcox, Ethan Gotlieb},
  booktitle = {The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning},
  month     = nov,
  year      = {2024},
  address   = {Miami, FL, USA},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.conll-babylm.15/},
  pages     = {174--188}
}

@inproceedings{samuel-etal-2023-trained,
  title     = {Trained on 100 million words and still in shape: {BERT} meets {B}ritish {N}ational {C}orpus},
  author    = {Samuel, David  and
               Kutuzov, Andrey  and
               {\O}vrelid, Lilja  and
               Velldal, Erik},
  editor    = {Vlachos, Andreas  and
               Augenstein, Isabelle},
  booktitle = {Findings of the Association for Computational Linguistics: EACL 2023},
  month     = may,
  year      = {2023},
  address   = {Dubrovnik, Croatia},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.findings-eacl.146},
  doi       = {10.18653/v1/2023.findings-eacl.146},
  pages     = {1954--1974},
  abstract  = {While modern masked language models (LMs) are trained on ever larger corpora, we here explore the effects of down-scaling training to a modestly-sized but representative, well-balanced, and publicly available English text source {--} the British National Corpus. We show that pre-training on this carefully curated corpus can reach better performance than the original BERT model. We argue that this type of corpora has great potential as a language modeling benchmark. To showcase this potential, we present fair, reproducible and data-efficient comparative studies of LMs, in which we evaluate several training objectives and model architectures and replicate previous empirical results in a systematic way. We propose an optimized LM architecture called LTG-BERT.}
}

@inproceedings{sanabria2021difficulty,
  title     = {On the Difficulty of Segmenting Words with Attention},
  author    = {Sanabria, Ramon and Tang, Hao and Goldwater, Sharon},
  booktitle = {Proceedings of the Second Workshop on Insights from Negative Results in NLP},
  pages     = {67--73},
  year      = {2021}
}

@article{schatz2021early,
  title     = {Early phonetic learning without phonetic categories: Insights from large-scale simulations on realistic input},
  author    = {Schatz, Thomas and Feldman, Naomi H and Goldwater, Sharon and Cao, Xuan-Nga and Dupoux, Emmanuel},
  journal   = {Proceedings of the National Academy of Sciences},
  volume    = {118},
  number    = {7},
  pages     = {e2001844118},
  year      = {2021},
  publisher = {National Acad Sciences}
}

@inproceedings{schultz2002globalphone,
  title     = {Globalphone: a multilingual speech and text database developed at karlsruhe university.},
  author    = {Schultz, Tanja},
  booktitle = {Interspeech},
  volume    = {2},
  pages     = {345--348},
  year      = {2002}
}

@inproceedings{schultz2013globalphone,
  title        = {Globalphone: A multilingual text \& speech database in 20 languages},
  author       = {Schultz, Tanja and Vu, Ngoc Thang and Schlippe, Tim},
  booktitle    = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
  pages        = {8126--8130},
  year         = {2013},
  organization = {IEEE}
}

@inproceedings{schuster-etal-2012-wordpiece,
  title     = {Japanese and korean voice search},
  author    = {Schuster, Mike and Nakajima, Kaisuke},
  booktitle = {2012 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages     = {5149--5152},
  year      = {2012}
}

@article{seidenberg1989distributed,
  title     = {A distributed, developmental model of word recognition and naming.},
  author    = {Seidenberg, Mark S and McClelland, James L},
  journal   = {Psychological review},
  volume    = {96},
  number    = {4},
  pages     = {523},
  year      = {1989},
  publisher = {American Psychological Association}
}

@inproceedings{sennrich-etal-2016-bpe,
  title     = {Neural Machine Translation of Rare Words with Subword Units},
  author    = {Sennrich, Rico  and
               Haddow, Barry  and
               Birch, Alexandra},
  editor    = {Erk, Katrin  and
               Smith, Noah A.},
  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = aug,
  year      = {2016},
  address   = {Berlin, Germany},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/P16-1162},
  doi       = {10.18653/v1/P16-1162},
  pages     = {1715--1725}
}

@article{seyssel-2023-realistic,
  title     = {Realistic and broad-scope learning simulations: first results and challenges},
  author    = {de Seyssel, Maureen and Lavechin, Marvin and Dupoux, Emmanuel},
  journal   = {Journal of Child Language},
  volume    = {50},
  number    = {6},
  pages     = {1294--1317},
  year      = {2023},
  publisher = {Cambridge University Press}
}

@inproceedings{shen2024bambino,
  title     = {BAMBINO-LM:(Bilingual-) Human-Inspired Continual Pre-training of BabyLM},
  author    = {Shen, Zhewen and Joshi, Aditya and Chen, Ruey-Cheng},
  booktitle = {Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics},
  pages     = {1--7},
  year      = {2024}
}

@misc{snow1977talking,
  title     = {Talking to children: Language input and acquisition},
  author    = {Snow, CE},
  year      = {1977},
  publisher = {Cambridge University Press}
}

@inproceedings{sohn2024zero,
  title     = {Zero-Shot Cross-Lingual {NER} Using Phonemic Representations for Low-Resource Languages},
  author    = {Sohn, Jimin and Jung, Haeji and Cheng, Alex and Kang, Jooeon and Du, Yilin and Mortensen, David R},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages     = {13595--13602},
  year      = {2024}
}

@article{strachan-2024-tom,
  title     = {Testing theory of mind in large language models and humans},
  author    = {Strachan, James WA and Albergo, Dalila and Borghini, Giulia and Pansardi, Oriana and Scaliti, Eugenio and Gupta, Saurabh and Saxena, Krati and Rufo, Alessandro and Panzeri, Stefano and Manzi, Guido and others},
  journal   = {Nature Human Behaviour},
  pages     = {1--11},
  year      = {2024},
  publisher = {Nature Publishing Group UK London}
}

@inproceedings{sun-etal-2023-characters,
  title     = {From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding},
  author    = {Sun, Li  and
               Luisier, Florian  and
               Batmanghelich, Kayhan  and
               Florencio, Dinei  and
               Zhang, Cha},
  editor    = {Rogers, Anna  and
               Boyd-Graber, Jordan  and
               Okazaki, Naoaki},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = jul,
  year      = {2023},
  address   = {Toronto, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.acl-long.200},
  doi       = {10.18653/v1/2023.acl-long.200},
  pages     = {3605--3620}
}

@inproceedings{sundararaman-2021-phonemebert,
  author    = {Mukuntha Narayanan Sundararaman and Ayush Kumar and Jithendra Vepa},
  title     = {{PhonemeBERT: Joint Language Modelling of Phoneme Sequence and ASR Transcript}},
  year      = 2021,
  booktitle = {Proc. Interspeech 2021},
  pages     = {3236--3240},
  doi       = {10.21437/Interspeech.2021-1582},
  issn      = {2958-1796}
}

@article{Suomi1997,
  abstract  = {Finnish vowael harmony rules require that if the vowel in the first syllable of a word belongs to one of two vowel sets, then all subsequent vowels in that word must belong either to the same set or to a neutral set. A harmony mismatch between two syllables containing vowels from the opposing sets thus signals a likely word boundary. We report five experiments showing that Finnish listeners can exploit this information in an on-line speech segmentation task. Listeners found it easier to detect words like hymy at the end of the nonsense string puhymy (where there is a harmony mismatch between the first two syllables) than in the string pyhymy (where there is no mismatch). There was no such effect, however, when the target words appeared at the beginning of the nonsense string (e.g., hymypu vs hymypy). Stronger harmony effects were found for targets containing front harmony vowels (e.g., hymy) than for targets containing back harmony vowels (e.g., palo in kypalo and kupalo). The same pattern of results appeared whether target position within the string was predictable or unpredictable. Harmony mismatch thus appears to provide a useful segmentation cue for the detection of word onsets in Finnish speech. {\textcopyright} 1997 Academic Press.},
  author    = {Suomi, Kari and McQueen, James M. and Cutler, Anne},
  doi       = {10.1006/jmla.1996.2495},
  issn      = {0749596X},
  journal   = {Journal of Memory and Language},
  month     = {apr},
  number    = {3},
  pages     = {422--444},
  publisher = {Academic Press Inc.},
  title     = {{Vowel harmony and speech segmentation in Finnish}},
  volume    = {36},
  year      = {1997}
}

@inproceedings{suvarna-etal-2024-phonologybench,
  title     = {{P}honology{B}ench: Evaluating Phonological Skills of Large Language Models},
  author    = {Suvarna, Ashima  and
               Khandelwal, Harshita  and
               Peng, Nanyun},
  booktitle = {Proceedings of the 1st Workshop on Towards Knowledgeable Language Models (KnowLLM 2024)},
  month     = aug,
  year      = {2024},
  address   = {Bangkok, Thailand},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.knowllm-1.1},
  pages     = {1--14}
}

@inproceedings{suzgun-2023-Big-Bench,
  title     = {Challenging {BIG}-Bench Tasks and Whether Chain-of-Thought Can Solve Them},
  author    = {Suzgun, Mirac  and
               Scales, Nathan  and
               Sch{\"a}rli, Nathanael  and
               Gehrmann, Sebastian  and
               Tay, Yi  and
               Chung, Hyung Won  and
               Chowdhery, Aakanksha  and
               Le, Quoc  and
               Chi, Ed  and
               Zhou, Denny  and
               Wei, Jason},
  editor    = {Rogers, Anna  and
               Boyd-Graber, Jordan  and
               Okazaki, Naoaki},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
  month     = jul,
  year      = {2023},
  address   = {Toronto, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.findings-acl.824},
  doi       = {10.18653/v1/2023.findings-acl.824},
  pages     = {13003--13051}
}

@misc{taubert_2024_pinyin-to-ipa_2024,
  author  = {Taubert, Stefan},
  doi     = {10.5281/zenodo.10639971},
  license = {MIT},
  month   = feb,
  title   = {{pinyin-to-ipa}},
  url     = {https://github.com/stefantaubert/pinyin-to-ipa},
  version = {0.0.2},
  year    = {2024}
}

@article{taylor2003penn,
  title     = {The Penn treebank: an overview},
  author    = {Taylor, Ann and Marcus, Mitchell and Santorini, Beatrice},
  journal   = {Treebanks: Building and using parsed corpora},
  pages     = {5--22},
  year      = {2003},
  publisher = {Springer}
}

@inproceedings{timiryasov-tastet-2023-baby,
  title     = {Baby {L}lama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty},
  author    = {Timiryasov, Inar  and
               Tastet, Jean-Loup},
  editor    = {Warstadt, Alex  and
               Mueller, Aaron  and
               Choshen, Leshem  and
               Wilcox, Ethan  and
               Zhuang, Chengxu  and
               Ciro, Juan  and
               Mosquera, Rafael  and
               Paranjabe, Bhargavi  and
               Williams, Adina  and
               Linzen, Tal  and
               Cotterell, Ryan},
  booktitle = {Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning},
  month     = dec,
  year      = {2023},
  address   = {Singapore},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.conll-babylm.24},
  doi       = {10.18653/v1/2023.conll-babylm.24},
  pages     = {279--289}
}

@article{touvron-2023-llama,
  title   = {Llama 2: Open foundation and fine-tuned chat models},
  author  = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal = {arXiv preprint arXiv:2307.09288},
  year    = {2023}
}

@article{trott-2023-large,
  title     = {Do large language models know what humans know?},
  author    = {Trott, Sean and Jones, Cameron and Chang, Tyler and Michaelov, James and Bergen, Benjamin},
  journal   = {Cognitive Science},
  year      = {2023},
  publisher = {Wiley-Blackwell Publishing Ltd.}
}

@inproceedings{ustun-etal-2018-characters,
  title     = {Characters or Morphemes: How to Represent Words?},
  author    = {{\"U}st{\"u}n, Ahmet  and
               Kurfal{\i}, Murathan  and
               Can, Burcu},
  editor    = {Augenstein, Isabelle  and
               Cao, Kris  and
               He, He  and
               Hill, Felix  and
               Gella, Spandana  and
               Kiros, Jamie  and
               Mei, Hongyuan  and
               Misra, Dipendra},
  booktitle = {Proceedings of the Third Workshop on Representation Learning for {NLP}},
  month     = jul,
  year      = {2018},
  address   = {Melbourne, Australia},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/W18-3019},
  doi       = {10.18653/v1/W18-3019},
  pages     = {144--153}
}

@article{van1977some,
  title     = {Some interactional aspects of language acquisition},
  author    = {Van der Geest, Ton},
  journal   = {Talking to children: Language input and acquisition},
  pages     = {89--107},
  year      = {1977},
  publisher = {Cambridge University Press Cambridge}
}

@article{Venkataraman2001,
  abstract      = {A statistical model for segmentation and word discovery in continuous speech is presented. An incremental unsupervised learning algorithm to infer word boundaries based on this model is described. Results are also presented of empirical tests showing that the algorithm is competitive with other models that have been used for similar tasks.},
  archiveprefix = {arXiv},
  arxivid       = {cs/0111065},
  author        = {Venkataraman, Anand},
  doi           = {10.1162/089120101317066113},
  eprint        = {0111065},
  issn          = {08912017},
  journal       = {Computational Linguistics},
  month         = {mar},
  number        = {3},
  pages         = {350--372},
  primaryclass  = {cs},
  publisher     = {MIT Press Journals},
  title         = {{A statistical model for word discovery in transcribed speech}},
  url           = {https://www.mitpressjournals.org/doix/abs/10.1162/089120101317066113},
  volume        = {27},
  year          = {2001}
}

@inproceedings{wang-etal-2018-glue,
  title     = {{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author    = {Wang, Alex  and
               Singh, Amanpreet  and
               Michael, Julian  and
               Hill, Felix  and
               Levy, Omer  and
               Bowman, Samuel},
  editor    = {Linzen, Tal  and
               Chrupa{\l}a, Grzegorz  and
               Alishahi, Afra},
  booktitle = {Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}},
  month     = nov,
  year      = {2018},
  address   = {Brussels, Belgium},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/W18-5446},
  doi       = {10.18653/v1/W18-5446},
  pages     = {353--355}
}

@inproceedings{wang-etal-2019-superglue,
  author    = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {},
  title     = {{SuperGLUE}: A Stickier Benchmark for General-Purpose Language Understanding Systems},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2019/file/4496bf24afe7fab6f046bf4923da8de6-Paper.pdf},
  volume    = {32},
  year      = {2019}
}

@article{warstadt-2020-blimp,
  title     = {{BLiMP}: The benchmark of linguistic minimal pairs for English},
  author    = {Warstadt, Alex and Parrish, Alicia and Liu, Haokun and Mohananey, Anhad and Peng, Wei and Wang, Sheng-Fu and Bowman, Samuel R},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {8},
  pages     = {377--392},
  year      = {2020},
  publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@incollection{warstadt-2022-artificial,
  title     = {What artificial neural networks can tell us about human language acquisition},
  author    = {Warstadt, Alex and Bowman, Samuel R},
  booktitle = {Algebraic structures in natural language},
  pages     = {17--60},
  year      = {2022},
  publisher = {CRC Press}
}


@inproceedings{warstadt-2023-babylm-findings,
  title     = {Findings of the {B}aby{LM} Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora},
  author    = {Warstadt, Alex  and
               Mueller, Aaron  and
               Choshen, Leshem  and
               Wilcox, Ethan  and
               Zhuang, Chengxu  and
               Ciro, Juan  and
               Mosquera, Rafael  and
               Paranjabe, Bhargavi  and
               Williams, Adina  and
               Linzen, Tal  and
               Cotterell, Ryan},
  booktitle = {Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning},
  month     = dec,
  year      = {2023},
  address   = {Singapore},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.conll-babylm.1},
  doi       = {10.18653/v1/2023.conll-babylm.1},
  pages     = {1--34}
}



@article{warstadt-etal-2020-blimp-benchmark,
  title     = {{BL}i{MP}: The Benchmark of Linguistic Minimal Pairs for {E}nglish},
  author    = {Warstadt, Alex  and
               Parrish, Alicia  and
               Liu, Haokun  and
               Mohananey, Anhad  and
               Peng, Wei  and
               Wang, Sheng-Fu  and
               Bowman, Samuel R.},
  editor    = {Johnson, Mark  and
               Roark, Brian  and
               Nenkova, Ani},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {8},
  year      = {2020},
  address   = {Cambridge, MA},
  publisher = {MIT Press},
  url       = {https://aclanthology.org/2020.tacl-1.25},
  doi       = {10.1162/tacl_a_00321},
  pages     = {377--392},
  abstract  = {We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP),1 a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,000 minimal pairs{---}that is, pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. We generate the data according to linguist-crafted grammar templates, and human aggregate agreement with the labels is 96.4{\%}. We evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair. We find that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands.}
}


@article{wells1992standard,
  title   = {Standard computer-compatible transcription},
  author  = {Wells, John and Barry, William and Grice, Martine and Fourcin, Adrian and Gibbon, Dafydd},
  journal = {Esprit project 2589 (SAM), Doc. no. SAM-UCL},
  volume  = {37},
  pages   = {64},
  year    = {1992}
}

@incollection{wightman1997aligner,
  title     = {The aligner: Text-to-speech alignment using Markov models},
  author    = {Wightman, Colin W and Talkin, David T},
  booktitle = {Progress in speech synthesis},
  pages     = {313--323},
  year      = {1997},
  publisher = {Springer}
}

@article{wilcox2025bigger,
  title    = {Bigger is not always better: {The} importance of human-scale language modeling for psycholinguistics},
  volume   = {144},
  issn     = {0749-596X},
  url      = {https://www.sciencedirect.com/science/article/pii/S0749596X25000439},
  doi      = {https://doi.org/10.1016/j.jml.2025.104650},
  abstract = {When trained to place high probability on a training corpus, neural network language models can learn a surprising amount about language. Recent work has demonstrated that large performance improvements can arise from simply increasing, i.e., scaling, the size of the corpora they are trained on and the number of parameters in those models. Accordingly, many contemporary systems are trained on trillions of words. While largely beneficial to performance on language applications, scaling has several downsides for both computational psycholinguistics and natural language processing research. We discuss the scientific challenges presented by the scaling paradigm, as well as the benefits that would result from language models that can learn from human-scale data. In the second half of this paper, we report on findings from a recent effort to bring about human-scale language model pretraining: the first iteration of the BabyLM Challenge, a shared task organized by the authors that invited participants to train a language model on 100 million words or less. The challenge produced several concrete best practices for practitioners interested in small-scale language modeling. For cognitive scientists, the challenge demonstrated that robust linguistic generalizations can be learned by models trained on a human-scale dataset, though this is not yet achieved through cognitively plausible mechanisms. Furthermore, it established a population of “BabyLMs” that are all effective at data-efficient language learning. Studying such models can help us identify hypotheses for the computational mechanisms that underlie human language acquisition.},
  journal  = {Journal of Memory and Language},
  author   = {Wilcox, Ethan Gotlieb and Hu, Michael Y. and Mueller, Aaron and Warstadt, Alex and Choshen, Leshem and Zhuang, Chengxu and Williams, Adina and Cotterell, Ryan and Linzen, Tal},
  year     = {2025},
  keywords = {Cognitive modeling, Connectionist networks, Language acquisition, Language modeling, Psycholinguistics, Scaling},
  pages    = {104650}
}

@inproceedings{wolf-etal-2020-transformers,
  title     = {Transformers: {S}tate-of-the-Art Natural Language Processing},
  author    = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and {von Platen}, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
  year      = 2020,
  month     = oct,
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{System Demonstrations}}},
  publisher = {{Association for Computational Linguistics}},
  address   = {{Online}},
  pages     = {38--45},
  doi       = {10.18653/v1/2020.emnlp-demos.6},
  url       = {https://aclanthology.org/2020.emnlp-demos.6}
}

@article{xue-2022-byt5,
  title     = {Byt5: Towards a token-free future with pre-trained byte-to-byte models},
  author    = {Xue, Linting and Barua, Aditya and Constant, Noah and Al-Rfou, Rami and Narang, Sharan and Kale, Mihir and Roberts, Adam and Raffel, Colin},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {10},
  pages     = {291--306},
  year      = {2022},
  publisher = {MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@inproceedings{yadavalli2023slabert,
  title     = {{SLABERT} Talk Pretty One Day: Modeling Second Language Acquisition with {BERT}},
  author    = {Yadavalli, Aditya and Yadavalli, Alekhya and Tobin, Vera},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages     = {11763--11777},
  year      = {2023}
}


@inproceedings{ye-2021-pngbert,
  title     = {PnG BERT: Augmented BERT on Phonemes and Graphemes for Neural TTS},
  author    = {Ye Jia and Heiga Zen (Byungha Chun) and Jonathan Shen and Yu Zhang and Yonghui Wu},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.15060},
  booktitle = {Interspeech}
}

@inproceedings{yedetore-etal-2023-poor,
  title     = {How poor is the stimulus? Evaluating hierarchical generalization in neural networks trained on child-directed speech},
  author    = {Yedetore, Aditya  and
               Linzen, Tal  and
               Frank, Robert  and
               McCoy, R. Thomas},
  editor    = {Rogers, Anna  and
               Boyd-Graber, Jordan  and
               Okazaki, Naoaki},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = jul,
  year      = {2023},
  address   = {Toronto, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.acl-long.521/},
  doi       = {10.18653/v1/2023.acl-long.521},
  pages     = {9370--9393},
  abstract  = {When acquiring syntax, children consistently choose hierarchical rules over competing non-hierarchical possibilities. Is this preference due to a learning bias for hierarchical structure, or due to more general biases that interact with hierarchical cues in children`s linguistic input? We explore these possibilities by training LSTMs and Transformers - two types of neural networks without a hierarchical bias - on data similar in quantity and content to children`s linguistic input: text from the CHILDES corpus. We then evaluate what these models have learned about English yes/no questions, a phenomenon for which hierarchical structure is crucial. We find that, though they perform well at capturing the surface statistics of child-directed speech (as measured by perplexity), both model types generalize in a way more consistent with an incorrect linear rule than the correct hierarchical rule. These results suggest that human-like generalization from text alone requires stronger biases than the general sequence-processing biases of standard neural network architectures.}
}

@article{yuan2008speaker,
  title     = {Speaker identification on the SCOTUS corpus},
  author    = {Yuan, Jiahong and Liberman, Mark and others},
  journal   = {Journal of the Acoustical Society of America},
  volume    = {123},
  number    = {5},
  pages     = {3878},
  year      = {2008},
  publisher = {[New York: Acoustical Society of America]}
}


@inproceedings{zellers-2019-hellaswag,
  title     = {HellaSwag: Can a Machine Really Finish Your Sentence?},
  author    = {Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages     = {4791--4800},
  year      = {2019}
}

@inproceedings{zellers-etal-2019-hellaswag,
  title     = {{H}ella{S}wag: Can a Machine Really Finish Your Sentence?},
  author    = {Zellers, Rowan  and
               Holtzman, Ari  and
               Bisk, Yonatan  and
               Farhadi, Ali  and
               Choi, Yejin},
  editor    = {Korhonen, Anna  and
               Traum, David  and
               M{\`a}rquez, Llu{\'\i}s},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2019},
  address   = {Florence, Italy},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/P19-1472},
  pages     = {4791--4800}
}

@inproceedings{zhu-etal-2024-taste,
  title     = {The taste of {IPA}: Towards open-vocabulary keyword spotting and forced alignment in any language},
  author    = {Zhu, Jian  and
               Yang, Changbing  and
               Samir, Farhan  and
               Islam, Jahurul},
  editor    = {Duh, Kevin  and
               Gomez, Helena  and
               Bethard, Steven},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  month     = jun,
  year      = {2024},
  address   = {Mexico City, Mexico},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.naacl-long.43},
  doi       = {10.18653/v1/2024.naacl-long.43},
  pages     = {750--772},
  abstract  = {In this project, we demonstrate that phoneme-based models for speech processing can achieve strong crosslinguistic generalizability to unseen languages. We curated the IPAPACK, a massively multilingual speech corpora with phonemic transcriptions, encompassing more than 115 languages from diverse language families, selectively checked by linguists. Based on the IPAPACK, we propose CLAP-IPA, a multi-lingual phoneme-speech contrastive embedding model capable of open-vocabulary matching between arbitrary speech signals and phonemic sequences. The proposed model was tested on 95 unseen languages, showing strong generalizability across languages. Temporal alignments between phonemes and speech signals also emerged from contrastive training, enabling zeroshot forced alignment in unseen languages. We further introduced a neural forced aligner IPA-ALIGNER by finetuning CLAP-IPA with the Forward-Sum loss to learn better phone-to-audio alignment. Evaluation results suggest that IPA-ALIGNER can generalize to unseen languages without adaptation.}
}

@inproceedings{Zhu2022,
  author    = {Zhu, J. and Zhang, C. and Jurgens, D.},
  title     = {{ByT5} Model for Massively Multilingual Grapheme-to-Phoneme Conversion},
  booktitle = {Proceedings of INTERSPEECH 2022},
  year      = {2022},
  pages     = {446--450},
  doi       = {10.21437/Interspeech.2022-538}
}§

@incollection{zue1996transcription,
  title     = {Transcription and alignment of the TIMIT database},
  author    = {Zue, Victor W and Seneff, Stephanie},
  booktitle = {Recent Research Towards Advanced Man-Machine Interface Through Spoken Language},
  pages     = {515--525},
  year      = {1996},
  publisher = {Elsevier}
}


@book{zipf_human_1949,
  title={Human behavior and the principle of least effort: An introduction to human ecology},
  author={Zipf, George Kingsley},
  year={1949},
  publisher={Addison-Wesley Press}
}

@inproceedings{cucerzan_spelling_2004,
  title={Spelling correction as an iterative process that exploits the collective knowledge of web users},
  author={Cucerzan, Silviu and Brill, Eric},
  booktitle={Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing},
  pages={293--300},
  year={2004}
}

@article{katz2003estimation,
  title={Estimation of probabilities from sparse data for the language model component of a speech recognizer},
  author={Katz, Slava},
  journal={IEEE transactions on acoustics, speech, and signal processing},
  volume={35},
  number={3},
  pages={400--401},
  year={2003},
  publisher={IEEE}
}

@article{ney1994structuring,
  title={On structuring probabilistic dependences in stochastic language modelling},
  author={Ney, Hermann and Essen, Ute and Kneser, Reinhard},
  journal={Computer Speech \& Language},
  volume={8},
  number={1},
  pages={1--38},
  year={1994},
  publisher={Elsevier}
}

@manual{young2006htk,
  title={The HTK Book (for HTK Version 3.4)},
  author={Young, Steve and Evermann, Gunnar and Gales, Mark and Hain, Thomas and Kershaw, Dan and Liu, Xunying and Moore, Gareth and Odell, Julian and Ollason, Dave and Povey, Daniel and others},
  year={2006},
  organization={Cambridge University Engineering Department}
}

@book{jurafsky2009speech,
  title={Speech and Language Processing},
  author={Jurafsky, Daniel and Martin, James H.},
  edition={2nd},
  year={2009},
  publisher={Pearson Prentice Hall}
}

@inproceedings{mikolov2010recurrent,
  title={Recurrent neural network based language model},
  author={Mikolov, Tom{\'a}{\v{s}} and Karafi{\'a}t, Martin and Burget, Luk{\'a}{\v{s}} and {\v{C}}ernock{\`y}, Jan and Khudanpur, Sanjeev},
  booktitle={Proc. Interspeech 2010},
  pages={1045--1048},
  year={2010}
}

@inproceedings{sundermeyer2012lstm,
  title={Lstm neural networks for language modeling.},
  author={Sundermeyer, Martin and Schl{\"u}ter, Ralf and Ney, Hermann},
  booktitle={Interspeech},
  volume={2012},
  pages={194--197},
  year={2012}
}

@article{bengio2000neural,
 author = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 pages = {},
 publisher = {MIT Press},
 title = {A Neural Probabilistic Language Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf},
 volume = {13},
 year = {2000}
}


@inproceedings{mikolov_distributed_2013,
 author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Distributed Representations of Words and Phrases and their Compositionality},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf},
 volume = {26},
 year = {2013}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@inproceedings{devlin2019bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers)},
  pages={4171--4186},
  year={2019}
}

@misc{radford2018gpt1,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018},
  note={Technical report, OpenAI},
  howpublished={\url{https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf}}
}

@inproceedings{brown2020gpt3,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{liu2019roberta,
    title = {{RoBERTa}: A Robustly Optimized {BERT} Pretraining Approach}, 
    author = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
    year = {2019},
    volume = {arXiv:1907.11692},
    journal = {arXiv preprint arXiv:1907.11692},
    url = {https://arxiv.org/abs/1907.11692},
}

@inproceedings{strubell-etal-2019-energy,
    title = "Energy and Policy Considerations for Deep Learning in {NLP}",
    author = "Strubell, Emma  and
      Ganesh, Ananya  and
      McCallum, Andrew",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1355/",
    doi = "10.18653/v1/P19-1355",
    pages = "3645--3650",
    abstract = "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice."
}

@misc{patterson2021carbonemissionslargeneural,
      title={Carbon Emissions and Large Neural Network Training}, 
      author={David Patterson and Joseph Gonzalez and Quoc Le and Chen Liang and Lluis-Miquel Munguia and Daniel Rothchild and David So and Maud Texier and Jeff Dean},
      year={2021},
      eprint={2104.10350},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2104.10350}, 
}

@misc{luccioni2022estimatingcarbonfootprintbloom,
      title={Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model}, 
      author={Alexandra Sasha Luccioni and Sylvain Viguier and Anne-Laure Ligozat},
      year={2022},
      eprint={2211.02001},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.02001}, 
}

@inproceedings{bender2021parrots,
author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445922},
doi = {10.1145/3442188.3445922},
abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {610–623},
numpages = {14},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@book{bird2009nltk,
  title={Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit},
  author={Bird, Steven and Klein, Ewan and Loper, Edward},
  year={2009},
  publisher={O'Reilly Media, Inc.},
  isbn={9780596516499}
}

@InProceedings{habash2009,
  author = {Nizar Habash, Owen Rambow and Ryan Roth},
  title = {MADA+TOKAN: A Toolkit for Arabic Tokenization, Diacritization, Morphological Disambiguation, POS Tagging, Stemming and Lemmatization},
  booktitle = {Proceedings of the Second International Conference on Arabic Language Resources and Tools},
  year = {2009},
  month = {April},
  date = {22-23},
  address = {Cairo, Egypt},
  editor = {Khalid Choukri and Bente Maegaard},
  publisher = {The MEDAR Consortium},
  isbn = {2-9517408-5-9},
  language = {english}
  }

@incollection{creutz2005unsupervised,
  title={Unsupervised morpheme segmentation and morphology induction from text corpora using Morfessor 1.0},
  author={Creutz, Mathias and Lagus, Krista},
  booktitle={Unsupervised morpheme segmentation and morphology induction from text corpora using Morfessor 1.0},
  year={2005},
  publisher={Helsinki University of Technology}
}

@inproceedings{botha2014compositional,
  title={Compositional morphology for word representations and language modelling},
  author={Botha, Jan and Blunsom, Phil},
  booktitle={International Conference on Machine Learning},
  pages={1899--1907},
  year={2014},
  organization={PMLR}
}

@inproceedings{vania2017morphology,
title = "From characters to words to in between: Do we capture morphology?",
abstract = "Words can be represented by composing the representations of subword units such as word segments, characters, and/or character n-grams. While such representations are effective and may capture the morphological regularities of words, they have not been systematically compared, and it is not understood how they interact with different morphological typologies. On a language modeling task, we present experiments that systematically vary (1) the basic unit of representation, (2) the composition of these representations, and (3) the morphological typology of the language modeled. Our results extend previous findings that character representations are effective across typologies, and we find that a previously unstudied combination of character trigram representations composed with bi-LSTMs outperforms most others. But we also find room for improvement: none of the character-level models match the predictive accuracy of a model with access to true morphological analyses, even when learned from an order of magnitude more data.",
author = "Clara Vania and Adam Lopez",
year = "2017",
month = aug,
day = "4",
doi = "10.18653/v1/P17-1184",
language = "English",
isbn = "978-1-945626-75-3 ",
pages = "2016--2027",
booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
publisher = "Association for Computational Linguistics",
note = "55th Annual Meeting of the Association for Computational Linguistics, ACL 2017 ; Conference date: 30-07-2017 Through 04-08-2017",
}

@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}

@article{gage1994new,
  title={A new algorithm for data compression},
  author={Gage, Philip},
  journal={The C Users Journal},
  volume={12},
  number={2},
  pages={23--38},
  year={1994},
  publisher={R \& D Publications, Inc. Lawrence, KS, USA}
}

@inproceedings{zheng-saparov-2023-noisy,
    title = "Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis",
    author = "Zheng, Hongyi  and
      Saparov, Abulhair",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.277/",
    doi = "10.18653/v1/2023.emnlp-main.277",
    pages = "4560--4568",
    abstract = "Recent advances in prompt engineering enable large language models (LLMs) to solve multi-hop logical reasoning problems with impressive accuracy. However, there is little existing work investigating the robustness of LLMs with few-shot prompting techniques. Therefore, we introduce a systematic approach to test the robustness of LLMs in multi-hop reasoning tasks via domain-agnostic perturbations. We include perturbations at multiple levels of abstractions (e.g. lexical perturbations such as typos, and semantic perturbations such as the inclusion of intermediate reasoning steps in the questions) to conduct behavioral analysis on the LLMs. Throughout our experiments, we find that models are more sensitive to certain perturbations such as replacing words with their synonyms. We also demonstrate that increasing the proportion of perturbed exemplars in the prompts improves the robustness of few-shot prompting methods."
}

@article{baevski2020wav2vec,
  title={wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author={Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={12449--12460},
  year={2020}
}

@inproceedings{radford2023robust,
  title={Robust speech recognition via large-scale weak supervision},
  author={Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  booktitle={International conference on machine learning},
  pages={28492--28518},
  year={2023},
  organization={PMLR}
}

@inproceedings{feng-etal-2020-codebert,
    title = "{C}ode{BERT}: A Pre-Trained Model for Programming and Natural Languages",
    author = "Feng, Zhangyin  and
      Guo, Daya  and
      Tang, Duyu  and
      Duan, Nan  and
      Feng, Xiaocheng  and
      Gong, Ming  and
      Shou, Linjun  and
      Qin, Bing  and
      Liu, Ting  and
      Jiang, Daxin  and
      Zhou, Ming",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.139/",
    doi = "10.18653/v1/2020.findings-emnlp.139",
    pages = "1536--1547",
    abstract = "We present CodeBERT, a bimodal pre-trained model for programming language (PL) and natural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language code search, code documentation generation, etc. We develop CodeBERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both ``bimodal'' data of NL-PL pairs and ``unimodal data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NLPL probing."
}

@inproceedings{beltagy2019scibert,
  title={SciBERT: A Pretrained Language Model for Scientific Text},
  author={Beltagy, Iz and Lo, Kyle and Cohan, Arman},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={3615--3620},
  year={2019}
}

@inproceedings{chalkidis2020legal,
  title={LEGAL-BERT: The Muppets straight out of Law School},
  author={Chalkidis, Ilias and Fergadiotis, Manos and Malakasiotis, Prodromos and Aletras, Nikolaos and Androutsopoulos, Ion},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={2898--2904},
  year={2020}
}

@article{lee2020biobert,
  title={BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
  author={Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
  journal={Bioinformatics},
  volume={36},
  number={4},
  pages={1234--1240},
  year={2020},
  publisher={Oxford University Press}
}

@inproceedings{alsentzer2019publicly,
  title={Publicly Available Clinical BERT Embeddings},
  author={Alsentzer, Emily and Murphy, John and Boag, William and Weng, Wei-Hung and Jindi, Di and Naumann, Tristan and McDermott, Matthew},
  booktitle={Proceedings of the 2nd Clinical Natural Language Processing Workshop},
  pages={72--78},
  year={2019}
}

@inproceedings{chang2024multilinguality,
  title={When Is Multilinguality a Curse? Language Modeling for 250 High-and Low-Resource Languages},
  author={Chang, Tyler and Arnett, Catherine and Tu, Zhuowen and Bergen, Ben},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={4074--4096},
  year={2024}
}

@article{chang2024goldfish,
  title={Goldfish: Monolingual language models for 350 languages},
  author={Chang, Tyler A and Arnett, Catherine and Tu, Zhuowen and Bergen, Benjamin K},
  journal={arXiv preprint arXiv:2408.10441},
  year={2024}
}

@inproceedings{gerz-etal-2018-relation,
    title = "On the Relation between Linguistic Typology and (Limitations of) Multilingual Language Modeling",
    author = "Gerz, Daniela  and
      Vuli{\'c}, Ivan  and
      Ponti, Edoardo Maria  and
      Reichart, Roi  and
      Korhonen, Anna",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1029/",
    doi = "10.18653/v1/D18-1029",
    pages = "316--327",
    abstract = "A key challenge in cross-lingual NLP is developing general language-independent architectures that are equally applicable to any language. However, this ambition is largely hampered by the variation in structural and semantic properties, i.e. the typological profiles of the world{'}s languages. In this work, we analyse the implications of this variation on the language modeling (LM) task. We present a large-scale study of state-of-the art n-gram based and neural language models on 50 typologically diverse languages covering a wide variety of morphological systems. Operating in the full vocabulary LM setup focused on word-level prediction, we demonstrate that a coarse typology of morphological systems is predictive of absolute LM performance. Moreover, fine-grained typological features such as exponence, flexivity, fusion, and inflectional synthesis are borne out to be responsible for the proliferation of low-frequency phenomena which are organically difficult to model by statistical architectures, or for the meaning ambiguity of character n-grams. Our study strongly suggests that these features have to be taken into consideration during the construction of next-level language-agnostic LM architectures, capable of handling morphologically complex languages such as Tamil or Korean."
}

@misc{fittschen2025pretraininglanguagemodelsdiachronic,
      title={Pretraining Language Models for Diachronic Linguistic Change Discovery}, 
      author={Elisabeth Fittschen and Sabrina Li and Tom Lippincott and Leshem Choshen and Craig Messner},
      year={2025},
      eprint={2504.05523},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.05523}, 
}

@article{gerz2018,
    author = {Gerz, Daniela and Vulić, Ivan and Ponti, Edoardo and Naradowsky, Jason and Reichart, Roi and Korhonen, Anna},
    title = {Language Modeling for Morphologically Rich Languages: Character-Aware Modeling for Word-Level Prediction},
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {6},
    pages = {451-465},
    year = {2018},
    month = {07},
    abstract = {Neural architectures are prominent in the construction of language models (LMs). However, word-level prediction is typically agnostic of subword-level information (characters and character sequences) and operates over a closed vocabulary, consisting of a limited word set. Indeed, while subword-aware models boost performance across a variety of NLP tasks, previous work did not evaluate the ability of these models to assist next-word prediction in language modeling tasks. Such subword-level informed models should be particularly effective for morphologically-rich languages (MRLs) that exhibit high type-to-token ratios. In this work, we present a large-scale LM study on 50 typologically diverse languages covering a wide variety of morphological systems, and offer new LM benchmarks to the community, while considering subword-level information. The main technical contribution of our work is a novel method for injecting subword-level information into semantic word vectors, integrated into the neural language modeling training, to facilitate word-level prediction. We conduct experiments in the LM setting where the number of infrequent words is large, and demonstrate strong perplexity gains across our 50 languages, especially for morphologically-rich languages. Our code and data sets are publicly available.},
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00032},
    url = {https://doi.org/10.1162/tacl\_a\_00032},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00032/1567630/tacl\_a\_00032.pdf},
}





@article{padovani2025child,
  title={Child-Directed Language Does Not Consistently Boost Syntax Learning in Language Models},
  author={Padovani, Francesca and Jumelet, Jaap and Matusevych, Yevgen and Bisazza, Arianna},
  journal={arXiv preprint arXiv:2505.23689},
  year={2025}
}

@article{hu2022lora,
  title={Lora: Low-rank adaptation of large language models.},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  journal={ICLR},
  volume={1},
  number={2},
  pages={3},
  year={2022}
}

@article{grattafiori2024llama,
  title={The llama 3 herd of models},
  author={Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{wei2022emergent,
  title={Emergent Abilities of Large Language Models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={Transactions on Machine Learning Research},
  year={2022}
}

@inproceedings{ganguli2022predictability,
  title={Predictability and surprise in large generative models},
  author={Ganguli, Deep and Hernandez, Danny and Lovitt, Liane and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Dassarma, Nova and Drain, Dawn and Elhage, Nelson and others},
  booktitle={Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={1747--1764},
  year={2022}
}

@inproceedings{warstadt2023findings,
  title={Findings of the BabyLM Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora},
  author={Warstadt, Alex and Mueller, Aaron and Choshen, Leshem and Wilcox, Ethan and Zhuang, Chengxu and Ciro, Juan and Mosquera, Rafael and Paranjabe, Bhargavi and Williams, Adina and Linzen, Tal and others},
  booktitle={Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning},
  pages={1--34},
  year={2023}
}

@inproceedings{charpentier2024bert,
  title={BERT or GPT: why not both?},
  author={Charpentier, Lucas Georges Gabriel and Samuel, David},
  booktitle={The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning},
  pages={262--283},
  year={2024}
}

@inproceedings{conneau2020unsupervised,
  title={Unsupervised Cross-lingual Representation Learning at Scale},
  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, {\'E}douard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={8440--8451},
  year={2020}
}

@article{huang2023lawyer,
  title={Lawyer llama technical report},
  author={Huang, Quzhe and Tao, Mingxu and Zhang, Chen and An, Zhenwei and Jiang, Cong and Chen, Zhibin and Wu, Zirui and Feng, Yansong},
  journal={arXiv preprint arXiv:2305.15062},
  year={2023}
}

@inproceedings{hale-2001-probabilistic,
    title = "A Probabilistic {E}arley Parser as a Psycholinguistic Model",
    author = "Hale, John",
    booktitle = "Second Meeting of the North {A}merican Chapter of the Association for Computational Linguistics",
    year = "2001",
    url = "https://aclanthology.org/N01-1021/"
}

@article{levy2008expectation,
  title={Expectation-based syntactic comprehension},
  author={Levy, Roger},
  journal={Cognition},
  volume={106},
  number={3},
  pages={1126--1177},
  year={2008},
  publisher={Elsevier}
}

@inproceedings{wilcox2020predictive,
  title={On the Predictive Power of Neural Language Models for Human Real-Time Comprehension Behavior},
  author={Wilcox, Ethan G and Gauthier, Jon and Hu, Jennifer and Qian, Peng and Levy, Roger P},
  booktitle={Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume={42},
  year={2020}
}

@article{schrimpf2021neural,
  title={The neural architecture of language: Integrative modeling converges on predictive processing},
  author={Schrimpf, Martin and Blank, Idan Asher and Tuckute, Greta and Kauf, Carina and Hosseini, Eghbal A and Kanwisher, Nancy and Tenenbaum, Joshua B and Fedorenko, Evelina},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={45},
  pages={e2105646118},
  year={2021},
  publisher={National Academy of Sciences}
}

@article{gold1967language,
  title={Language identification in the limit},
  author={Gold, E Mark},
  journal={Information and control},
  volume={10},
  number={5},
  pages={447--474},
  year={1967},
  publisher={Elsevier}
}

@article{macdonald1994lexical,
  title={The lexical nature of syntactic ambiguity resolution.},
  author={MacDonald, Maryellen C and Pearlmutter, Neal J and Seidenberg, Mark S},
  journal={Psychological review},
  volume={101},
  number={4},
  pages={676},
  year={1994},
  publisher={American Psychological Association}
}

@inproceedings{futrell2019neural,
  title={Neural language models as psycholinguistic subjects: Representations of syntactic state},
  author={Futrell, Richard and Wilcox, Ethan and Morita, Takashi and Qian, Peng and Ballesteros, Miguel and Levy, Roger},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={32--42},
  year={2019}
}

@inproceedings{oh2024frequency,
  title={Frequency Explains the Inverse Correlation of Large Language Models’ Size, Training Data Amount, and Surprisal’s Fit to Reading Times},
  author={Oh, Byung-Doh and Yue, Shisen and Schuler, William},
  booktitle={Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2644--2663},
  year={2024}
}

@article{shain2024large,
  title={Large-scale evidence for logarithmic effects of word predictability on reading time},
  author={Shain, Cory and Meister, Clara and Pimentel, Tiago and Cotterell, Ryan and Levy, Roger},
  journal={Proceedings of the National Academy of Sciences},
  volume={121},
  number={10},
  pages={e2307876121},
  year={2024},
  publisher={National Academy of Sciences}
}

@article{gilkerson2017mapping,
  title={Mapping the early language environment using all-day recordings and automated analysis},
  author={Gilkerson, Jill and Richards, Jeffrey A and Warren, Steven F and Montgomery, Judith K and Greenwood, Charles R and Kimbrough Oller, D and Hansen, John HL and Paul, Terrance D},
  journal={American journal of speech-language pathology},
  volume={26},
  number={2},
  pages={248--265},
  year={2017},
  publisher={American Speech-Language-Hearing Association}
}

@inproceedings{linzen-2020-accelerate,
    title = "How Can We Accelerate Progress Towards Human-like Linguistic Generalization?",
    author = "Linzen, Tal",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.465/",
    doi = "10.18653/v1/2020.acl-main.465",
    pages = "5210--5217",
    abstract = "This position paper describes and critiques the Pretraining-Agnostic Identically Distributed (PAID) evaluation paradigm, which has become a central tool for measuring progress in natural language understanding. This paradigm consists of three stages: (1) pre-training of a word prediction model on a corpus of arbitrary size; (2) fine-tuning (transfer learning) on a training set representing a classification task; (3) evaluation on a test set drawn from the same distribution as that training set. This paradigm favors simple, low-bias architectures, which, first, can be scaled to process vast amounts of data, and second, can capture the fine-grained statistical properties of a particular data set, regardless of whether those properties are likely to generalize to examples of the task outside the data set. This contrasts with humans, who learn language from several orders of magnitude less data than the systems favored by this evaluation paradigm, and generalize to new tasks in a consistent way. We advocate for supplementing or replacing PAID with paradigms that reward architectures that generalize as quickly and robustly as humans."
}

@inproceedings{uzan-etal-2024-greed,
	title        = {Greed is All You Need: An Evaluation of Tokenizer Inference Methods},
	author       = {Uzan, Omri  and Schmidt, Craig W.  and Tanner, Chris  and Pinter, Yuval},
	year         = {2024},
	month        = aug,
	booktitle    = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Bangkok, Thailand},
	pages        = {813--822},
	doi          = {10.18653/v1/2024.acl-short.73},
	url          = {https://aclanthology.org/2024.acl-short.73/},
	editor       = {Ku, Lun-Wei  and Martins, Andre  and Srikumar, Vivek},
}

@article{kozma-etal-2024-theoretical,
	title        = {Theoretical Analysis of Byte-Pair Encoding},
	author       = {L\'{a}szl\'{o} Kozma and Johannes Voderholzer},
	year         = {2024},
	journal      = {arXiv preprint 2411.08671},
	url          = {https://arxiv.org/abs/2411.08671},
}

@article{whittington2024tokenisation,
	title        = {Tokenisation is {NP}-Complete},
	author       = {Philip Whittington and Gregor Bachmann and Tiago Pimentel},
	year         = {2024},
	journal      = {arxiv preprint 2412.15210},
	url          = {https://arxiv.org/abs/2412.15210},
}

@inproceedings{limisiewicz-etal-2024-myte,
    title = "{MYTE}: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling",
    author = "Limisiewicz, Tomasz  and
      Blevins, Terra  and
      Gonen, Hila  and
      Ahia, Orevaoghene  and
      Zettlemoyer, Luke",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.804/",
    doi = "10.18653/v1/2024.acl-long.804",
    pages = "15059--15076",
    abstract = "A major consideration in multilingual language modeling is how to best represent languages with diverse vocabularies and scripts.Although contemporary text encoding methods cover most of the world{'}s writing systems, they exhibit bias towards the high-resource languages of the Global West. As a result, texts of underrepresented languages tend to be segmented into long sequences of linguistically meaningless units. To address the disparities, we introduce a new paradigm that encodes the same information with segments of consistent size across diverse languages. Our encoding convention (MYTE) is based on morphemes, as their inventories are more balanced across languages than characters, which are used in previous methods. We show that MYTE produces shorter encodings for all 99 analyzed languages, with the most notable improvements for non-European languages and non-Latin scripts. This, in turn, improves multilingual LM performance and diminishes the perplexity gap throughout diverse languages."
}

@inproceedings{bauwens-delobelle-2024-bpe,
    title = "{BPE}-knockout: Pruning Pre-existing {BPE} Tokenisers with Backwards-compatible Morphological Semi-supervision",
    author = "Bauwens, Thomas  and
      Delobelle, Pieter",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.324/",
    doi = "10.18653/v1/2024.naacl-long.324",
    pages = "5810--5832",
    abstract = "Byte-pair encoding (BPE) has become the default subword tokeniser in language models (LMs), allowing the representation of an infinite space of text with a finite set of units. Yet, BPE training is unsupervised, receiving no explicit information about a language{'}s morphology. This results in a subword vocabulary wherein many units are a concatenation of partial morphemes, preventing their formation as tokens. This, in turn, causes consistent intra-word patterns to be displayed inconsistently to downstream models, and bloats the vocabulary, hence requiring unnecessary embedding storage. In this paper, we address this issue by identifying blameworthy BPE merges and removing the resulting subwords from the BPE vocabulary, without impeding further use of merges that relied on them. We find that our method, BPE-knockout, is effective at making BPE{'}s segmentation positions adhere better to derivational and compound boundaries in English, Dutch and German, and improves token-based tasks in Dutch RoBERTa models, indicating that a tokeniser{'}s adherence to morphology impacts downstream models. We demonstrate the latter not only by training LMs from scratch, but also by continuing the pre-training of existing LMs. This proves promising, showing that suboptimal tokenisers can be remedied whilst salvaging training cost of downstream LMs."
}

@article{salhancopil2025,
  title={Linguistics in the Age of {L}anguage {M}odels: What can {C}ognitively-{I}nspired {L}anguage {M}odels offer to {L}inguistic {T}heory?},
  author={Salhan, Suchir},
  journal={Cambridge Occasional Papers in Linguistics},
  volume={17},
  pages={--},
  year={2025},
  publisher={Cambridge Occasional Papers in Linguistics},
  issn={2050-5949},
  url={https://www.mmll.cam.ac.uk/cambridge-occasional-papers-linguistics}
}

@article{mielke2021between,
  title={Between words and characters: A brief history of open-vocabulary modeling and tokenization in NLP},
  author={Mielke, Sabrina J and Alyafeai, Zaid and Salesky, Elizabeth and Raffel, Colin and Dey, Manan and Gall{\'e}, Matthias and Raja, Arun and Si, Chenglei and Lee, Wilson Y and Sagot, Beno{\^\i}t and others},
  journal={arXiv preprint arXiv:2112.10508},
  year={2021}
}

@inproceedings{shao2025beyond,
  title={Beyond Next Token Prediction: Patch-Level Training for Large Language Models},
  author={Shao, Chenze and Meng, Fandong and Zhou, Jie},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025}
}

@article{asgari2025morphbpe,
  title={MorphBPE: A Morpho-Aware Tokenizer Bridging Linguistic Complexity for Efficient LLM Training Across Morphologies},
  author={Asgari, Ehsaneddin and Kheir, Yassine El and Javaheri, Mohammad Ali Sadraei},
  journal={arXiv preprint arXiv:2502.00894},
  year={2025}
}

@article{kildeberg2025sm,
  title={From Sm $\{$$\backslash$o$\}$ r-re-br $\{$$\backslash$o$\}$ d to Subwords: Training LLMs on Danish, One Morpheme at a Time},
  author={Kildeberg, Mikkel Wildner and Schledermann, Emil Allerslev and Larsen, Nicolaj and van der Goot, Rob},
  journal={arXiv preprint arXiv:2504.01540},
  year={2025}
}

@inproceedings{yu2023megabyte,
  author={Yu, Lili and Simig, D{\'a}niel and Flaherty, Colin and Aghajanyan, Armen and Zettlemoyer, Luke and Lewis, Mike},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {78808--78823},
 publisher = {Curran Associates, Inc.},
 title = {MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/f8f78f8043f35890181a824e53a57134-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@article{gupta2019character,
  title={Character-based nmt with transformer},
  author={Gupta, Rohit and Besacier, Laurent and Dymetman, Marc and Gall{\'e}, Matthias},
  journal={arXiv preprint arXiv:1911.04997},
  year={2019}
}

@article{graves2013generating,
  title={Generating sequences with recurrent neural networks},
  author={Graves, Alex},
  journal={arXiv preprint arXiv:1308.0850},
  year={2013}
}

@article{pan2020morphological,
  title={Morphological word segmentation on agglutinative languages for neural machine translation},
  author={Pan, Yirong and Li, Xiao and Yang, Yating and Dong, Rui},
  journal={arXiv preprint arXiv:2001.01589},
  year={2020}
}

@inproceedings{wang2020neural,
  title={Neural machine translation with byte-level subwords},
  author={Wang, Changhan and Cho, Kyunghyun and Gu, Jiatao},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  pages={9154--9160},
  year={2020}
}

@inproceedings{schuster-nakajima-2012-voice,
	title        = {{J}apanese and {K}orean voice search},
	author       = {Schuster, Mike and Nakajima, Kaisuke},
	year         = {2012},
	booktitle    = {2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	pages        = {5149--5152},
	doi          = {10.1109/icassp.2012.6289079},
}

@misc{liu2025superbpespacetravellanguage,
      title={SuperBPE: Space Travel for Language Models}, 
      author={Alisa Liu and Jonathan Hayase and Valentin Hofmann and Sewoong Oh and Noah A. Smith and Yejin Choi},
      year={2025},
      eprint={2503.13423},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2503.13423}, 
}

@inproceedings{rust-etal-2021,
	title        = {How {Good} is {Your} {Tokenizer}? {On} the {Monolingual} {Performance} of {Multilingual} {Language} {Models}},
	shorttitle   = {How {Good} is {Your} {Tokenizer}?},
	author       = {Rust, Phillip and Pfeiffer, Jonas and Vuli\'{c}, Ivan and Ruder, Sebastian and Gurevych, Iryna},
	year         = {2021},
	month        = aug,
	booktitle    = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher    = {Association for Computational Linguistics},
	pages        = {3118--3135},
	doi          = {10.18653/v1/2021.acl-long.243},
	url          = {https://aclanthology.org/2021.acl-long.243/},
	urldate      = {2025-04-25},
	editor       = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
}

@inproceedings{wu-dredze-2019-beto,
    title = "Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of {BERT}",
    author = "Wu, Shijie  and
      Dredze, Mark",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1077/",
    doi = "10.18653/v1/D19-1077",
    pages = "833--844",
    abstract = "Pretrained contextual representation models (Peters et al., 2018; Devlin et al., 2018) have pushed forward the state-of-the-art on many NLP tasks. A new release of BERT (Devlin, 2018) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task. This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classification, NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and find mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language specific features, and measure factors that influence cross-lingual transfer."
}

@inproceedings{zouhar-etal-2023-tokenization,
	title        = {Tokenization and the Noiseless Channel},
	author       = {Zouhar, Vil{\'e}m  and Meister, Clara  and Gastaldi, Juan  and Du, Li  and Sachan, Mrinmaya  and Cotterell, Ryan},
	year         = {2023},
	month        = jul,
	booktitle    = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Toronto, Canada},
	pages        = {5184--5207},
	doi          = {10.18653/v1/2023.acl-long.284},
	url          = {https://aclanthology.org/2023.acl-long.284/},
	editor       = {Rogers, Anna  and Boyd-Graber, Jordan  and Okazaki, Naoaki},
}

@misc{acs2019exploring,
	title        = {Exploring {BERT}'s vocabulary},
	author       = {{\'A}cs, Judit},
	year         = {2019},
	url      = {https://juditacs.github.io/2019/02/19/bert-tokenization-stats.html},
	note         = {Accessed: May 01, 2025},
}


@inproceedings{arnett2025language,
  title={Why do language models perform worse for morphologically complex languages?},
  author={Arnett, Catherine and Bergen, Benjamin},
  booktitle={Proceedings of the 31st International Conference on Computational Linguistics},
  pages={6607--6623},
  year={2025}
}

@inproceedings{gow-smith-etal-2022-improving,
    title = "Improving Tokenisation by Alternative Treatment of Spaces",
    author = "Gow-Smith, Edward  and
      Tayyar Madabushi, Harish  and
      Scarton, Carolina  and
      Villavicencio, Aline",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.786/",
    doi = "10.18653/v1/2022.emnlp-main.786",
    pages = "11430--11443",
    abstract = ""
}

@article{lotz2025beyond,
  title={Beyond Text Compression: Evaluating Tokenizers Across Scales},
  author={Lotz, Jonas F and Lopes, Ant{\'o}nio V and Peitz, Stephan and Setiawan, Hendra and Emili, Leonardo},
  journal={arXiv preprint arXiv:2506.03101},
  year={2025}
}

@article{shannon1948mathematical,
  title={A mathematical theory of communication},
  author={Shannon, Claude E},
  journal={The Bell system technical journal},
  volume={27},
  number={3},
  pages={379--423},
  year={1948},
  publisher={Nokia Bell Labs}
}

@misc{sims2025stochastokimprovingfinegrainedsubword,
      title={StochasTok: Improving Fine-Grained Subword Understanding in LLMs}, 
      author={Anya Sims and Thom Foster and Klara Kaleb and Tuan-Duy H. Nguyen and Joseph Lee and Jakob N. Foerster and Yee Whye Teh and Cong Lu},
      year={2025},
      eprint={2506.01687},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2506.01687}, 
}

@inproceedings{schmidt2024tokenization,
  title={Tokenization Is More Than Compression},
  author={Schmidt, Craig and Reddy, Varshini and Zhang, Haoran and Alameddine, Alec and Uzan, Omri and Pinter, Yuval and Tanner, Chris},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={678--702},
  year={2024}
}

@article{schmidt2025boundless,
  title={Boundless Byte Pair Encoding: Breaking the Pre-tokenization Barrier},
  author={Schmidt, Craig W and Reddy, Varshini and Tanner, Chris and Pinter, Yuval},
  journal={arXiv preprint arXiv:2504.00178},
  year={2025}
}

@inproceedings{zouhar2023formal,
  title={A Formal Perspective on Byte-Pair Encoding},
  author={Zouhar, Vil{\'e}m and Meister, Clara and Gastaldi, Juan and Du, Li and Vieira, Tim and Sachan, Mrinmaya and Cotterell, Ryan},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={598--614},
  year={2023}
}

@inproceedings{galle2019investigating,
  title={Investigating the effectiveness of BPE: The power of shorter sequences},
  author={Gall{\'e}, Matthias},
  booktitle={Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)},
  pages={1375--1381},
  year={2019}
}

@article{lesci2025causal,
  title={Causal Estimation of Tokenisation Bias},
  author={Lesci, Pietro and Meister, Clara and Hofmann, Thomas and Vlachos, Andreas and Pimentel, Tiago},
  journal={arXiv preprint arXiv:2506.03149},
  year={2025}
}



@article{shain2024,
author = {Cory Shain  and Clara Meister  and Tiago Pimentel  and Ryan Cotterell  and Roger Levy },
title = {Large-scale evidence for logarithmic effects of word predictability on reading time},
journal = {Proceedings of the National Academy of Sciences},
volume = {121},
number = {10},
pages = {e2307876121},
year = {2024},
doi = {10.1073/pnas.2307876121},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2307876121},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2307876121},
}



@inproceedings{wendler-etal-2024-llamas,
    title = "Do Llamas Work in {E}nglish? On the Latent Language of Multilingual Transformers",
    author = "Wendler, Chris  and
      Veselovsky, Veniamin  and
      Monea, Giovanni  and
      West, Robert",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.820/",
    doi = "10.18653/v1/2024.acl-long.820",
    pages = "15366--15394",
    abstract = "We ask whether multilingual language models trained on unbalanced, English-dominated corpora use English as an internal pivot language{---}-a question of key importance for understanding how language models function and the origins of linguistic bias. Focusing on the Llama-2 family of transformer models, our study is based on carefully constructed non-English prompts with a unique correct single-token continuation. From layer to layer, transformers gradually map an input embedding of the final prompt token to an output embedding from which next-token probabilities are computed. Tracking intermediate embeddings through their high-dimensional space reveals three distinct phases, whereby intermediate embeddings (1) start far away from output token embeddings; (2) already in middle layers allow for decoding a semantically correct next token, but giving higher probability to its version in English than in the input language; (3) move into an input-language-specific region of the embedding space. We cast these results into a conceptual model where the three phases operate in ``input space'', ``concept space'', and ``output space'', respectively. Crucially, our evidence suggests that the abstract ``concept space'' lies closer to English than to other input languages, which may have important consequences regarding the biases embodied by multilingual language models."
}

@misc{jumelet2025multiblimp10massivelymultilingual,
      title={MultiBLiMP 1.0: A Massively Multilingual Benchmark of Linguistic Minimal Pairs}, 
      author={Jaap Jumelet and Leonie Weissweiler and Arianna Bisazza},
      year={2025},
      eprint={2504.02768},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.02768}, 
}

@inproceedings{yehezkel2023incorporating,
  title={Incorporating Context into Subword Vocabularies},
  author={Yehezkel, Shaked and Pinter, Yuval},
  booktitle={Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},
  pages={623--635},
  year={2023}
}

@misc{gazit2025splinteringnonconcatenativelanguagesbetter,
      title={Splintering Nonconcatenative Languages for Better Tokenization}, 
      author={Bar Gazit and Shaltiel Shmidman and Avi Shmidman and Yuval Pinter},
      year={2025},
      eprint={2503.14433},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2503.14433}, 
}

@article{pinter2021learning,
  title={Learning to look inside: Augmenting token-based encoders with character-level information},
  author={Pinter, Yuval and Stent, Amanda and Dredze, Mark and Eisenstein, Jacob},
  journal={arXiv preprint arXiv:2108.00391},
  year={2021}
}

@article{batsuren2024evaluating,
  title={Evaluating subword tokenization: Alien subword composition and oov generalization challenge},
  author={Batsuren, Khuyagbaatar and Vylomova, Ekaterina and Dankers, Verna and Delgerbaatar, Tsetsuukhei and Uzan, Omri and Pinter, Yuval and Bella, G{\'a}bor},
  journal={arXiv preprint arXiv:2404.13292},
  year={2024}
}

@inproceedings{mansimov-etal-2020-towards,
    title = "Towards End-to-End In-Image Neural Machine Translation",
    author = "Mansimov, Elman  and
      Stern, Mitchell  and
      Chen, Mia  and
      Firat, Orhan  and
      Uszkoreit, Jakob  and
      Jain, Puneet",
    editor = "Castellucci, Giuseppe  and
      Filice, Simone  and
      Poria, Soujanya  and
      Cambria, Erik  and
      Specia, Lucia",
    booktitle = "Proceedings of the First International Workshop on Natural Language Processing Beyond Text",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.nlpbt-1.8/",
    doi = "10.18653/v1/2020.nlpbt-1.8",
    pages = "70--74",
    abstract = "In this paper, we offer a preliminary investigation into the task of in-image machine translation: transforming an image containing text in one language into an image containing the same text in another language. We propose an end-to-end neural model for this task inspired by recent approaches to neural machine translation, and demonstrate promising initial results based purely on pixel-level supervision. We then offer a quantitative and qualitative evaluation of our system outputs and discuss some common failure modes. Finally, we conclude with directions for future work."
}

@inproceedings{salesky-etal-2021-robust,
    title = "Robust Open-Vocabulary Translation from Visual Text Representations",
    author = "Salesky, Elizabeth  and
      Etter, David  and
      Post, Matt",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.576/",
    doi = "10.18653/v1/2021.emnlp-main.576",
    pages = "7235--7252",
    abstract = "Machine translation models have discrete vocabularies and commonly use subword segmentation techniques to achieve an `open vocabulary.' This approach relies on consistent and correct underlying unicode sequences, and makes models susceptible to degradation from common types of noise and variation. Motivated by the robustness of human language processing, we propose the use of visual text representations, which dispense with a finite set of text embeddings in favor of continuous vocabularies created by processing visually rendered text with sliding windows. We show that models using visual text representations approach or match performance of traditional text models on small and larger datasets. More importantly, models with visual embeddings demonstrate significant robustness to varied types of noise, achieving e.g., 25.9 BLEU on a character permuted German{--}English task where subword models degrade to 1.9."
}

@InProceedings{pmlr-v202-lee23g,
  title = 	 {{P}ix2{S}truct: Screenshot Parsing as Pretraining for Visual Language Understanding},
  author =       {Lee, Kenton and Joshi, Mandar and Turc, Iulia Raluca and Hu, Hexiang and Liu, Fangyu and Eisenschlos, Julian Martin and Khandelwal, Urvashi and Shaw, Peter and Chang, Ming-Wei and Toutanova, Kristina},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {18893--18912},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/lee23g/lee23g.pdf},
  url = 	 {https://proceedings.mlr.press/v202/lee23g.html},
  abstract = 	 {Visually-situated language is ubiquitous—sources range from textbooks with diagrams to web pages with images and tables, to mobile apps with buttons and forms. Perhaps due to this diversity, previous work has typically relied on domain-specific recipes with limited sharing of the underlying data, model architectures, and objectives. We present Pix2Struct, a pretrained image-to-text model for purely visual language understanding, which can be finetuned on tasks containing visually-situated language. Pix2Struct is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks. Intuitively, this objective subsumes common pretraining signals such as OCR, language modeling, and image captioning. In addition to the novel pretraining strategy, we introduce a variable-resolution input representation and a more flexible integration of language and vision inputs, where language prompts such as questions are rendered directly on top of the input image. For the first time, we show that a single pretrained model can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.}
}


@article{zhang2024pixels,
  title={From Pixels to Tokens: Byte-Pair Encoding on Quantized Visual Modalities},
  author={Zhang, Wanpeng and Xie, Zilong and Feng, Yicheng and Li, Yijiang and Xing, Xingrun and Zheng, Sipeng and Lu, Zongqing},
  journal={CoRR},
  year={2024}
}

@article{team2024chameleon,
  title={Chameleon: Mixed-modal early-fusion foundation models},
  author={Team, Chameleon},
  journal={arXiv preprint arXiv:2405.09818},
  year={2024}
}

@inproceedings{cui2024survey,
  title={A Survey on Multimodal Large Language Models for Autonomous Driving},
  author={Cui, Can and Ma, Yunsheng and Cao, Xu and Ye, Wenqian and Zhou, Yang and Liang, Kaizhao and Chen, Jintai and Lu, Juanwu and Yang, Zichong and Liao, Kuei-Da and others},
  booktitle={WACV (Workshops)},
  year={2024}
}

@article{liu2023visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={34892--34916},
  year={2023}
}

@article{lakhotia2021generative,
  title={On Generative Spoken Language Modeling from Raw Audio},
  author={Lakhotia, Kushal and Kharitonov, Eugene and Hsu, Wei-Ning and Adi, Yossi and Polyak, Adam and Bolte, Benjamin and Nguyen, Tu Anh and Copet, Jade and Baevski, Alexei and Mohamed, Abdelrahman and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={1336--1354},
  year={2021}
}

@article{nguyen-etal-2025-spirit,
    title = "{S}pi{R}it-{LM}: Interleaved Spoken and Written Language Model",
    author = "Nguyen, Tu Anh  and
      Muller, Benjamin  and
      Yu, Bokai  and
      Costa-jussa, Marta R.  and
      Elbayad, Maha  and
      Popuri, Sravya  and
      Ropers, Christophe  and
      Duquenne, Paul-Ambroise  and
      Algayres, Robin  and
      Mavlyutov, Ruslan  and
      Gat, Itai  and
      Williamson, Mary  and
      Synnaeve, Gabriel  and
      Pino, Juan  and
      Sagot, Beno{\^i}t  and
      Dupoux, Emmanuel",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "13",
    year = "2025",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2025.tacl-1.2/",
    doi = "10.1162/tacl_a_00728",
    pages = "30--52",
    abstract = "We introduce SpiRit-LM, a foundation multimodal language model that freely mixes text and speech. Our model is based on a 7B pretrained text language model that we extend to the speech modality by continuously training it on text and speech units. Speech and text sequences are concatenated as a single stream of tokens, and trained with a word-level interleaving method using a small automatically curated speech-text parallel corpus. SpiRit-LM comes in two versions: a Base version that uses speech phonetic units (HuBERT) and an Expressive version that models expressivity using pitch and style units in addition to the phonetic units. For both versions, the text is encoded with subword BPE tokens. The resulting model displays both the semantic abilities of text models and the expressive abilities of speech models. Additionally, we demonstrate that SpiRit-LM can learn new tasks in a few-shot fashion across modalities (i.e., ASR, TTS, Speech Classification). We make available model weights and inference code.1,2"
}

@article{bapna2021slam,
  title={SLAM: A unified encoder for speech and language modeling via speech-text joint pre-training},
  author={Bapna, Ankur and Chung, Yu-an and Wu, Nan and Gulati, Anmol and Jia, Ye and Clark, Jonathan H and Johnson, Melvin and Riesa, Jason and Conneau, Alexis and Zhang, Yu},
  journal={arXiv preprint arXiv:2110.10329},
  year={2021}
}

@article{arivazhagan2019massively,
  title={Massively multilingual neural machine translation in the wild: Findings and challenges},
  author={Arivazhagan, Naveen and Bapna, Ankur and Firat, Orhan and Lepikhin, Dmitry and Johnson, Melvin and Krikun, Maxim and Chen, Mia Xu and Cao, Yuan and Foster, George and Cherry, Colin and others},
  journal={arXiv preprint arXiv:1907.05019},
  year={2019}
}

@inproceedings{lucassen1984information,
  title={An information theoretic approach to the automatic determination of phonemic baseforms},
  author={Lucassen, John and Mercer, Robert},
  booktitle={ICASSP'84. IEEE International Conference on Acoustics, Speech, and Signal Processing},
  volume={9},
  pages={304--307},
  year={1984},
  organization={IEEE}
}

@inproceedings{mccarthy-etal-2023-sigmorphon,
    title = "The {SIGMORPHON} 2022 Shared Task on Cross-lingual and Low-Resource Grapheme-to-Phoneme Conversion",
    author = "McCarthy, Arya D.  and
      Lee, Jackson L.  and
      DeLucia, Alexandra  and
      Bartley, Travis  and
      Agarwal, Milind  and
      Ashby, Lucas F.E.  and
      Del Signore, Luca  and
      Gibson, Cameron  and
      Raff, Reuben  and
      Wu, Winston",
    editor = {Nicolai, Garrett  and
      Chodroff, Eleanor  and
      Mailhot, Frederic  and
      {\c{C}}{\"o}ltekin, {\c{C}}a{\u{g}}r{\i}},
    booktitle = "Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.sigmorphon-1.27/",
    doi = "10.18653/v1/2023.sigmorphon-1.27",
    pages = "230--238",
    abstract = "Grapheme-to-phoneme conversion is an important component in many speech technologies, but until recently there were no multilingual benchmarks for this task. The third iteration of the SIGMORPHON shared task on multilingual grapheme-to-phoneme conversion features many improvements from the previous year{'}s task (Ashby et al., 2021), including additional languages, three subtasks varying the amount of available resources, extensive quality assurance procedures, and automated error analyses. Three teams submitted a total of fifteen systems, at best achieving relative reductions of word error rate of 14{\%} in the crosslingual subtask and 14{\%} in the very-low resource subtask. The generally consistent result is that cross-lingual transfer substantially helps grapheme-to-phoneme modeling, but not to the same degree as in-language examples."
}

@inproceedings{li-etal-2022-zero,
    title = "Zero-shot Learning for Grapheme to Phoneme Conversion with Language Ensemble",
    author = "Li, Xinjian  and
      Metze, Florian  and
      Mortensen, David  and
      Watanabe, Shinji  and
      Black, Alan",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.166/",
    doi = "10.18653/v1/2022.findings-acl.166",
    pages = "2106--2115",
    abstract = "Grapheme-to-Phoneme (G2P) has many applications in NLP and speech fields. Most existing work focuses heavily on languages with abundant training datasets, which limits the scope of target languages to less than 100 languages. This work attempts to apply zero-shot learning to approximate G2P models for all low-resource and endangered languages in Glottolog (about 8k languages). For any unseen target language, we first build the phylogenetic tree (i.e. language family tree) to identify top-$k$ nearest languages for which we have training sets. Then we run models of those languages to obtain a hypothesis set, which we combine into a confusion network to propose a most likely hypothesis as an approximation to the target language. We test our approach on over 600 unseen languages and demonstrate it significantly outperforms baselines."
}

@inproceedings{zhang2022mixed,
  title={Mixed-Phoneme BERT: Improving BERT with Mixed Phoneme and Sup-Phoneme Representations for Text to Speech},
  author={Zhang, Guangyan and Song, Kai and Tan, Xu and Tan, Dun and Yan, Yongqiang and Liu, Yuxuan and Wang, Guangzhi and Zhou, Wei and Qin, Tao and Lee, Tan and Zhao, Sheng},
  booktitle={Proc. Interspeech 2022},
  pages={456--460},
  year={2022},
  doi={10.21437/Interspeech.2022-621}
}

@inproceedings{jia2021png,
  title={PnG BERT: Augmented BERT on Phonemes and Graphemes for Neural TTS},
  author={Jia, Ye and Zen, Heiga and Shen, Jonathan and Zhang, Yu and Wu, Yonghui},
  booktitle={Proc. Interspeech 2021},
  pages={151--155},
  year={2021},
  doi={10.21437/Interspeech.2021-1757}
}

@article{li2022recent,
  title={Recent advances in end-to-end automatic speech recognition},
  author={Li, Jinyu and others},
  journal={APSIPA Transactions on Signal and Information Processing},
  volume={11},
  number={1},
  year={2022},
  publisher={Now Publishers, Inc.}
}

@book{international1999handbook,
  title={Handbook of the International Phonetic Association: A guide to the use of the International Phonetic Alphabet},
  author={International Phonetic Association},
  year={1999},
  publisher={Cambridge University Press}
}

@book{lehmann2013historical,
  title={Historical linguistics: An introduction},
  author={Lehmann, Winfred P},
  year={2013},
  publisher={Routledge}
}

@book{abondolo1998uralic,
  title = {The {Uralic} Languages},
  editor = {Abondolo, Daniel},
  year = {1998},
  edition = {1},
  publisher = {Routledge},
  doi = {10.4324/9781315003283}
}

@misc{kirshenbaum2011ascii,
  author       = {Evan Kirshenbaum},
  title        = {Representing IPA phonetics in ASCII},
  year         = {2011},
  howpublished = {\url{https://web.archive.org/web/20160419125856/http://www.kirshenbaum.net/IPA/ascii-ipa.pdf}},
  note         = {Archived PDF, accessed via the Internet Archive on June 15, 2025},
}

@article{elman1990finding,
	title        = {Finding structure in time},
	author       = {Elman, Jeffrey L},
	year         = {1990},
	journal      = {Cognitive science},
	publisher    = {Wiley Online Library},
	volume       = {14},
	number       = {2},
	pages        = {179--211},
url = {https://www.sciencedirect.com/science/article/pii/036402139090002E}
}

@article{mcclelland1986trace,
  title={The TRACE model of speech perception},
  author={McClelland, James L and Elman, Jeffrey L},
  journal={Cognitive psychology},
  volume={18},
  number={1},
  pages={1--86},
  year={1986},
  publisher={Elsevier}
}

@article{MARSLENWILSON1978,
title = {Processing interactions and lexical access during word recognition in continuous speech},
journal = {Cognitive Psychology},
volume = {10},
number = {1},
pages = {29-63},
year = {1978},
issn = {0010-0285},
doi = {https://doi.org/10.1016/0010-0285(78)90018-X},
url = {https://www.sciencedirect.com/science/article/pii/001002857890018X},
author = {William D Marslen-Wilson and Alan Welsh},
abstract = {The interactions, during word-recognition in continuous speech, between the bottom-up analyses of the input and different forms of internally generated top-down constraint, were investigated using a shadowing task and a mispronunciation detection task (in the detection task the subject saw a text of the original passage as he listened to it). The listener's dependence on bottom-up analyses in the shadowing task, as measured by the number of fluent restorations of mispronounced words, was found to vary as a function of the syllable position of the mispronunciation within the word and of the contextual constraints on the word as a whole. In the detection task only syllable position effects were obtained. The results, discussed in conjunction with earlier research, were found to be inconsistent with either the logogen model of word-recognition or an autonomous search model. Instead, an active direct access model is proposed, in which top-down processing constraints interact directly with bottom-up information to produce the primary lexical interpretation of the acoustic-phonetic input.}
}

@article{Rumelhart1986,
   author = {David Rumelhart and James McClelland},
   city = {Cambridge, Massachusetts},
   journal = {Parallel Distributed Processing: explorations in the microstructure of cognition},
   pages = {216-269},
   publisher = {The MIT Press},
   title = {On Learning the Past Tenses of English Verbs},
   volume = {2},
   year = {1986},
}

@article{pinker_language_1988,
	title = {On language and connectionism: {Analysis} of a parallel distributed processing model of language acquisition},
	volume = {28},
	issn = {0010-0277},
	url = {https://www.sciencedirect.com/science/article/pii/0010027788900327},
	doi = {https://doi.org/10.1016/0010-0277(88)90032-7},
	number = {1},
	journal = {Cognition},
	author = {Pinker, Steven and Prince, Alan},
	year = {1988},
	pages = {73--193},
}

@article{donhauser2020two,
  title={Two distinct neural timescales for predictive speech processing},
  author={Donhauser, Peter W and Baillet, Sylvain},
  journal={Neuron},
  volume={105},
  number={2},
  pages={385--393},
  year={2020},
  publisher={Elsevier}
}

@article{scharenborg2020speech,
  title={Speech technology for unwritten languages},
  author={Scharenborg, Odette and Besacier, Laurent and Black, Alan and Hasegawa-Johnson, Mark and Metze, Florian and Neubig, Graham and St{\"u}ker, Sebastian and Godard, Pierre and M{\"u}ller, Markus and Ondel, Lucas and others},
  journal={IEEE/ACM transactions on audio, speech, and language processing},
  volume={28},
  pages={964--975},
  year={2020},
  publisher={IEEE}
}

@inproceedings{martinez2024tending,
  title={Tending Towards Stability: Convergence Challenges in Small Language Models},
  author={Martinez, Richard Diehl and Lesci, Pietro and Buttery, Paula},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  pages={3275--3286},
  year={2024}
}

@inproceedings{lison-tiedemann-2016-opensubtitles2016,
    title = "{O}pen{S}ubtitles2016: Extracting Large Parallel Corpora from Movie and {TV} Subtitles",
    author = {Lison, Pierre  and
      Tiedemann, J{\"o}rg},
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Grobelnik, Marko  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, Helene  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L16-1147/",
    pages = "923--929",
    abstract = "We present a new major release of the OpenSubtitles collection of parallel corpora. The release is compiled from a large database of movie and TV subtitles and includes a total of 1689 bitexts spanning 2.6 billion sentences across 60 languages. The release also incorporates a number of enhancements in the preprocessing and alignment of the subtitles, such as the automatic correction of OCR errors and the use of meta-data to estimate the quality of each subtitle and score subtitle pairs."
}

@article{stolcke-etal-2000-dialogue,
    title = "Dialogue act modeling for automatic tagging and recognition of conversational speech",
    author = "Stolcke, Andreas  and
      Ries, Klaus  and
      Coccaro, Noah  and
      Shriberg, Elizabeth  and
      Bates, Rebecca  and
      Jurafsky, Daniel  and
      Taylor, Paul  and
      Martin, Rachel  and
      Van Ess-Dykema, Carol  and
      Meteer, Marie",
    journal = "Computational Linguistics",
    volume = "26",
    number = "3",
    year = "2000",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/J00-3003/",
    pages = "339--374"
}

@misc{penedo2024finewebdatasetsdecantingweb,
      title={The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale}, 
      author={Guilherme Penedo and Hynek Kydlíček and Loubna Ben allal and Anton Lozhkov and Margaret Mitchell and Colin Raffel and Leandro Von Werra and Thomas Wolf},
      year={2024},
      eprint={2406.17557},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.17557}, 
}

@misc{bunzeck2025constructiondistributionsshapeformal,
      title={Do Construction Distributions Shape Formal Language Learning In German BabyLMs?}, 
      author={Bastian Bunzeck and Daniel Duran and Sina Zarrieß},
      year={2025},
      eprint={2503.11593},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2503.11593}, 
      note={Accepted at CoNLL 2025}
}

@misc{suozzi2025bambidevelopingbabylanguage,
      title={BAMBI: Developing Baby Language Models for Italian}, 
      author={Alice Suozzi and Luca Capone and Gianluca E. Lebani and Alessandro Lenci},
      year={2025},
      eprint={2503.09481},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2503.09481}, 
}

@inproceedings{haga-etal-2024-babylm,
    title = "{B}aby{LM} Challenge: Exploring the effect of variation sets on language model training efficiency",
    author = "Haga, Akari  and
      Fukatsu, Akiyo  and
      Oba, Miyu  and
      Bisazza, Arianna  and
      Oseki, Yohei",
    editor = "Hu, Michael Y.  and
      Mueller, Aaron  and
      Ross, Candace  and
      Williams, Adina  and
      Linzen, Tal  and
      Zhuang, Chengxu  and
      Choshen, Leshem  and
      Cotterell, Ryan  and
      Warstadt, Alex  and
      Wilcox, Ethan Gotlieb",
    booktitle = "The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning",
    month = nov,
    year = "2024",
    address = "Miami, FL, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.conll-babylm.23/",
    pages = "252--261",
    abstract = "While current large language models have achieved a remarkable success, their data efficiency remains a challenge to overcome. Recently it has been suggested that child-directed speech (CDS) can improve training data efficiency of modern language models based on Transformer neural networks. However, it is not yet understood which specific properties of CDS are effective for training these models. In the context of the BabyLM Challenge, we focus on Variation Sets (VSs), sets of consecutive utterances expressing a similar intent with slightly different words and structures, which are ubiquitous in CDS. To assess the impact of VSs on training data efficiency, we augment CDS data with different proportions of artificial VSs and use these datasets to train an auto-regressive model, GPT-2. We find that the best proportion of VSs depends on the evaluation benchmark: BLiMP and GLUE scores benefit from the presence of VSs, but EWOK scores do not. Additionally, the results vary depending on multiple factors such as the number of epochs and the order of utterance presentation. Taken together, these findings suggest that VSs can have a beneficial influence on language models, while leaving room for further investigation."
}

@inproceedings{capone-etal-2024-concretegpt,
    title = "{C}oncrete{GPT}: A Baby {GPT}-2 Based on Lexical Concreteness and Curriculum Learning",
    author = "Capone, Luca  and
      Bondielli, Alessandro  and
      Lenci, Alessandro",
    editor = "Hu, Michael Y.  and
      Mueller, Aaron  and
      Ross, Candace  and
      Williams, Adina  and
      Linzen, Tal  and
      Zhuang, Chengxu  and
      Choshen, Leshem  and
      Cotterell, Ryan  and
      Warstadt, Alex  and
      Wilcox, Ethan Gotlieb",
    booktitle = "The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning",
    month = nov,
    year = "2024",
    address = "Miami, FL, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.conll-babylm.16/",
    pages = "189--196",
    abstract = "We present a model for the Strict-Small track of the BabyLM Challenge 2024 (Choshen et al. 2024). We introduce a Curriculum Learning approach for training a specialized version of GPT-2 (Radford et al. 2019), that we name ConcreteGPT. We utilize the norms from (Brysbaert et al. 2014) which provide concreteness ratings for 40,000 English lexical items based on human subjects. Using these norms, we assign a concreteness score to each sentence in the training dataset and develop two curriculum strategies that progressively introduce more complex and abstract language patterns in the training data. Compared to the baselines, our best model shows lower performance on zero-shot tasks but demonstrates superior performance in fine-tuning tasks. Notably, our curriculum-trained models exhibit significant improvements over a non-curriculum based training of the same model."
}

@inproceedings{nguyen-etal-2024-automatic,
    title = "Automatic Quality Estimation for Data Selection and Curriculum Learning",
    author = "Nguyen, Hiep  and
      Yip, Lynn  and
      DeBenedetto, Justin",
    editor = "Hu, Michael Y.  and
      Mueller, Aaron  and
      Ross, Candace  and
      Williams, Adina  and
      Linzen, Tal  and
      Zhuang, Chengxu  and
      Choshen, Leshem  and
      Cotterell, Ryan  and
      Warstadt, Alex  and
      Wilcox, Ethan Gotlieb",
    booktitle = "The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning",
    month = nov,
    year = "2024",
    address = "Miami, FL, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.conll-babylm.18/",
    pages = "212--220",
    abstract = "The size of neural models within natural language processing has increased at a rapid pace in recent years.With this increase in model size comes an increase in the amount of training data required for training.While these larger models have shown strong performance, their use comes with added training and data costs, can be resource-prohibitive for many researchers, and uses an amount of language data that is not always available for all languages.This work focuses on exploring quality estimation as a method of data selection or filtering.The aim is to provide models with higher quality data as compared to larger amounts of data.This approach was applied to machine translation models with varying data sizes as well as to the BabyLM Challenge.Given the 100M word dataset provided in the BabyLM Challenge, we test out various strategies for selecting 10M words for pretraining and use a curriculum learning approach based on the quality estimation scoring.We find small improvements in certain data settings."
}

@inproceedings{debenedetto-2023-byte,
    title = "Byte-ranked Curriculum Learning for {B}aby{LM} Strict-small Shared Task 2023",
    author = "DeBenedetto, Justin",
    editor = "Warstadt, Alex  and
      Mueller, Aaron  and
      Choshen, Leshem  and
      Wilcox, Ethan  and
      Zhuang, Chengxu  and
      Ciro, Juan  and
      Mosquera, Rafael  and
      Paranjabe, Bhargavi  and
      Williams, Adina  and
      Linzen, Tal  and
      Cotterell, Ryan",
    booktitle = "Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.conll-babylm.17/",
    doi = "10.18653/v1/2023.conll-babylm.17",
    pages = "198--206"
}


@inproceedings{yam-paek-2024-baby,
    title = "What should Baby Models read? Exploring Sample-Efficient Data Composition on Model Performance",
    author = "Yam, Hong Meng  and
      Paek, Nathan",
    editor = "Hu, Michael Y.  and
      Mueller, Aaron  and
      Ross, Candace  and
      Williams, Adina  and
      Linzen, Tal  and
      Zhuang, Chengxu  and
      Choshen, Leshem  and
      Cotterell, Ryan  and
      Warstadt, Alex  and
      Wilcox, Ethan Gotlieb",
    booktitle = "The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning",
    month = nov,
    year = "2024",
    address = "Miami, FL, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.conll-babylm.25/",
    pages = "284--291",
    abstract = "We explore the impact of pre-training data composition on the performance of small language models in a sample-efficient setting. Using datasets capped at 10 million words, we evaluate several data sources{---}including child-directed speech (CHILDES), classic fiction (Gutenberg), a mixed dataset (Mix), and synthetic TinyStories{---}across different model sizes ranging from 18 million to 705 million parameters. Our experiments show that smaller models (e.g., GPT2-18M and GPT2-44M) benefit from training on diverse datasets like Mix, achieving better performance on linguistic benchmarks. In contrast, larger models (e.g., GPT2-97M, GPT2-705M, and LLaMA-360M) perform better when trained on more complex and rich datasets like Gutenberg. Models trained on the CHILDES and TinyStories datasets underperformed across all model sizes. These findings suggest that the optimal dataset for sample-efficient training depends on the model size, and that neither child-directed speech nor simplified stories are optimal for small language models of all sizes. We highlight the importance of considering both dataset composition and model capacity for effective sample-efficient language model training."
}

@inproceedings{mueller-etal-2020-cross,
    title = "{Cross-Linguistic Syntactic Evaluation of Word Prediction Models}",
    author = "Mueller, Aaron  and
      Nicolai, Garrett  and
      Petrou-Zeniou, Panayiota  and
      Talmina, Natalia  and
      Linzen, Tal",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}",
    month = jul,
    year = "2020",
    pages = "5523--5539",
    abstract = "A range of studies have concluded that neural word prediction models can distinguish grammatical from ungrammatical sentences with high accuracy. However, these studies are based primarily on monolingual evidence from English. To investigate how these models{'} ability to learn syntax varies by language, we introduce CLAMS (Cross-Linguistic Assessment of Models on Syntax), a syntactic evaluation suite for monolingual and multilingual models. CLAMS includes subject-verb agreement challenge sets for English, French, German, Hebrew and Russian, generated from grammars we develop. We use CLAMS to evaluate LSTM language models as well as monolingual and multilingual BERT. Across languages, monolingual LSTMs achieved high accuracy on dependencies without attractors, and generally poor accuracy on agreement across object relative clauses. On other constructions, agreement accuracy was generally higher in languages with richer morphology. Multilingual models generally underperformed monolingual models. Multilingual BERT showed high syntactic accuracy on English, but noticeable deficiencies in other languages.",
}

@inproceedings{someya-oseki-2023-jblimp,
    title = "{JBL}i{MP}: {J}apanese Benchmark of Linguistic Minimal Pairs",
    author = "Someya, Taiga  and
      Oseki, Yohei",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "{Findings of the Association for Computational Linguistics: EACL 2023}",
    month = may,
    year = "2023",
    pages = "1581--1594",
    abstract = "In this paper, we introduce JBLiMP (Japanese Benchmark of Linguistic Minimal Pairs), a novel dataset for targeted syntactic evaluations of language models in Japanese. JBLiMP consists of 331 minimal pairs, which are created based on acceptability judgments extracted from journal articles in theoretical linguistics. These minimal pairs are grouped into 11 categories, each covering a different linguistic phenomenon. JBLiMP is unique in that it successfully combines two important features independently observed in existing datasets: (i) coverage of complex linguistic phenomena (cf. CoLA) and (ii) presentation of sentences as minimal pairs (cf. BLiMP). In addition, JBLiMP is the first dataset for targeted syntactic evaluations of language models in Japanese, thus allowing the comparison of syntactic knowledge of language models across different languages. We then evaluate the syntactic knowledge of several language models on JBLiMP: GPT-2, LSTM, and n-gram language models. The results demonstrated that all the architectures achieved comparable overall accuracies around 75{\%}. Error analyses by linguistic phenomenon further revealed that these language models successfully captured local dependencies like nominal structures, but not long-distance dependencies such as verbal agreement and binding.",
}

@inproceedings{xiang-etal-2021-climp,
    title = "{CL}i{MP}: A Benchmark for {C}hinese Language Model Evaluation",
    author = "Xiang, Beilei  and
      Yang, Changbing  and
      Li, Yu  and
      Warstadt, Alex  and
      Kann, Katharina",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "{Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume}",
    month = apr,
    year = "2021",
    pages = "2784--2790",
    abstract = "Linguistically informed analyses of language models (LMs) contribute to the understanding and improvement of such models. Here, we introduce the corpus of Chinese linguistic minimal pairs (CLiMP) to investigate what knowledge Chinese LMs acquire. CLiMP consists of sets of 1000 minimal pairs (MPs) for 16 syntactic contrasts in Chinese, covering 9 major Chinese linguistic phenomena. The MPs are semi-automatically generated, and human agreement with the labels in CLiMP is 95.8{\%}. We evaluate 11 different LMs on CLiMP, covering n-grams, LSTMs, and Chinese BERT. We find that classifier{--}noun agreement and verb complement selection are the phenomena that models generally perform best at. However, models struggle the most with the ba construction, binding, and filler-gap dependencies. Overall, Chinese BERT achieves an 81.8{\%} average accuracy, while the performances of LSTMs and 5-grams are only moderately above chance level.",
}


@inproceedings{song-etal-2022-sling,
    title = "{SLING}: {S}ino Linguistic Evaluation of Large Language Models",
    author = "Song, Yixiao  and
      Krishna, Kalpesh  and
      Bhatt, Rajesh  and
      Iyyer, Mohit",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}",
    month = dec,
    year = "2022",
    pages = "4606--4634",
    abstract = "To understand what kinds of linguistic knowledge are encoded by pretrained Chinese language models (LMs), we introduce the benchmark of Sino LINGuistics (SLING), which consists of 38K minimal sentence pairs in Mandarin Chinese grouped into 9 high-level linguistic phenomena. Each pair demonstrates the acceptability contrast of a specific syntactic or semantic phenomenon (e.g., The keys are lost vs. The keys is lost), and an LM should assign lower perplexity to the acceptable sentence. In contrast to the CLiMP dataset (Xiang et al., 2021), which also contains Chinese minimal pairs and was created by translating the vocabulary of the English BLiMP dataset, the minimal pairs in SLING are derived primarily by applying syntactic and lexical transformations to naturally-occurring, linguist-annotated sentences from the Chinese Treebank 9.0, thus addressing severe issues in CLiMP{'}s data generation process. We test 18 publicly available pretrained monolingual (e.g., BERT-base-zh, CPM) and multi-lingual (e.g., mT5, XLM) language models on SLING. Our experiments show that the average accuracy for LMs is far below human performance (69.7{\%} vs. 97.1{\%}), while BERT-base-zh achieves the highest accuracy (84.8{\%}) of all tested LMs, even much larger ones. Additionally, we find that most LMs have a strong gender and number (singular/plural) bias, and they perform better on local phenomena than hierarchical ones.",
}

@misc{videau2025bytesideaslanguagemodeling,
      title={From Bytes to Ideas: Language Modeling with Autoregressive U-Nets}, 
      author={Mathurin Videau and Badr Youbi Idrissi and Alessandro Leite and Marc Schoenauer and Olivier Teytaud and David Lopez-Paz},
      year={2025},
      eprint={2506.14761},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2506.14761}, 
}


@article{rousseeuw1987,
title = {Silhouettes: A graphical aid to the interpretation and validation of cluster analysis},
journal = {Journal of Computational and Applied Mathematics},
volume = {20},
pages = {53-65},
year = {1987},
issn = {0377-0427},
doi = {https://doi.org/10.1016/0377-0427(87)90125-7},
url = {https://www.sciencedirect.com/science/article/pii/0377042787901257},
author = {Peter J. Rousseeuw},
keywords = {Graphical display, cluster analysis, clustering validity, classification},
abstract = {A new graphical display is proposed for partitioning techniques. Each cluster is represented by a so-called silhouette, which is based on the comparison of its tightness and separation. This silhouette shows which objects lie well within their cluster, and which ones are merely somewhere in between clusters. The entire clustering is displayed by combining the silhouettes into a single plot, allowing an appreciation of the relative quality of the clusters and an overview of the data configuration. The average silhouette width provides an evaluation of clustering validity, and might be used to select an ‘appropriate’ number of clusters.}
}