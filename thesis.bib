%%%%%%%%%%%%%%%%%
% DATASET CHAPTER
%%%%%%%%%%%%%%%%%

%%%%%%%%%%
% Datasets
@article{pile,
  title={The {P}ile: An 800GB Dataset of Diverse Text for Language Modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@inproceedings{godfrey1992switchboard,
  title={SWITCHBOARD: Telephone speech corpus for research and development},
  author={Godfrey, John J and Holliman, Edward C and McDaniel, Jane},
  booktitle={Acoustics, speech, and signal processing, ieee international conference on},
  volume={1},
  pages={517--520},
  year={1992},
  organization={IEEE Computer Society}
}

@article{francis1979brown,
  title={Brown corpus manual},
  author={Francis, W Nelson and Kucera, Henry},
  journal={Letters to the Editor},
  volume={5},
  number={2},
  pages={7},
  year={1979}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@inproceedings{schultz2013globalphone,
  title={Globalphone: A multilingual text \& speech database in 20 languages},
  author={Schultz, Tanja and Vu, Ngoc Thang and Schlippe, Tim},
  booktitle={2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
  pages={8126--8130},
  year={2013},
  organization={IEEE}
}

@article{garofolo1993darpa,
  title={DARPA TIMIT acoustic-phonetic continous speech corpus CD-ROM. NIST speech disc 1-1.1},
  author={Garofolo, John S and Lamel, Lori F and Fisher, William M and Fiscus, Jonathan G and Pallett, David S},
  journal={NASA STI/Recon technical report n},
  volume={93},
  pages={27403},
  year={1993}
}

@inproceedings{lamel1989speech,
  title={Speech database development: Design and analysis of the acoustic-phonetic corpus},
  author={Lamel, Lori F and Kassel, Robert H and Seneff, Stephanie},
  booktitle={Proc. SIOA 1989},
  pages={Vol--2},
  year={1989}
}

@article{pitt2007buckeye,
  title={Buckeye corpus of conversational speech (2nd release)},
  author={Pitt, Mark A and Dilley, Laura and Johnson, Keith and Kiesling, Scott and Raymond, William and Hume, Elizabeth and Fosler-Lussier, Eric},
  journal={Columbus, OH: Department of Psychology, Ohio State University},
  pages={265--270},
  year={2007}
}

@inproceedings{Kahn_2020,
   title={Libri-Light: A Benchmark for ASR with Limited or No Supervision},
   url={http://dx.doi.org/10.1109/ICASSP40776.2020.9052942},
   DOI={10.1109/icassp40776.2020.9052942},
   booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
   publisher={IEEE},
   author={Kahn, J. and Riviere, M. and Zheng, W. and Kharitonov, E. and Xu, Q. and Mazare, P.E. and Karadayi, J. and Liptchinsky, V. and Collobert, R. and Fuegen, C. and Likhomanenko, T. and Synnaeve, G. and Joulin, A. and Mohamed, A. and Dupoux, E.},
   year={2020},
   month=may, pages={7669–7673} }

@inproceedings{gales2014speech,
  title={Speech recognition and keyword spotting for low-resource languages: Babel project research at cued},
  author={Gales, Mark JF and Knill, Kate M and Ragni, Anton and Rath, Shakti P},
  booktitle={Fourth International workshop on spoken language technologies for under-resourced languages (SLTU-2014)},
  pages={16--23},
  year={2014},
  organization={International Speech Communication Association (ISCA)}
}

@inproceedings{cieri2004fisher,
  title={The Fisher corpus: A resource for the next generations of speech-to-text.},
  author={Cieri, Christopher and Miller, David and Walker, Kevin},
  booktitle={LREC},
  volume={4},
  pages={69--71},
  year={2004}
}

@inproceedings{ardila-etal-2020-common,
    title = "Common Voice: A Massively-Multilingual Speech Corpus",
    author = "Ardila, Rosana  and
      Branson, Megan  and
      Davis, Kelly  and
      Kohler, Michael  and
      Meyer, Josh  and
      Henretty, Michael  and
      Morais, Reuben  and
      Saunders, Lindsay  and
      Tyers, Francis  and
      Weber, Gregor",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.520",
    pages = "4218--4222",
    abstract = "The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identification). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla{'}s DeepSpeech Speech-to-Text toolkit. By applying transfer learning from a source English model, we find an average Character Error Rate improvement of 5.99 {\mbox{$\pm$}} 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the first ever published results on end-to-end Automatic Speech Recognition.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@article{coleman2012audio,
  title={Audio BNC: the audio edition of the Spoken British National Corpus},
  author={Coleman, John and Baghai-Ravary, Ladan and Pybus, John and Grau, Sergio},
  journal={Phonetics Laboratory, University of Oxford},
  year={2012}
}

 @misc{bnc2007,
 title = {British National Corpus, {XML} edition},
 author = {{BNC} Consortium},
 url = {http://hdl.handle.net/20.500.14106/2554},
 note = {Literary and Linguistic Data Service},
 copyright = {Distributed by the University of Oxford under the {BNC} User Licence. Clicking to download implies acceptance of the licence conditions.},
 year = {2007} }
@article{macwhinney_understanding_2019,
	title = {Understanding spoken language through {TalkBank}},
	volume = {51},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/s13428-018-1174-9},
	doi = {10.3758/s13428-018-1174-9},
	abstract = {Ongoing advances in computer technology have opened up a deluge of new datasets for understanding human behavior (Goldstone \& Lupyan, 2016). Many of these datasets provide information on the use of written language. However, data on naturally occurring spoken-language conversations are much more difficult to obtain. A major exception to this is the TalkBank system, which provides online multimedia data for 14 types of spoken-language data: language in aphasia, child language, stuttering, child phonology, autism spectrum disorder, bilingualism, Conversation Analysis, classroom discourse, dementia, right hemisphere damage, Danish conversation, second language learning, traumatic brain injury, and daylong recordings in the home. The present report reviews these resources and describes the ways they are being used to further our understanding of human language and communication.},
	number = {4},
	journal = {Behavior Research Methods},
	author = {MacWhinney, Brian},
	month = aug,
	year = {2019},
	pages = {1919--1927},
}

@article{macwhinney1985child,
  title={{The Child Language Data Exchange System}},
  author={MacWhinney, Brian and Snow, Catherine},
  journal={Journal of Child Language},
  volume={12},
  number={2},
  pages={271--295},
  year={1985},
  publisher={Cambridge University Press}
}


@article{bernstein_ratner_augmenting_2024,
	title = {Augmenting {Clinical} {Insights} with {Computing}: {How} {TalkBank} has {Impacted} {Assessment} and {Treatment} of {Speech} and {Language} {Disorders}},
	volume = {44},
	issn = {2667-6753},
	shorttitle = {Augmenting {Clinical} {Insights} with {Computing}},
	url = {https://www.eurokd.com/doi/10.32038/ltrq.2024.44.05},
	doi = {10.32038/ltrq.2024.44.05},
	abstract = {Our purpose is to highlight the contributions of TalkBank initiatives to improved understanding of clinical impairments in adult and child speakers and examine remaining challenges and proposed solutions.We review the origins and development of TalkBank initiatives that have targeted a wide array of typical and atypical child and adult populations. In particular, we discuss how such sets of data have given rise to evaluation and validation of traditional measures used to appraise spoken language performance. The durable contributions of AphasiaBank and CHILDES archives are already evident in a body of published research that has re-evaluated, refined and reconceptualized how we evaluate and set therapeutic goals for speakers with expressive speech and language impairments. More recent archival initiatives, such as PhonBank and FluencyBank, are also making impacts. Beyond improvements in basic and applied science in communication development and disorders, archival data are also being used to test and improve accessibility for communicatively impaired speakers. TalkBank has transformed how research in communication disorders is conducted. It no longer relies on small, unshared research ventures that enable limited clinical impact or follow-up research inquiries. Rather, it has enabled largescale, more generalizable research more likely to spur further research and enable more rapid translation to clinical practice.},
	language = {en},
	urldate = {2024-11-18},
	journal = {Language Teaching Research Quarterly},
	author = {Bernstein Ratner, Nan},
	month = oct,
	year = {2024},
	pages = {31--40},
	file = {Bernstein Ratner - 2024 - Augmenting Clinical Insights with Computing How T.pdf:/Users/zebulongoriely/Zotero/storage/MEVBB9WN/Bernstein Ratner - 2024 - Augmenting Clinical Insights with Computing How T.pdf:application/pdf},
}

@misc{gerlach2018standardizedprojectgutenbergcorpus,
      title={A standardized Project Gutenberg corpus for statistical analysis of natural language and quantitative linguistics}, 
      author={Martin Gerlach and Francesc Font-Clos},
      year={2018},
      eprint={1812.08092},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1812.08092}, 
}

%%%%%%%%%%%%%%%%%%%%
% Dataset Properties

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

%%%%%%%%%%%%%%%%%%%
% English dominance

@misc{ebbertz2002,
  title={Internet Statistics: Distribution of languages on the Internet},
  author={Ebbertz, Martin},
  year={2002},
  url={https://netz-tipp.de/languages.html},
  note={Retrieved November 14, 2024}
}

@book{danet2007multilingual,
  title={The multilingual Internet: Language, culture, and communication online},
  author={Danet, Brenda and Herring, Susan C},
  year={2007},
  publisher={Oxford University Press}
}

@misc{DataReportal2024,
  author       = {DataReportal and We Are Social and Meltwater},
  title        = {Languages most frequently used for web content as of January 2024, by share of websites [Graph]},
  month        = {January 31},
  year         = {2024},
  publisher    = {Statista},
  url          = {https://www.statista.com/statistics/262946/most-common-languages-on-the-internet/},
  note         = {Retrieved November 14, 2024}
}

%%%%%%%%%%%
% Alphabets

@article{wells1992standard,
  title={Standard computer-compatible transcription},
  author={Wells, John and Barry, William and Grice, Martine and Fourcin, Adrian and Gibbon, Dafydd},
  journal={Esprit project 2589 (SAM), Doc. no. SAM-UCL},
  volume={37},
  pages={64},
  year={1992}
}

@article{misc,
  title={Standard computer-compatible transcription},
  author={Wells, John and Barry, William and Grice, Martine and Fourcin, Adrian and Gibbon, Dafydd},
  journal={Esprit project 2589 (SAM), Doc. no. SAM-UCL},
  volume={37},
  pages={64},
  year={1992}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%
% Automatic transcriptions


@inproceedings{zissman1994automatic,
  title={Automatic language identification of telephone speech messages using phoneme recognition and n-gram modeling},
  author={Zissman, Marc A and Singer, Elliot},
  booktitle={Proceedings of ICASSP'94. IEEE International Conference on Acoustics, Speech and Signal Processing},
  volume={1},
  pages={I--305},
  year={1994},
  organization={IEEE}
}


%%%%%%%%%%%%%%%%%%%%%
% Phoneme Inventories

@book{duanmu2007phonology,
  title={The phonology of standard Chinese},
  author={Duanmu, San},
  year={2007},
  publisher={Oxford University Press}
}

@book{lin2007sounds,
  title={The Sounds of Chinese},
  author={Lin, Yen-Hwei},
  year={2007},
  publisher={Cambridge University Press}
}

@book{phoible,
  address   = {Jena},
  editor    = {Steven Moran and Daniel McCloy},
  publisher = {Max Planck Institute for the Science of Human History},
  title     = {PHOIBLE 2.0},
  url       = {https://phoible.org/},
  year      = {2019}
}


%%%%%%%%%%%%%%%%%%
% Phonemizer Tools

@article{Bernard2021,
  doi = {10.21105/joss.03958},
  url = {https://doi.org/10.21105/joss.03958},
  year = {2021},
  publisher = {The Open Journal},
  volume = {6},
  number = {68},
  pages = {3958},
  author = {Mathieu Bernard and Hadrien Titeux},
  title = {Phonemizer: Text to Phones Transcription for Multiple Languages in Python},
  journal = {Journal of Open Source Software}
}

@misc{Dunn2019,
    url = {https://github.com/espeak-ng/espeak-ng},
    publisher = {GitHub},
    year = {2022},
    title = {e{S}peak {NG} speech synthesizer. {I}n {G}it{H}ub respository ({V}ersion 1.51)},
    author = {R.H. Dunn and V. Vitolins}
}

@misc{robert_forkel_2019_3549784,
  author       = {Robert Forkel and
                  Steven Moran and
                  Johann-Mattis List and
                  Simon J Greenhill and
                  Lucas Ashby and
                  Kyle Gorman and
                  Gereon Kaiping},
  title        = {cldf/segments: Unicode Standard tokenization},
  month        = nov,
  year         = 2019,
  publisher    = {Zenodo},
  version      = {v2.1.3},
  doi          = {10.5281/zenodo.3549784},
  url          = {https://doi.org/10.5281/zenodo.3549784}
}

@InProceedings{Mortensen-et-al:2018,
  author = {Mortensen, David R.  and Dalmia, Siddharth and Littell, Patrick},
  title = {Epitran: Precision {G2P} for Many Languages},
  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},
  year = {2018},
  month = {May},
  date = {7--12},
  location = {Miyazaki, Japan},
  editor = {Nicoletta Calzolari (Conference chair) and Khalid Choukri and Christopher Cieri and Thierry Declerck and Sara Goggi and Koiti Hasida and Hitoshi Isahara and Bente Maegaard and Joseph Mariani and H\'el\`ene Mazo and Asuncion Moreno and Jan Odijk and Stelios Piperidis and Takenobu Tokunaga},
  publisher = {European Language Resources Association (ELRA)},
  address = {Paris, France},
  isbn = {979-10-95546-00-9},
  language = {english}
  }

@inproceedings{black2001flite,
  title={Flite: a small fast run-time synthesis engine},
  author={Black, Alan W and Lenzo, Kevin A},
  booktitle={4th ISCA Tutorial and Research Workshop (ITRW) on Speech Synthesis},
  year={2001}
}

@misc{taubert_2024_pinyin-to-ipa_2024,
    author = {Taubert, Stefan},
    doi = {10.5281/zenodo.10639971},
    license = {MIT},
    month = feb,
    title = {{pinyin-to-ipa}},
    url = {https://github.com/stefantaubert/pinyin-to-ipa},
    version = {0.0.2},
    year = {2024}
}

@inproceedings{lhoest-etal-2021-datasets,
    title = "Datasets: A Community Library for Natural Language Processing",
    author = "Lhoest, Quentin  and
      Villanova del Moral, Albert  and
      Jernite, Yacine  and
      Thakur, Abhishek  and
      von Platen, Patrick  and
      Patil, Suraj  and
      Chaumond, Julien  and
      Drame, Mariama  and
      Plu, Julien  and
      Tunstall, Lewis  and
      Davison, Joe  and
      {\v{S}}a{\v{s}}ko, Mario  and
      Chhablani, Gunjan  and
      Malik, Bhavitvya  and
      Brandeis, Simon  and
      Le Scao, Teven  and
      Sanh, Victor  and
      Xu, Canwen  and
      Patry, Nicolas  and
      McMillan-Major, Angelina  and
      Schmid, Philipp  and
      Gugger, Sylvain  and
      Delangue, Cl{\'e}ment  and
      Matussi{\`e}re, Th{\'e}o  and
      Debut, Lysandre  and
      Bekman, Stas  and
      Cistac, Pierric  and
      Goehringer, Thibault  and
      Mustar, Victor  and
      Lagunas, Fran{\c{c}}ois  and
      Rush, Alexander  and
      Wolf, Thomas",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-demo.21",
    pages = "175--184",
    abstract = "The scale, variety, and quantity of publicly-available NLP datasets has grown rapidly as researchers propose new tasks, larger models, and novel benchmarks. Datasets is a community library for contemporary NLP designed to support this ecosystem. Datasets aims to standardize end-user interfaces, versioning, and documentation, while providing a lightweight front-end that behaves similarly for small datasets as for internet-scale corpora. The design of the library incorporates a distributed, community-driven approach to adding datasets and documenting usage. After a year of development, the library now includes more than 650 unique datasets, has more than 250 contributors, and has helped support a variety of novel cross-dataset research projects and shared tasks. The library is available at https://github.com/huggingface/datasets.",
    eprint={2109.02846},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
}

%%%%%%%%%%%%%%%%
% MODEL CHAPTER

@book{beinborn2024cognitive,
  title={Cognitive plausibility in natural language processing},
  author={Beinborn, Lisa and Hollenstein, Nora},
  year={2024},
  publisher={Springer}
}

% ---- Historical Background ----
@article{elman-1990-finding,
  title={Finding structure in time},
  author={Elman, Jeffrey L},
  journal={Cognitive science},
  volume={14},
  number={2},
  pages={179--211},
  year={1990},
  publisher={Wiley Online Library}
}

@article{prince-1997-optimality,
  title={Optimality: From neural networks to universal grammar},
  author={Prince, Alan and Smolensky, Paul},
  journal={Science},
  volume={275},
  number={5306},
  pages={1604--1610},
  year={1997},
  publisher={American Association for the Advancement of Science}
}

@article{seidenberg-1989-word-recognition,
   author = {Mark S. Seidenberg and James L. McClelland},
   doi = {10.1037/0033-295X.96.4.523},
   issn = {0033295X},
   issue = {4},
   journal = {Psychological Review},
   pages = {523-568},
   pmid = {2798649},
   title = {A Distributed, Developmental Model of Word Recognition and Naming},
   volume = {96},
   year = {1989},
}


% ---- Human Language Acquisition Background ----

% interaction as important to language acquisition
@book{clark-2009-first,
  title={First Language Acquisition},
  author={Clark, Eve V},
  year={2009},
  publisher={Cambridge University Press},
  pages={21--50}
}

% pointing as isual information 
@article{povinelli-1997-pointing,
title = {Exploitation of pointing as a referential gesture in young children, but not adolescent chimpanzees},
journal = {Cognitive Development},
volume = {12},
number = {4},
pages = {423-461},
year = {1997},
issn = {0885-2014},
doi = {https://doi.org/10.1016/S0885-2014(97)90017-4},
url = {https://www.sciencedirect.com/science/article/pii/S0885201497900174},
author = {Daniel J. Povinelli and James E. Reaux and Donna T. Bierschwale and Ashley D. Allain and Bridgett B. Simon},
}


% stress signal in voice used by children 
@article{jusczyk-1999-stress-voice,
  title={The beginnings of word segmentation in English-learning infants},
  author={Jusczyk, Peter W and Houston, Derek M and Newsome, Mary},
  journal={Cognitive psychology},
  volume={39},
  number={3-4},
  pages={159--207},
  year={1999},
  publisher={Elsevier}
}



% ---- Use of LMs for acquisition studies ----

@inproceedings{hu-etal-2020-systematic,
    title = "A Systematic Assessment of Syntactic Generalization in Neural Language Models",
    author = "Hu, Jennifer  and
      Gauthier, Jon  and
      Qian, Peng  and
      Wilcox, Ethan  and
      Levy, Roger",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.158",
    doi = "10.18653/v1/2020.acl-main.158",
    pages = "1725--1744",
}

@article{kirov-2018-recurrent,
	author = {Kirov, Christo and Cotterell, Ryan},
	doi = {10.1162/tacl_a_00247},
	eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00247/1567668/tacl\_a\_00247.pdf},
	issn = {2307-387X},
	journal = {Transactions of the Association for Computational Linguistics},
	month = {12},
	pages = {651-665},
	title = {{Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate}},
	url = {https://doi.org/10.1162/tacl\_a\_00247},
	volume = {6},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1162/tacl%5C_a%5C_00247},
	bdsk-url-2 = {https://doi.org/10.1162/tacl_a_00247}
}

@article{trott-2023-large,
  title={Do large language models know what humans know?},
  author={Trott, Sean and Jones, Cameron and Chang, Tyler and Michaelov, James and Bergen, Benjamin},
  journal={Cognitive Science},
  year={2023},
  publisher={Wiley-Blackwell Publishing Ltd.}
}

% theory of mind 
@article{strachan-2024-tom,
  title={Testing theory of mind in large language models and humans},
  author={Strachan, James WA and Albergo, Dalila and Borghini, Giulia and Pansardi, Oriana and Scaliti, Eugenio and Gupta, Saurabh and Saxena, Krati and Rufo, Alessandro and Panzeri, Stefano and Manzi, Guido and others},
  journal={Nature Human Behaviour},
  pages={1--11},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{dupoux-2018-cognitive,
  title={Cognitive science in the era of artificial intelligence: A roadmap for reverse-engineering the infant language-learner},
  author={Dupoux, Emmanuel},
  journal={Cognition},
  volume={173},
  pages={43--59},
  year={2018},
  publisher={Elsevier}
}

@incollection{warstadt-2022-artificial,
  title={What artificial neural networks can tell us about human language acquisition},
  author={Warstadt, Alex and Bowman, Samuel R},
  booktitle={Algebraic structures in natural language},
  pages={17--60},
  year={2022},
  publisher={CRC Press}
}

@incollection{baroni-2022-proper,
  title={On the proper role of linguistically oriented deep net analysis in linguistic theorising},
  author={Baroni, Marco},
  booktitle={Algebraic structures in natural language},
  pages={1--16},
  year={2022},
  publisher={CRC Press}
}

@inproceedings{evanson-2023-language,
  title={Language acquisition: do children and language models follow similar learning stages?},
  author={Evanson, Linnea and Lakretz, Yair and King, Jean R{\'e}mi},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={12205--12218},
  year={2023}
}

@article{seyssel-2023-realistic,
  title={Realistic and broad-scope learning simulations: first results and challenges},
  author={de Seyssel, Maureen and Lavechin, Marvin and Dupoux, Emmanuel},
  journal={Journal of Child Language},
  volume={50},
  number={6},
  pages={1294--1317},
  year={2023},
  publisher={Cambridge University Press}
}

@inproceedings{hollenstein-2021-reading-times,
  title={Multilingual Language Models Predict Human Reading Behavior},
  author={Hollenstein, Nora and Pirovano, Federico and Zhang, Ce and J{\"a}ger, Lena and Beinborn, Lisa},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={106--123},
  year={2021}
}

% --- Modeling Brain Signal ---- 

@article{caucheteux-2023-predictive-coding,
  title={Evidence of a predictive coding hierarchy in the human brain listening to speech},
  author={Caucheteux, Charlotte and Gramfort, Alexandre and King, Jean-R{\'e}mi},
  journal={Nature human behaviour},
  volume={7},
  number={3},
  pages={430--441},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{jain-2018-fmri-context,
  title={Incorporating context into language encoding models for fMRI},
  author={Jain, Shailee and Huth, Alexander},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

% ---- LLM Architectures ----

% LLAMA-2
@article{touvron-2023-llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

% PALM
@article{chowdhery-2023-palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}

% GPT-2
@article{radford-2019-gpt2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}


% GPT-3
@article{brown-2020-gpt3,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

% PYTHIA 
@inproceedings{biderman2023pythia,
  title={Pythia: A suite for analyzing large language models across training and scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O’Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  booktitle={International Conference on Machine Learning},
  pages={2397--2430},
  year={2023},
}

@inproceedings{hoffmann-2022-chinchilla,
     author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Thomas and Noland, Eric and Millican, Katherine and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Kar\'{e}n and Elsen, Erich and Vinyals, Oriol and Rae, Jack and Sifre, Laurent},
     booktitle = {Advances in Neural Information Processing Systems},
     editor={S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
     pages={30016--30030},
     title={An empirical analysis of compute-optimal large language model training},
     url={https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf},
     volume={35},
     year={2022}
}


% ---- Tokenization Approaches ----

% BPE 
@inproceedings{sennrich-etal-2016-bpe,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    editor = "Erk, Katrin  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1162",
    doi = "10.18653/v1/P16-1162",
    pages = "1715--1725",
}

% byte-pair encoding compression 
@article{gage-1994-compression-algorithm,
  title={A new algorithm for data compression},
  author={Gage, Philip},
  journal={The C Users Journal},
  volume={12},
  number={2},
  pages={23--38},
  year={1994},
  publisher={R \& D Publications, Inc. Lawrence, KS, USA}
}

% Unigram 
@inproceedings{kudo-2018-unigram,
  title={Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates},
  author={Kudo, Taku},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={66--75},
  year={2018}
}

% Word Piece 
@inproceedings{schuster-etal-2012-wordpiece,
  title={Japanese and korean voice search},
  author={Schuster, Mike and Nakajima, Kaisuke},
  booktitle={2012 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={5149--5152},
  year={2012},
}

% Sentence Piece 
@inproceedings{kudo-richardson-2018-sentencepiece,
    title = "{S}entence{P}iece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing",
    author = "Kudo, Taku  and
      Richardson, John",
    editor = "Blanco, Eduardo  and
      Lu, Wei",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-2012",
    doi = "10.18653/v1/D18-2012",
    pages = "66--71",
}


@article{hahn-baroni-2019-tabula,
    title = "Tabula Nearly Rasa: Probing the Linguistic Knowledge of Character-level Neural Language Models Trained on Unsegmented Text",
    author = "Hahn, Michael  and
      Baroni, Marco",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q19-1033",
    doi = "10.1162/tacl_a_00283",
    pages = "467--484",
}

% Canine 
@article{clark-2022-canine,
  title={Canine: Pre-training an efficient tokenization-free encoder for language representation},
  author={Clark, Jonathan H and Garrette, Dan and Turc, Iulia and Wieting, John},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={73--91},
  year={2022},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{xue-2022-byt5,
  title={Byt5: Towards a token-free future with pre-trained byte-to-byte models},
  author={Xue, Linting and Barua, Aditya and Constant, Noah and Al-Rfou, Rami and Narang, Sharan and Kale, Mihir and Roberts, Adam and Raffel, Colin},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={291--306},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

% --- Character level models ---


@inproceedings{sun-etal-2023-characters,
    title = "From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding",
    author = "Sun, Li  and
      Luisier, Florian  and
      Batmanghelich, Kayhan  and
      Florencio, Dinei  and
      Zhang, Cha",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.200",
    doi = "10.18653/v1/2023.acl-long.200",
    pages = "3605--3620",
}

@article{al-rfou_character-level_2019,
	title = {Character-level Language Modeling with Deeper Self-Attention},
	volume = {33},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4182},
	doi = {10.1609/aaai.v33i01.33013159},
	number = {01},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Al-Rfou, Rami and Choe, Dokook and Constant, Noah and Guo, Mandy and Jones, Llion},
	month = jul,
	year = {2019},
	pages = {3159--3166},
}

@misc{jozefowicz2016exploringlimitslanguagemodeling,
      title={Exploring the Limits of Language Modeling}, 
      author={Rafal Jozefowicz and Oriol Vinyals and Mike Schuster and Noam Shazeer and Yonghui Wu},
      year={2016},
      eprint={1602.02410},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1602.02410}, 
}

@inproceedings{ma-etal-2020-charbert,
    title = "{C}har{BERT}: Character-aware Pre-trained Language Model",
    author = "Ma, Wentao  and
      Cui, Yiming  and
      Si, Chenglei  and
      Liu, Ting  and
      Wang, Shijin  and
      Hu, Guoping",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.4",
    doi = "10.18653/v1/2020.coling-main.4",
    pages = "39--50",
}

@inproceedings{libovicky-fraser-2020-towards,
    title = "Towards Reasonably-Sized Character-Level Transformer {NMT} by Finetuning Subword Systems",
    author = "Libovick{\'y}, Jind{\v{r}}ich  and
      Fraser, Alexander",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.203",
    doi = "10.18653/v1/2020.emnlp-main.203",
    pages = "2572--2579",
}

@inproceedings{kim2016character,
  title={Character-aware neural language models},
  author={Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={30},
  year={2016}
}


% ---- Phonological Knowledge ----

@inproceedings{feliciano-de-faria-2019-utterance-boundaries,
    title = "The Role of Utterance Boundaries and Word Frequencies for Part-of-speech Learning in {B}razilian {P}ortuguese Through Distributional Analysis",
    author = "Feliciano de Faria, Pablo Picasso",
    editor = "Chersoni, Emmanuele  and
      Jacobs, Cassandra  and
      Lenci, Alessandro  and
      Linzen, Tal  and
      Pr{\'e}vot, Laurent  and
      Santus, Enrico",
    booktitle = "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-2917",
    doi = "10.18653/v1/W19-2917",
    pages = "152--159",
}


@phdthesis{eden-2018-phonological-distance,
  title={Measuring phonological distance between languages},
  author={Eden, S Elizabeth},
  year={2018},
  school={UCL (University College London)}
}


@article{mayer-2020-phonology-distribution,
  title={An algorithm for learning phonological classes from distributional similarity},
  author={Mayer, Connor},
  journal={Phonology},
  volume={37},
  number={1},
  pages={91--131},
  year={2020},
  publisher={Cambridge University Press}
}


% ---- Probing/ Linguistic Knowledge ----

@article{nguyen-2022-word-boundaries,
  title={Are word boundaries useful for unsupervised language learning?},
  author={Nguyen, Tu Anh and De Seyssel, Maureen and Algayres, Robin and Roze, Patricia and Dunbar, Ewan and Dupoux, Emmanuel},
  journal={arXiv preprint arXiv:2210.02956},
  year={2022}
}


@article{pasad-2024-know-about-words,
  title={What do self-supervised speech models know about words?},
  author={Pasad, Ankita and Chien, Chung-Ming and Settle, Shane and Livescu, Karen},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={372--391},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

% syntax trees are encoded in the geometry of BERT 
@inproceedings{hewitt-manning-2019-structural,
    title = "{A} Structural Probe for Finding Syntax in Word Representations",
    author = "Hewitt, John  and
      Manning, Christopher D.",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1419",
    doi = "10.18653/v1/N19-1419",
    pages = "4129--4138",
}

@article{manning-2020-emergent,
  title={Emergent linguistic structure in artificial neural networks trained by self-supervision},
  author={Manning, Christopher D and Clark, Kevin and Hewitt, John and Khandelwal, Urvashi and Levy, Omer},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={48},
  pages={30046--30054},
  year={2020},
  publisher={National Acad Sciences}
}

% ---- IPA-model training methodology ----- 

@article{matsuhira-2023-ipaclip,
  title={IPA-CLIP: Integrating phonetic priors into vision and language pretraining},
  author={Matsuhira, Chihaya and Kastner, Marc A and Komamizu, Takahiro and Hirayama, Takatsugu and Doman, Keisuke and Kawanishi, Yasutomo and Ide, Ichiro},
  journal={arXiv preprint arXiv:2303.03144},
  year={2023}
}

@inproceedings{gale-etal-2023-bort,
    title = "Mixed Orthographic/Phonemic Language Modeling: Beyond Orthographically Restricted Transformers ({BORT})",
    author = "Gale, Robert  and
      Salem, Alexandra  and
      Fergadiotis, Gerasimos  and
      Bedrick, Steven",
    booktitle = "Proceedings of the 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.repl4nlp-1.18",
    doi = "10.18653/v1/2023.repl4nlp-1.18",
    pages = "212--225",
}


@article{hsu-2021-hubert,
  title={Hubert: Self-supervised speech representation learning by masked prediction of hidden units},
  author={Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
  journal={IEEE/ACM transactions on audio, speech, and language processing},
  volume={29},
  pages={3451--3460},
  year={2021},
}


% ----- Applications of IPA-training ----- 

@article{ding-2024-songcomposer,
  title={Songcomposer: A large language model for lyric and melody composition in song generation},
  author={Ding, Shuangrui and Liu, Zihan and Dong, Xiaoyi and Zhang, Pan and Qian, Rui and He, Conghui and Lin, Dahua and Wang, Jiaqi},
  journal={arXiv preprint arXiv:2402.17645},
  year={2024}
}

@inproceedings{ye-2021-pngbert, 
    title={PnG BERT: Augmented BERT on Phonemes and Graphemes for Neural TTS},
    author={Ye Jia and Heiga Zen (Byungha Chun) and Jonathan Shen and Yu Zhang and Yonghui Wu},
    year={2021},
    URL={https://arxiv.org/abs/2103.15060},
    booktitle={Interspeech}
}


@inproceedings{sundararaman-2021-phonemebert,
  author={Mukuntha Narayanan Sundararaman and Ayush Kumar and Jithendra Vepa},
  title={{PhonemeBERT: Joint Language Modelling of Phoneme Sequence and ASR Transcript}},
  year=2021,
  booktitle={Proc. Interspeech 2021},
  pages={3236--3240},
  doi={10.21437/Interspeech.2021-1582},
  issn={2958-1796}
}

@inproceedings{li-2023-phoneme-level-bert,
  title={Phoneme-level {BERT} for enhanced prosody of text-to-speech with grapheme predictions},
  author={Li, Yinghao Aaron and Han, Cong and Jiang, Xilin and Mesgarani, Nima},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
}

@inproceedings{feng-2023-language-universal-phonetic,
  title={Language-Universal Phonetic Representation in Multilingual Speech Pretraining for Low-Resource Speech Recognition },
  author={Feng, Siyuan and Tu, Ming and Xia, Rui and Huang, Chuanzeng and Wang, Yuxuan},
  BOOKTITLE = {{INTERSPEECH 2023}},
  ADDRESS = {Dublin, Ireland},
  PUBLISHER = {{ISCA}},
  year={2023}
}

@inproceedings{zhu-2024-taste,
  title={The taste of IPA: Towards open-vocabulary keyword spotting and forced alignment in any language},
  author={Zhu, Jian and Yang, Changbing and Samir, Farhan and Islam, Jahurul},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={750--772},
  year={2024}
}




% example of datasets that contain phonetic transcriptions 

@inproceedings{elazar-2024-redpajama,
  title={What's In My Big Data?},
  author={Elazar, Yanai and Bhagia, Akshita and Magnusson, Ian Helgi and Ravichander, Abhilasha and Schwenk, Dustin and Suhr, Alane and Walsh, Evan Pete and Groeneveld, Dirk and Soldaini, Luca and Singh, Sameer and others},
  booktitle={The Twelfth International Conference on Learning Representations},
  year = {2024}
}

% Example of tools for converting text phonemes 

@article{bisani-2008-g2p,
  title={Joint-sequence models for grapheme-to-phoneme conversion},
  author={Bisani, Maximilian and Ney, Hermann},
  journal={Speech communication},
  volume={50},
  number={5},
  pages={434--451},
  year={2008},
  publisher={Elsevier}
}

@inproceedings{hasegawa-2020-g2pmultilingual,
  title={Grapheme-to-phoneme transduction for cross-language ASR},
  author={Hasegawa-Johnson, Mark and Rolston, Leanne and Goudeseune, Camille and Levow, Gina-Anne and Kirchhoff, Katrin},
  booktitle={International Conference on Statistical Language and Speech Processing},
  pages={3--19},
  year={2020},
  organization={Springer}
}



% ----- Applications of IPA-training ----- 

@article{ivanova2024elements,
  title={Elements of World Knowledge (EWOK): A cognition-inspired framework for evaluating basic world knowledge in language models},
  author={Anna A. Ivanova and Aalok Sathe and Benjamin Lipkin and Unnathi Kumar and Setayesh Radkani and Thomas H. Clark and Carina Kauf and Jennifer Hu and R. T. Pramod and Gabriel Grand and Vivian Paulun and Maria Ryskina and Ekin Akyurek and Ethan Wilcox and Nafisa Rashid and Leshem Choshen and Roger Levy and Evelina Fedorenko and Joshua Tenenbaum and Jacob Andreas},
  journal={arXiv preprint arXiv:2405.09605},
  year={2024},
  url={https://arxiv.org/abs/2405.09605}
}

@article{warstadt-2020-blimp,
  title={{BLiMP}: The benchmark of linguistic minimal pairs for English},
  author={Warstadt, Alex and Parrish, Alicia and Liu, Haokun and Mohananey, Anhad and Peng, Wei and Wang, Sheng-Fu and Bowman, Samuel R},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={377--392},
  year={2020},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{suvarna-etal-2024-phonologybench,
    title = "{P}honology{B}ench: Evaluating Phonological Skills of Large Language Models",
    author = "Suvarna, Ashima  and
      Khandelwal, Harshita  and
      Peng, Nanyun",
    booktitle = "Proceedings of the 1st Workshop on Towards Knowledgeable Language Models (KnowLLM 2024)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.knowllm-1.1",
    pages = "1--14",
}

@inproceedings{li2023phoneme,
  title={Phoneme-level bert for enhanced prosody of text-to-speech with grapheme predictions},
  author={Li, Yinghao Aaron and Han, Cong and Jiang, Xilin and Mesgarani, Nima},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
}


@inproceedings{leong-2022-phone,
  title={Phone-ing it in: Towards flexible multi-modal language model training by phonetic representations of data},
  author={Leong, Colin and Whitenack, Daniel},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={5306--5315},
  year={2022}
}

@inproceedings{lavechin,
  TITLE = {{BabySLM: language-acquisition-friendly benchmark of self-supervised spoken language models}},
  AUTHOR = {Lavechin, Marvin and Sy, Yaya and Titeux, Hadrien and Bland{\'o}n, Mar{\'i}a Andrea Cruz and R{\"a}s{\"a}nen, Okko and Bredin, Herv{\'e} and Dupoux, Emmanuel and Cristia, Alejandrina},
  URL = {https://hal.science/hal-04247612},
  BOOKTITLE = {{INTERSPEECH 2023}},
  ADDRESS = {Dublin, Ireland},
  PUBLISHER = {{ISCA}},
  PAGES = {4588-4592},
  YEAR = {2023},
  MONTH = Aug,
  DOI = {10.21437/Interspeech.2023-978},
  KEYWORDS = {Spoken language modeling ; Language acquisition ; Self-supervised learning ; Child language},
  PDF = {https://hal.science/hal-04247612/file/_Interspeech_2023__BabySLM.pdf},
  HAL_ID = {hal-04247612},
  HAL_VERSION = {v1},
}

% --- Audio models ---

@article{schatz2021early,
  title={Early phonetic learning without phonetic categories: Insights from large-scale simulations on realistic input},
  author={Schatz, Thomas and Feldman, Naomi H and Goldwater, Sharon and Cao, Xuan-Nga and Dupoux, Emmanuel},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={7},
  pages={e2001844118},
  year={2021},
  publisher={National Acad Sciences}
}

@misc{lavechin2022can,
  title={Can statistical learning bootstrap early language acquisition? A modeling investigation},
  author={Lavechin, Marvin and De Seyssel, Maureen and Titeux, Hadrien and Bredin, Herv{\'e} and Wisniewski, Guillaume and Cristia, Alejandrina and Dupoux, Emmanuel},
  year={2022},
  publisher={PsyArXiv}
}

@article{matusevych2023infant,
  title={Infant phonetic learning as perceptual space learning: A crosslinguistic evaluation of computational models},
  author={Matusevych, Yevgen and Schatz, Thomas and Kamper, Herman and Feldman, Naomi H and Goldwater, Sharon},
  journal={Cognitive Science},
  volume={47},
  number={7},
  pages={e13314},
  year={2023},
  publisher={Wiley Online Library}
}

@article{dunbar_self-supervised_2022,
	title = {Self-{Supervised} {Language} {Learning} {From} {Raw} {Audio}: {Lessons} {From} the {Zero} {Resource} {Speech} {Challenge}},
	volume = {16},
	issn = {1941-0484},
	shorttitle = {Self-{Supervised} {Language} {Learning} {From} {Raw} {Audio}},
	doi = {10.1109/JSTSP.2022.3206084},
	number = {6},
	journal = {IEEE Journal of Selected Topics in Signal Processing},
	author = {Dunbar, Ewan and Hamilakis, Nicolas and Dupoux, Emmanuel},
	month = oct,
	year = {2022},
	note = {Conference Name: IEEE Journal of Selected Topics in Signal Processing},
	keywords = {Benchmark testing, representation learning, Representation learning, Self-supervised learning, Speech processing, Textless speech processing, unsupervised and self-supervised learning, Unsupervised learning},
	pages = {1211--1226},
}

% ----- Miscellaneous ----- 

@article{kazanina2018phonemes,
  title={Phonemes: Lexical access and beyond},
  author={Kazanina, Nina and Bowers, Jeffrey S and Idsardi, William},
  journal={Psychonomic bulletin \& review},
  volume={25},
  number={2},
  pages={560--585},
  year={2018},
  publisher={Springer}
}

% 2024 BabyLM Call for Papers
@article{choshen-et-al-2024-callforpapers-babylm2,
  title={Call for Papers -- {The BabyLM Challenge}: Sample-efficient pretraining on a developmentally plausible corpus},
  author={Choshen, Leshem and Cotterell, Ryan and Hu, Michael Y and Linzen, Tal and Mueller, Aaron and Ross, Candace and Warstadt, Alex and Wilcox, Ethan and Williams, Adina and Zhuang, Chengxu},
  journal={arXiv preprint arXiv:2404.06214},
  year={2024}
}

% 2023 BabyLM proceedings 
@inproceedings{warstadt-2023-babylm-findings,
    title = "Findings of the {B}aby{LM} Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora",
    author = "Warstadt, Alex  and
      Mueller, Aaron  and
      Choshen, Leshem  and
      Wilcox, Ethan  and
      Zhuang, Chengxu  and
      Ciro, Juan  and
      Mosquera, Rafael  and
      Paranjabe, Bhargavi  and
      Williams, Adina  and
      Linzen, Tal  and
      Cotterell, Ryan",
    booktitle = "Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.conll-babylm.1",
    doi = "10.18653/v1/2023.conll-babylm.1",
    pages = "1--34",
}

%babyllama
@inproceedings{timiryasov-tastet-2023-baby,
    title = "Baby {L}lama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty",
    author = "Timiryasov, Inar  and
      Tastet, Jean-Loup",
    editor = "Warstadt, Alex  and
      Mueller, Aaron  and
      Choshen, Leshem  and
      Wilcox, Ethan  and
      Zhuang, Chengxu  and
      Ciro, Juan  and
      Mosquera, Rafael  and
      Paranjabe, Bhargavi  and
      Williams, Adina  and
      Linzen, Tal  and
      Cotterell, Ryan",
    booktitle = "Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.conll-babylm.24",
    doi = "10.18653/v1/2023.conll-babylm.24",
    pages = "279--289",
}

%ltg-bert
@inproceedings{charpentier-samuel-2023-layers,
    title = "Not all layers are equally as important: Every Layer Counts {BERT}",
    author = "Charpentier, Lucas Georges Gabriel and
      Samuel, David",
    editor = "Warstadt, Alex  and
      Mueller, Aaron  and
      Choshen, Leshem  and
      Wilcox, Ethan  and
      Zhuang, Chengxu  and
      Ciro, Juan  and
      Mosquera, Rafael  and
      Paranjabe, Bhargavi  and
      Williams, Adina  and
      Linzen, Tal  and
      Cotterell, Ryan",
    booktitle = "Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.conll-babylm.20",
    doi = "10.18653/v1/2023.conll-babylm.20",
    pages = "238--252",
}

%ltg-bert
@inproceedings{samuel-etal-2023-trained,
    title = "Trained on 100 million words and still in shape: {BERT} meets {B}ritish {N}ational {C}orpus",
    author = "Samuel, David  and
      Kutuzov, Andrey  and
      {\O}vrelid, Lilja  and
      Velldal, Erik",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Findings of the Association for Computational Linguistics: EACL 2023",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-eacl.146",
    doi = "10.18653/v1/2023.findings-eacl.146",
    pages = "1954--1974",
    abstract = "While modern masked language models (LMs) are trained on ever larger corpora, we here explore the effects of down-scaling training to a modestly-sized but representative, well-balanced, and publicly available English text source {--} the British National Corpus. We show that pre-training on this carefully curated corpus can reach better performance than the original BERT model. We argue that this type of corpora has great potential as a language modeling benchmark. To showcase this potential, we present fair, reproducible and data-efficient comparative studies of LMs, in which we evaluate several training objectives and model architectures and replicate previous empirical results in a systematic way. We propose an optimized LM architecture called LTG-BERT.",
}

% Zipf's law 
@book{manning2009introduction,
  title={An introduction to information retrieval},
  author={Manning, Christopher D},
  year={2009},
  publisher={Cambridge university press}
}

@book{leon-cristia-2024,
    author = { Mathilde Léon and Alejandrina Cristia },
    title = {Data Protection Handbook for Long-Form Recording Research: Navigating Data Protection Laws across the Globe},
    year = {2024},
    publisher={OSF},
url = {https://doi.org/10.31219/osf.io/dy4wt}
}

@inproceedings{martinez-etal-2023-climb,
    title = "{CLIMB} {--} Curriculum Learning for Infant-inspired Model Building",
    author = "Diehl Martinez, Richard   and
      McGovern, Hope  and
      Goriely, Zebulon  and
      Davis, Christopher  and
      Caines, Andrew  and
      Buttery, Paula  and
      Beinborn, Lisa",
    editor = "Warstadt, Alex  and
      Mueller, Aaron  and
      Choshen, Leshem  and
      Wilcox, Ethan  and
      Zhuang, Chengxu  and
      Ciro, Juan  and
      Mosquera, Rafael  and
      Paranjabe, Bhargavi  and
      Williams, Adina  and
      Linzen, Tal  and
      Cotterell, Ryan",
    booktitle = "Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.conll-babylm.10",
    pages = "84--99",
}

% ----- Standard Benchmarks ----- 

@inproceedings{wang-etal-2018-glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    editor = "Linzen, Tal  and
      Chrupa{\l}a, Grzegorz  and
      Alishahi, Afra",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5446",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
}

@inproceedings{wang-etal-2019-superglue,
 author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 title = {{SuperGLUE}: A Stickier Benchmark for General-Purpose Language Understanding Systems},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/4496bf24afe7fab6f046bf4923da8de6-Paper.pdf},
 volume = {32},
 year = {2019}
}


@inproceedings{suzgun-2023-Big-Bench,
    title = "Challenging {BIG}-Bench Tasks and Whether Chain-of-Thought Can Solve Them",
    author = {Suzgun, Mirac  and
      Scales, Nathan  and
      Sch{\"a}rli, Nathanael  and
      Gehrmann, Sebastian  and
      Tay, Yi  and
      Chung, Hyung Won  and
      Chowdhery, Aakanksha  and
      Le, Quoc  and
      Chi, Ed  and
      Zhou, Denny  and
      Wei, Jason},
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.824",
    doi = "10.18653/v1/2023.findings-acl.824",
    pages = "13003--13051",
}


@inproceedings{zellers-2019-hellaswag,
  title={HellaSwag: Can a Machine Really Finish Your Sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4791--4800},
  year={2019}
}

@inproceedings{hendrycks-2020-mmlu,
  title={Measuring Massive Multitask Language Understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{wolf-etal-2020-transformers,
	title        = {Transformers: {S}tate-of-the-Art Natural Language Processing},
	author       = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and {von Platen}, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
	year         = 2020,
	month        = oct,
	booktitle    = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{System Demonstrations}}},
	publisher    = {{Association for Computational Linguistics}},
	address      = {{Online}},
	pages        = {38--45},
	doi          = {10.18653/v1/2020.emnlp-demos.6},
	url          = {https://aclanthology.org/2020.emnlp-demos.6}
}

@inproceedings{paszke-etal-2019-pytorch,
	title        = {{{PyTorch}}: {A}n Imperative Style, High-Performance Deep Learning Library},
	author       = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year         = 2019,
	booktitle    = {Advances in {{Neural Information Processing Systems}}},
	volume       = 32,
	url          = {https://proceedings.neurips.cc/paper\_files/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html}
}


@inproceedings{bansal-2022-datascaling,
  title={Data scaling laws in {NMT}: The effect of noise and architecture},
  author={Bansal, Yamini and Ghorbani, Behrooz and Garg, Ankush and Zhang, Biao and Cherry, Colin and Neyshabur, Behnam and Firat, Orhan},
  booktitle={International Conference on Machine Learning},
  pages={1466--1482},
  year={2022},
  organization={PMLR}
}

@inproceedings{huebner-etal-2021-babyberta,
    title = "{B}aby{BERT}a: Learning More Grammar With Small-Scale Child-Directed Language",
    author = "Huebner, Philip A.  and
      Sulem, Elior  and
      Cynthia, Fisher  and
      Roth, Dan",
    editor = "Bisazza, Arianna  and
      Abend, Omri",
    booktitle = "Proceedings of the 25th Conference on Computational Natural Language Learning",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.conll-1.49",
    doi = "10.18653/v1/2021.conll-1.49",
    pages = "624--646",
}

@article{coleman2011mining,
  title={Mining a year of speech},
  author={Coleman, John and Liberman, Mark and Kochanski, Greg and Burnard, Lou and Yuan, Jiahong},
  journal={VLSP 2011: New tools and methods for very-large-scale phonetics research},
  pages={16--19},
  year={2011}
}

@inproceedings{borschinger2013joint,
  title={A joint model of word segmentation and phonological variation for English word-final/t/-deletion},
  author={B{\"o}rschinger, Benjamin and Johnson, Mark and Demuth, Katherine},
  booktitle={Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1508--1516},
  year={2013}
}

@article{Blanchard2010,
author = {Blanchard, Daniel and Heinz, Jeffrey and Golinkoff, Roberta},
doi = {10.1017/S030500090999050X},
issn = {03050009},
journal = {Journal of Child Language},
number = {3},
pages = {487--511},
pmid = {20307346},
publisher = {Cambridge University Press},
title = {{Modeling the contribution of phonotactic cues to the problem of word segmentation}},
volume = {37},
year = {2010}
}

@article{Coltekin2017,
author = {{\c{C}}{\"{o}}ltekin, {\c{C}}ağrı},
doi = {10.1111/cogs.12454},
issn = {03640213},
journal = {Cognitive Science},
keywords = {Computational modeling,Language acquisition,Predictability entropy,Word segmentation},
month = {sep},
number = {7},
pages = {1988--2021},
publisher = {Wiley-Blackwell Publishing},
title = {{Using Predictability for Lexical Segmentation}},
url = {http://doi.wiley.com/10.1111/cogs.12454},
volume = {41},
year = {2017}
}

@article{algayres_dp-parse_2022,
	title = {{DP}-{Parse}: {Finding} {Word} {Boundaries} from {Raw} {Speech} with an {Instance} {Lexicon}},
	volume = {10},
	issn = {2307-387X},
	url = {https://doi.org/10.1162/tacl\_a\_00505},
	doi = {10.1162/tacl_a_00505},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Algayres, Robin and Ricoul, Tristan and Karadayi, Julien and Laurençon, Hugo and Zaiem, Salah and Mohamed, Abdelrahman and Sagot, Benoît and Dupoux, Emmanuel},
	month = sep,
	year = {2022},
	pages = {1051--1065},
}

@article{Brent1999,
archivePrefix = {arXiv},
arxivId = {cs/9905007},
author = {Brent, Michael R.},
doi = {10.1023/a:1007541817488},
eprint = {9905007},
issn = {08856125},
journal = {Machine Learning},
keywords = {bayesian grammar induction,language acquisition,minimum description length (MDL),probability models,segmentation,unsupervised learning},
number = {1},
pages = {71--105},
primaryClass = {cs},
publisher = {Kluwer Academic Publishers},
title = {{Efficient, probabilistically sound algorithm for segmentation and word discovery}},
url = {https://link.springer.com/article/10.1023/A:1007541817488},
volume = {34},
year = {1999}
}

@article{goriely2023word,
  title={Word segmentation from transcriptions of child-directed speech using lexical and sub-lexical cues},
  author={Goriely, Z{\'e}bulon and Caines, Andrew and Buttery, Paula},
  journal={Journal of Child Language},
  pages={1--41},
  year={2023},
  publisher={Cambridge University Press}
}

@inproceedings{panayotov2015librispeech,
  title={Librispeech: an {ASR} corpus based on public domain audio books},
  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle={2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={5206--5210},
  year={2015},
}

@article{bergelson-etal-2023,
author = {Elika Bergelson  and Melanie Soderstrom  and Iris-Corinna Schwarz  and Caroline F. Rowland  and Nairán Ramírez-Esparza  and Lisa R. Hamrick  and Ellen Marklund  and Marina Kalashnikova  and Ava Guez  and Marisa Casillas  and Lucia Benetti  and Petra van Alphen  and Alejandrina Cristia },
title = {Everyday language input and production in 1,001 children from six continents},
journal = {Proceedings of the National Academy of Sciences},
volume = {120},
number = {52},
pages = {e2300671120},
year = {2023},
doi = {10.1073/pnas.2300671120},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2300671120},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2300671120},
}


@inproceedings{fan-sun-2023-constructivist,
    title = "Constructivist Tokenization for {E}nglish",
    author = "Fan, Allison  and
      Sun, Weiwei",
    editor = "Bonial, Claire  and
      Tayyar Madabushi, Harish",
    booktitle = "Proceedings of the First International Workshop on Construction Grammars and NLP (CxGs+NLP, GURT/SyntaxFest 2023)",
    month = mar,
    year = "2023",
    address = "Washington, D.C.",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.cxgsnlp-1.5",
    pages = "36--40",
}

@inproceedings{batsuren-etal-2022-sigmorphon,
    title = "The {SIGMORPHON} 2022 Shared Task on Morpheme Segmentation",
    author = "Batsuren, Khuyagbaatar  and
      Bella, G{\'a}bor  and
      Arora, Aryaman  and
      Martinovic, Viktor  and
      Gorman, Kyle  and
      {\v{Z}}abokrtsk{\'y}, Zden{\v{e}}k  and
      Ganbold, Amarsanaa  and
      Dohnalov{\'a}, {\v{S}}{\'a}rka  and
      {\v{S}}ev{\v{c}}{\'\i}kov{\'a}, Magda  and
      Pelegrinov{\'a}, Kate{\v{r}}ina  and
      Giunchiglia, Fausto  and
      Cotterell, Ryan  and
      Vylomova, Ekaterina",
    editor = "Nicolai, Garrett  and
      Chodroff, Eleanor",
    booktitle = "Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology",
    month = jul,
    year = "2022",
    address = "Seattle, Washington",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.sigmorphon-1.11",
    doi = "10.18653/v1/2022.sigmorphon-1.11",
    pages = "103--116",
}

@inproceedings{ustun-etal-2018-characters,
    title = "Characters or Morphemes: How to Represent Words?",
    author = {{\"U}st{\"u}n, Ahmet  and
      Kurfal{\i}, Murathan  and
      Can, Burcu},
    editor = "Augenstein, Isabelle  and
      Cao, Kris  and
      He, He  and
      Hill, Felix  and
      Gella, Spandana  and
      Kiros, Jamie  and
      Mei, Hongyuan  and
      Misra, Dipendra",
    booktitle = "Proceedings of the Third Workshop on Representation Learning for {NLP}",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-3019",
    doi = "10.18653/v1/W18-3019",
    pages = "144--153",
}


@inproceedings{nzeyimana-niyongabo-rubungo-2022-kinyabert,
    title = "{K}inya{BERT}: a Morphology-aware {K}inyarwanda Language Model",
    author = "Nzeyimana, Antoine  and
      Niyongabo Rubungo, Andre",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.367",
    doi = "10.18653/v1/2022.acl-long.367",
    pages = "5347--5363",
    abstract = "Pre-trained language models such as BERT have been successful at tackling many natural language processing tasks. However, the unsupervised sub-word tokenization methods commonly used in these models (e.g., byte-pair encoding - BPE) are sub-optimal at handling morphologically rich languages. Even given a morphological analyzer, naive sequencing of morphemes into a standard BERT architecture is inefficient at capturing morphological compositionality and expressing word-relative syntactic regularities. We address these challenges by proposing a simple yet effective two-tier BERT architecture that leverages a morphological analyzer and explicitly represents morphological compositionality.Despite the success of BERT, most of its evaluations have been conducted on high-resource languages, obscuring its applicability on low-resource languages. We evaluate our proposed method on the low-resource morphologically rich Kinyarwanda language, naming the proposed model architecture KinyaBERT. A robust set of experimental results reveal that KinyaBERT outperforms solid baselines by 2{\%} in F1 score on a named entity recognition task and by 4.3{\%} in average score of a machine-translated GLUE benchmark. KinyaBERT fine-tuning has better convergence and achieves more robust results on multiple tasks even in the presence of translation noise.",
}


@article{warstadt-etal-2020-blimp-benchmark,
    title = "{BL}i{MP}: The Benchmark of Linguistic Minimal Pairs for {E}nglish",
    author = "Warstadt, Alex  and
      Parrish, Alicia  and
      Liu, Haokun  and
      Mohananey, Anhad  and
      Peng, Wei  and
      Wang, Sheng-Fu  and
      Bowman, Samuel R.",
    editor = "Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2020.tacl-1.25",
    doi = "10.1162/tacl_a_00321",
    pages = "377--392",
    abstract = "We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP),1 a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,000 minimal pairs{---}that is, pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. We generate the data according to linguist-crafted grammar templates, and human aggregate agreement with the labels is 96.4{\%}. We evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair. We find that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands.",
}

@inproceedings{borschinger-etal-2013-joint,
    title = "A joint model of word segmentation and phonological variation for {E}nglish word-final /t/-deletion",
    author = {B{\"o}rschinger, Benjamin  and
      Johnson, Mark  and
      Demuth, Katherine},
    editor = "Schuetze, Hinrich  and
      Fung, Pascale  and
      Poesio, Massimo",
    booktitle = "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P13-1148",
    pages = "1508--1516",
}

@inproceedings{evanson-etal-2023-language,
	title = "Language acquisition: do children and language models follow similar learning stages?",
	author = "Evanson, Linnea  and
	  Lakretz, Yair  and
	  King, Jean R{\'e}mi",
	editor = "Rogers, Anna  and
	  Boyd-Graber, Jordan  and
	  Okazaki, Naoaki",
	booktitle = "Findings of the Association For Computational Linguistics: ACL 2023",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-acl.773.pdf",
	doi = "10.18653/v1/2023.findings-acl.773",
	pages = "12205--12218",
}

@inproceedings{hollenstein-etal-2021-multilingual,
	title = "Multilingual Language Models Predict Human Reading Behavior",
	author = {Hollenstein, Nora  and
	  Pirovano, Federico  and
	  Zhang, Ce  and
	  J{\"a}ger, Lena  and
	  Beinborn, Lisa},
	editor = "Toutanova, Kristina  and
	  Rumshisky, Anna  and
	  Zettlemoyer, Luke  and
	  Hakkani-Tur, Dilek  and
	  Beltagy, Iz  and
	  Bethard, Steven  and
	  Cotterell, Ryan  and
	  Chakraborty, Tanmoy  and
	  Zhou, Yichao",
	booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
	month = jun,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-acl.773.pdf",
	doi = "10.18653/v1/2021.naacl-main.10",
	pages = "106--123",
}

@article{clark-etal-2022-canine,
    title = "Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation",
    author = "Clark, Jonathan H.  and
      Garrette, Dan  and
      Turc, Iulia  and
      Wieting, John",
    editor = "Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "10",
    year = "2022",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2022.tacl-1.5",
    doi = "10.1162/tacl_a_00448",
    pages = "73--91",
    abstract = "Pipelined NLP systems have largely been superseded by end-to-end neural modeling, yet nearly all commonly used models still require an explicit tokenization step. While recent tokenization approaches based on data-derived subword lexicons are less brittle than manually engineered tokenizers, these techniques are not equally suited to all languages, and the use of any fixed vocabulary may limit a model{'}s ability to adapt. In this paper, we present Canine, a neural encoder that operates directly on character sequences{---}without explicit tokenization or vocabulary{---}and a pre-training strategy that operates either directly on characters or optionally uses subwords as a soft inductive bias. To use its finer-grained input effectively and efficiently, Canine combines downsampling, which reduces the input sequence length, with a deep transformer stack, which encodes context. Canine outperforms a comparable mBert model by 5.7 F1 on TyDi QA, a challenging multilingual benchmark, despite having fewer model parameters.",
}

@inproceedings{zhu-etal-2024-taste,
    title = "The taste of {IPA}: Towards open-vocabulary keyword spotting and forced alignment in any language",
    author = "Zhu, Jian  and
      Yang, Changbing  and
      Samir, Farhan  and
      Islam, Jahurul",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.43",
    doi = "10.18653/v1/2024.naacl-long.43",
    pages = "750--772",
    abstract = "In this project, we demonstrate that phoneme-based models for speech processing can achieve strong crosslinguistic generalizability to unseen languages. We curated the IPAPACK, a massively multilingual speech corpora with phonemic transcriptions, encompassing more than 115 languages from diverse language families, selectively checked by linguists. Based on the IPAPACK, we propose CLAP-IPA, a multi-lingual phoneme-speech contrastive embedding model capable of open-vocabulary matching between arbitrary speech signals and phonemic sequences. The proposed model was tested on 95 unseen languages, showing strong generalizability across languages. Temporal alignments between phonemes and speech signals also emerged from contrastive training, enabling zeroshot forced alignment in unseen languages. We further introduced a neural forced aligner IPA-ALIGNER by finetuning CLAP-IPA with the Forward-Sum loss to learn better phone-to-audio alignment. Evaluation results suggest that IPA-ALIGNER can generalize to unseen languages without adaptation.",
}

@inproceedings{leong-whitenack-2022-phone,
    title = "Phone-ing it in: Towards Flexible Multi-Modal Language Model Training by Phonetic Representations of Data",
    author = "Leong, Colin  and
      Whitenack, Daniel",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.364",
    doi = "10.18653/v1/2022.acl-long.364",
    pages = "5306--5315",
    abstract = "Multi-modal techniques offer significant untapped potential to unlock improved NLP technology for local languages. However, many advances in language model pre-training are focused on text, a fact that only increases systematic inequalities in the performance of NLP tasks across the world{'}s languages. In this work, we propose a multi-modal approach to train language models using whatever text and/or audio data might be available in a language. Initial experiments using Swahili and Kinyarwanda data suggest the viability of the approach for downstream Named Entity Recognition (NER) tasks, with models pre-trained on phone data showing an improvement of up to 6{\%} F1-score above models that are trained from scratch. Preprocessing and training code will be uploaded to \url{https://github.com/sil-ai/phone-it-in}.",
}

@inproceedings{zellers-etal-2019-hellaswag,
	title = "{H}ella{S}wag: Can a Machine Really Finish Your Sentence?",
	author = "Zellers, Rowan  and
	  Holtzman, Ari  and
	  Bisk, Yonatan  and
	  Farhadi, Ali  and
	  Choi, Yejin",
	editor = "Korhonen, Anna  and
	  Traum, David  and
	  M{\`a}rquez, Llu{\'\i}s",
        booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	month = jul,
	year = "2019",
	address = "Florence, Italy",
	publisher = "Association for Computational Linguistics",
	doi = "10.18653/v1/P19-1472",
	pages = "4791--4800",
}

@article{xue-etal-2022-byt5,
  title={Byt5: Towards a token-free future with pre-trained byte-to-byte models},
  author={Xue, Linting and Barua, Aditya and Constant, Noah and Al-Rfou, Rami and Narang, Sharan and Kale, Mihir and Roberts, Adam and Raffel, Colin},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={291--306},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}