\chapter{Background}\label{chapter:background}

\Zeb{Argument of thesis is that it is important to explore the phonemic representation. In \cref{sec:12-inputrepresentations} I define ``input representation'', establishing a contrast between the default and phoneme input representation. Review past work on input representation in language models and recent work on tokenisation methods, concluding that the phoneme input representation is largely under-studied in the modern NLP landscape. In \cref{sec:12-phoneval} I review work regarding phonological evaluation of language models. This includes the use of the phoneme input representation in early connectionist models of language processing and computational models of acquisition as well as more recent work using phonemes in language models to study phonology cross-lingually to benchmarks that directly probe the phonological capabilities of LMs. Finally, in \cref{sec:12-babylm} I review work concerning the use of developmentally-plausible language models. Such models provide a useful framework for studying linguistic theories but mostly continue to use the default input representation, despite children not learning from arbitrary subwords. This chapter concludes that there is a clear need to study the use of the phoneme input representation in modern LM architectures.}

\section{Input representations in language models}\label{sec:12-inputrepresentations}

\Zeb{Here it might be good to try to formalise ``input representation'' with the many axes that you can compare them on. Perhaps a figure showing where classical and modern NLP fall on those axes, along with phonemic and other alternatives.}

\textbf{Language models} are distributional models of natural language; instead of using a grammar to determine the structure of a sentence, they provide a probability distribution for each lexical item based on its context --- the ``company it keeps'' \citep{firth1957synopsis}. Language models typically operate over discrete units known as \textbf{tokens}, drawn from a fixed \textbf{vocabulary} \(V\). For a sequence of tokens \(w_1, w_2, \dots, w_T\), language models estimates the probability:

\begin{align}
P(w_1, w_2, \dots, w_T) &= \prod_{t=1}^{T} P\left(w_t \mid w_1, w_2, \dots, w_{t-1} \right) \label{eq:languagemodel}
\end{align}

In order for these probabilities to be useful, there needs to be an interpretable mapping from \emph{instances of natural language} to \emph{sequences of tokens}. The nature of this mapping is the \textbf{input representation} used by the language model, which can encompass several properties:
\begin{itemize}
\item \textbf{Modality:} The modality of the raw data (e.g. orthographic text, phonemic transcriptions, audio).
\item \textbf{Pre-processing:} Specific operations to clean the raw data, such as removing whitespace or punctuation. 
\item \textbf{Tokenisation:} How the raw data split into discrete tokens.
\item \textbf{Encoding:} How the language model encodes the tokens (e.g. using IDs or vector embeddings).
\end{itemize}

Whereas the encoding property is largely determined by the architectures used for the language model, the other properties can vary. Whereas early \ngram language models used input representations that matched the downstream task, transformer-based architectures use a representation that enhances computational efficiency, as discussed in \cref{sec:12-architectures}. Notably, modern language models use \textbf{default input representation} consisting of a text-based modality, with minimal pre-processing and subword tokenisation. Tokenisation and pre-processing methods are presented in \cref{sec:12-tokenisation} with work studying the default input representation summarised in \cref{sec:12-default}. The alternative \textbf{phoneme input representation}, which uses a phonemic modality, without whitespace and phoneme-level tokenisation, is discussed in \cref{sec:12-phonemic}. Finally, alternative input representations, such as non-discrete patching methods as well as audio modalities are discussed in \cref{sec:12-alternatives}.

% The following sections provide further background and related work concerning the input representations used in language modelling. First, \cref{sec:12-default} provides a definition of the \textbf{default input representation} and an overview of tokenisation studies that have criticised, analysed and suggested alternative tokenisers that all produce tokens following this representation. \Cref{sec:12-phonemic} presents the \textbf{phonemic input representation} in contrast, an under-studied alternative representation and the subject of this thesis. Finally, these two representations are not the only possibilities; alternatives are briefly described in \cref{sec:12-alternatives}.

\subsection{Input representations across language model architectures}\label{sec:12-architectures}

Input representations in language models have evolved alongside the development of new architectures and their emerging use cases. For example, early \ngram language models \zeb{tenses are a mess in these paragraphs}operated on written sentences split into tokens consisting of individual \textbf{words}. These models estimated probabilities using frequency statistics and provided an estimate of \cref{eq:languagemodel} using a Markov assumption:

\begin{align}
    P(w_1, w_2, \dots, w_T) \approx \prod_{t=1}^{T} P\left(w_t \mid w_{t-(n-1)}, \dots, w_{t-1}\right) \label{eq:ngram}
\end{align}

Using word-level tokens was an intuitive choice for many NLP tasks, such as part-of-speech tagging, machine translation, text classification and language generation. Words naturally represent meaningful units in language and word-level predictions provide useful interpretations for these tasks. However, there were limitations to using words in \ngram models. One limitation was to do with their count-based nature. The number of possible \ngrams grows exponentially with $n$, making probabilities inefficient to store and difficult to estimate due to sparsity. These models also struggled with rare words --- which will Zipf's law states will inevitably appear \citep{zipf_human_1949} --- as well as with typos and other out-of-vocabulary (OOV) items, since they will not match any pre-computed \ngram probabilities. To mitigate sparsity and OOV issues, back-off procedures such as Kneser-Ney and Katz smoothing were developed \citep{ney1994structuring, katz2003estimation}, but the exponential factor meant that \ngram models still struggled to scale beyond relatively small context windows.

A lesser-used alternative to word-level tokens in \ngram models was to split sentences into individual \textbf{characters}. This reduces the vocabulary size considerably, allowing for slightly higher-order \ngrams to be computed, as well as providing a robust solution to OOV items. However, since words are split into many tokens, few syntactic dependencies are captured, limiting the utility of these models for many NLP tasks. Instead, these models were primarily used for character-sensitive tasks, such as spelling correction and auto-completion \citep{cucerzan_spelling_2004}.

\ngram models have also been used to model spoken language for tasks such as text-to-speech, accent recognition and language identification. Instead of the raw data consisting of written text, these models use an input representation where tokens consist of individual \textbf{phonemes} --- the basic units of sound that distinguish words in spoken communication. Speech recognition systems often paired these phoneme-level language models with \textbf{acoustic models}, which map from acoustic features to phonemes as implemented in tool-kits such as HTK \citep{young2006htk}.

For many years, \ngram models were a cornerstone of NLP, providing the statistical backbone for early systems in translation and speech \citep{jurafsky2009speech}. They were used with a range of input representations, with tokens typically representing core linguistic units like words, characters and phonemes, depending on the task. However, as neural architectures gradually replaced statistical language models across the NLP landscape, so too did the input representations leveraged by these models.

Neural language models date back to \citet{bengio2003neural}, who used a feed-forward neural network to predict words given a context. This addressed the sparsity issues of \ngrams with generalisable dense embeddings, but this approach was still limited by context and was expensive to train. The shift to neural architectures was catalysed by advances in representation learning — notably Word2Vec \citep{mikolov_distributed_2013} — and by the adoption of recurrent neural networks (\textbf{RNNs}) as language models \citep{mikolov2010recurrent}, which enabled modelling of sequences with theoretically unbounded context. In practice, RNNs suffered from `vanishing gradients', an issue addressed by long short-term memory networks (\textbf{LSTMs}, \citet{sundermeyer2012lstm}) and then by the attention mechanism of the \textbf{transformer} architecture \citep{vaswani2017attention}.

There are considerable variations of these architectures, most of which build off of the GPT-series of models \citep{radford2018gpt1,radford-2019-gpt2, brown2020gpt3} or BERT \citep{devlin2019bert}. GPT models are \textbf{auto-regressive}, trained with the language modelling objective, whereas BERT-style models are \textbf{auto-encoding}, using a \textbf{masked language modelling} objective to benefit from bi-directional context. These transformer-based language models soon became the dominant approach for tackling most NLP tasks and have become so ubiquitous that the term ``language model'' is synonymous with these architectures. The acronym \textbf{LM} will henceforth be used to refer to language models that use a transformer-based architecture.

Whereas the choice of input representation for \ngram language models was primarily driven by the needs of the specific downstream task, for LMs the choice is instead primarily driven by considerations of computational efficiency. LMs map from unique tokens to dense vector representations using an \textbf{embedding layer}, which grows linearly with the vocabulary size. Using word-level tokens therefore increasing space and time complexity, and reducing the vocabulary size leads to OOV issues. Character or byte-level tokens reduce the vocabulary size and deal with OOV items, but at the cost of increasing the number of tokens for each sentence. Due to the self-attention mechanism, the computational cost of training and inference scales quadratically with context size, so this input representation also undesirable.

The solution to this problem is to use a \textbf{subword} representation, where words not contained in the vocabulary are split into smaller units (such as character \ngrams or frequent subword units) with the ability to fall back to character or byte-level tokens for particularly rare or complex words. This provides a trade-off between word-level tokens and character-level tokens --- vocabulary sizes can be kept relatively small (at the order of tens of thousands of words for popular models)without creating overly long token sequences, all while being able to handle OOV items.

In the era of large language models (\textbf{LLMs}), the computational considerations have completely outweighed other factors when it comes to selecting the input representation. Instead of training separate models for each downstream task, contemporary practice instead prioritises pre-training a single, general-purpose model on massive text corpora, which can subsequently be adapted to a wide range of tasks through fine-tuning or even used in a zero-shot or few-shot setting. This shift necessitated a consistent input representation, as empirical evidence suggests that exposing models to more data yields greater performance gains than carefully engineered tokenization strategies \citep{brown-2020-gpt3}. Given that these models contain billions of parameters and are typically trained on trillions of tokens, the computational costs are enormous --- resulting in significant real-world costs, not only financially, but also for the environment \citep{strubell-etal-2019-energy, patterson2021carbonemissionslargeneural, bender2021parrots,luccioni2022estimatingcarbonfootprintbloom}.

Consequently, designing input representations that optimise pre-training efficiency while preserving generality has become the central motivator for the choice of input representation in the modern NLP landscape. 

\Zeb{Some statement here about potential of phoneme tokenisation and the fact that alternative input representations are largely under-studied in NLP, due to the convenience of the default one. Maybe here go into the historical use of the input representation and that it has been vastly understudied. }

%Architectures like GPT-2 that provide probabilities corresponding with \cref{eq:languagemodel} are known as \textbf{auto-regressive} architectures, whereas models like BERT solve an alternative objective known as \textbf{masked-language modelling}, 

\subsection{Tokenisation and pre-processing}\label{sec:12-tokenisation}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{example-image-a}
    \caption{The standard classical and modern pipeline for preparing text for language modelling tasks.}
    \label{fig:12-pipelinecomparison}
\end{figure}

Mention bert used lowercasing. Mention minimal pre-processing now since noisy data seems to be helpful. 

For example, BPE and WordPiece \addcites. \writemore. An important distinction with these approaches is the difference between their vocabulary-learning algorithm and their inference algorithm, \writemore. Formally, \writemore. 

Handling this complicated task was traditionally one of the first tasks of preparing written text for NLP tasks. Preceding steps could include by text-cleaning operations like lowercasing and the removal of unwanted symbols like punctuation, with post-tokenisation steps including stemming, part-of-speech tagging and stop-word removal depending on the task. This pipeline is illustrated in \cref{fig:12-pipelinecomparison} and is offered by python packages like NLTK \addcites.

For both word-level and character-level \ngram models, the text domain also created complications. Written text is noisy, often containing unwanted data that can interfere with language modelling, depending on the task. Typical NLP pipelines often began with text-cleaning operations like lowercasing and the removal of punctuation. The next step was \textbf{tokenisation}, mapping the character sequence to individual tokens. For character-level models this was straight-forward, but for word-level models this is a complicated task. Tokenisation needed to make arbitrary decisions on how to handle exceptions like clitics and compound words. In general, the very definition of a `word' is still debated, complicating this process further. In some languages like Chinese, words are not even delimited by whitespace, leading to an entire task in NLP dedicated to this word segmentation step \addcites. In certain cases, additional steps post-tokenisation included 

Both the inference and vocabulary-learning algorithms typically operate after text has already been split into word-like units; with what was previously called `tokenization' in the classical pipeline now referred to as ``pre-tokenization''. 

Several of the steps previously performed separately by the classical pipeline are now packaged together into a single `tokenizer' by libraries such as Huggingface Tokenizers.\footnote{\href{https://huggingface.co/docs/tokenizers/index}{huggingface.co/docs/tokenizers/}} This tokenisation pipeline is shown in \cref{fig:12-pipelinecomparison}. First, \textbf{normalization} performs the text-cleaning steps, before text is split into word-like units via \textbf{pre-tokenization}. The BPE pre-tokenizer not only splits text into word units, but also converts each unit into a byte-based representation, ensuring that the initial vocabulary contains only 256 items instead of every unique character in UTF-32. The tokenizer \textbf{model} then converts each pre-token into one or more tokens using a particular inference algorithm, such as longest-prefix matching for WordPiece or deterministic-merge application by BPE. These tokens are mapped to unique IDs to facilitate lookup into a LM's embedding layer. The pipeline may also include \textbf{post-processing} to add special tokens and \textbf{decoding} to convert IDs back into text for text generation tasks. 

The design and availability of this tokenization pipeline has largely been driven by the sheer scaling capabilities of LM architectures, the largest of which are called ``large'' language models (LLMs). Now, the vast majority of language models are distributed on platforms such as Huggingface with an associated tokenizers that consistently process orthographic text, pre-tokenize the text to preserve word boundaries, and return tokens representing subwords.

Packaging up these pre-processing steps into a single tokenizer provides convenience, but this has had consequences. In classical NLP, it was an important step to carefully considering the data cleaning operations. For example, removing punctuation... \writemore. The scaling ability of modern LMs has largely shifted the focus to language model architectures and machine learning algorithms, with many models using default tokenisers from Huggingface without considering the impact. This is particularly the case for studies using smaller LMs to study language or acquisition (see \cref{sec:12-babylm}) where subwords in particular may not be an appropriate base unit, nor the orthographic domain if simulating spoken speech. In these cases the default representation is no longer as crucial for performance, yet the convenience of the existing frameworks facilitates its use. 

\subsection{The default input representation}\label{sec:12-default}

\Zeb{Give background on what LMs use by default. Define as subwords + orthographic + pre-tokenization.}

\Zeb{Define tokenizers more formally and discuss BPE and WordPiece etc and pointing to some surveys about these methods. Greed is all you need method.}

\Zeb{Plausibility of subwords, issues in certain languages, misalignment etc. Superword paper as an alternative to word boundary-based pretokenization. Morpheme-aligned alternatives that never caught on. Many alternative approaches. Character-level models and byte-level models. Attempts to go `token-free' but byte-level still an arbitrary token choice.}

\zeb{Change this to be about the tokens rather than the tokenizers}
\begin{enumerate}
    \item process written (orthographic) text,
    \item pre-tokenize the text to preserve word boundaries, and
    \item split pre-tokenized into tokens that represent subwords.
\end{enumerate}

The combination of these three features will henceforth define the \textbf{default input representation}. Now, the vast majority of language models are distributed with an associated tokenizer. The tokenizer converts noisy text to unique token IDs, which are fed through the model, which produces contextual embeddings. Auto-regressive LMs are trained with a next-token prediction objective, allowing them to generate text one token at a time, which the tokenizer can convert back into readable text. Alternatively, the contextual representations are directly used, or the model is fine-tuned on a downstream task involving labelled data. There are countless variations to this setup in modern NLP but the vast majority use tokenisers...

In recent years, tokenization has become an increasingly popular topic, with the default configurations of popular tokenizers often critiqued and analysed, and many studies proposing improvements and alternatives to existing tokenizers. An overview of this work is provided in \cref{sec:12-default}. Despite this scrutiny, the focus is still on tokenizers that produce the default input representation. 

\subsection{The phonemic input representation}\label{sec:12-phonemic}

\Zeb{Formal definition of what I mean by the phonemic input representation. This is where the three transformations are described. Similar to character-level. Maybe here worth going more into detail with character-level models and tabula rasa model.}

%\subsubsection{Practical uses of the phonemic input representation}

\Zeb{Survey of methods using phonemes. Briefly discuss how important phonemes were for speech recognition technology but now a lot of that is end-to-end. Discuss more recent uses in LMs. End by stating maybe we're resource limited, as discussed in \cref{chapter:resources}}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{example-image-b}
    \caption{The phrase ``example phrase'' encoded using the default input representation compared to the phonemic input representation.}
    \label{fig:12-representation}
\end{figure}

An alternative to the default input representation, and the topic of this this thesis, is the \textbf{phonemic input representation}. This is defined in contrast to above as:

\begin{enumerate}
    \item process phonemic text,
    \item do not preserve word boundaries, and
    \item split text into tokens that represent individual phonemes.
\end{enumerate}

A major use of the phoneme representation is phonological experimentation. This includes connectionist models of language processing, computational models of word segmentation and word-level phonotactic models, as discussed in \cref{sec:12-phoneval}. A practical use was speech recognition technology. Was important to create phoneme-level models along with language models in a complicated system to do TTS, STT, voice recognition, language recognition, etc. Many resources were built to support this, many phonological datasets. A detailed analysis of existing phonological resources in given in \cref{chapter:resources}.

Despite this history, less work exploring phoneme representation in modern NLP landscape. One reason could be that the scaling abilities of LLMs have favoured text, which is more readily online and regularly scraped. Speech technology has also largely moved to end-to-end directly from/to audio, without needing discrete phoneme models.

Yet, phoneme LMs do offer some practical benefits which have been explored, \writemore. There are also analytical benefits, as discussed in \cref{sec:12-phoneval}.

% One reason could be a lack of resources, which is the topic of \cref{chapter:resources}. A benefit of exploring alternative input representations is to ablate the effect of each of the features of the default representation, as explored in \cref{chapter:modelling}. The cognitive aspect... \cref{chapter:phonology}... and could improve tokenisation methods... \cref{chapter:infotokenization}.

Phoneme language models. Many word-level models to study phonology. Very few examples of running text. Some examples of working with audio directly, as discussed in the next section.

Phoneme representation could be compared against default representation along three axes. There is some work comparing these effects. Several studies have challenged (3) by demonstrating that character-based or byte-based models can still be effectively utilised in language models \addcites. These argue to be "token free" but according to \addcites is still an arbitrary subword representation. A few studies have challenged (2) by relaxing the word-boundary pre-tokenization constraint to produce `superword' tokens \addcites or explore the effect of removing whitespace altogether to achieve a `tabula rasa' input representation \addcites but still orthographic text. Finally, Bunzeck paper. \cref{chapter:modelling} properly compares these to establish the effect of the phoneme representation in modern language models.

\subsection{Alternative input representations}\label{sec:12-alternatives}

\Zeb{Briefly survey speech models, pixel tokenisation etc. but probably don't go into too much detail.  Maybe patches work goes here as well.}

\section{Phonological experimentation using language models}\label{sec:12-phoneval}

\Zeb{Phoneme representation has been used in various contexts. First three sections below are all examples of trying to reach some conclusion about language by using a language model's predictions to justify some kind of argument. Final section is more about language models themselves, what kind of linguistic knowledge they acquire.}

\subsection{Connectionist models of language processing}

\Zeb{Brief survey of old connectionist models looking at language processing}

\subsection{Computational models of word segmentation}

\Zeb{Discuss word segmentation in detail here and discuss its relationship with subword tokenisation, forward referencing \cref{chapter:infotokenization}.}

\subsection{Cross-lingual studies of phonology}

\Zeb{More recent studies that use LMs to study phonology by using tiny LMs that train just over word types rather than tokens in context. Conclude that there have been few attempts at training cross-lingual LMs on naturalistic running text to get a model of phonology for each language.}

\subsection{Phonological evaluation of language models}

\Zeb{Discuss prior evaluation used to evaluate language models for phonological knowledge. Possibly start with wider background looking at how people evaluate LMs for knowledge of linguistic structure. Discuss phonologybench as the opposite of what we're looking for. BabySLM is more relevant, discussed in the next section.}

\section{Pre-training on developmentally-plausible data}\label{sec:12-babylm}

\Zeb{Idea that ``simulations must closely emulate real-life situations by training on developmentally plausible corpora'' in order to gain insights both about what language models can learn, and improve understanding about how infants learn language. Clearly could be valuable insights for phonology in this area.}

\Zeb{Early example is BabyBERTa paper, who trained on a version of AOCHILDES. Briefly discuss findings and criticisms.}

\Zeb{Later, BabyLM challenge created a framework for training and evaluting such models. Still motivated by developmentally plausible data, but still using default input representation. Findings more about architectures for low-data than specifically for insights into human learning. This framework provides a good way of testing this size model.}

\Zeb{Finally, BabySLM paper, which focused more on speech models. They also trained models on portions of CHILDES, including AOCHILDES, as well as Seedlings, an audio dataset, and compare speech, phonemes and text as input representations.}

\Zeb{Besides BabySLM and Bunzeck papers, very few examples of phoneme-based training on developmentally-plausible data, possible due to resource limitations (see \cref{chapter:resources}). However, these papers provide a useful starting point for establishing whether phoneme-based training is plausible and the datasets and evaluation criteria described below are leveraged in this thesis..}

\Zeb{Should mention \citet{huebner_structured_2018} as an early example of modelling child-directed speech using SRNs, LSTMS and skip-gram.}

\Zeb{Review \citet{wilcox_bigger_2025} and Suchir's paper for wider position pieces on the role of babylm for wider research.}

\Zeb{BabyLM framework is appropriate for two reasons. Firstly, the scale: it's appopriate for academic budgets, phoneme models can be useful for low-resource, and phoneme data is too limited for larger scale models (see next chapter). Secondly, the developmental/analytical angle. These models have been branded as useful for cognitive research due to the developmental plausibility. Phoneme representation so far has mostly been used for analytical purposes, so using develpmentally plausible framework is logical - and using the phoneme representation provide an additional aspect of plausibility beyond written text and exploration is worthwhile.}

\subsection{Datasets}

\subsubsection{The CHILDES database}

\Zeb{Describe CHILDES. Discuss people who have done language modeling with CHILDES (e.g. BabyBERTa).}

The Child Language Data Exchange System (CHILDES) is a repository of child-centred data originally developed with the aim of preserving and standardising data used for child language development research \citep{macwhinney1985child}. The project later grew into TalkBank, which contains over 1.4TB of transcript data and 5TB of associated media data across several ``banks'' \citep{macwhinney_understanding_2019}. Each bank focuses on a different area of human communication, with a general focus on spoken communication, with the CHILDES banks now containing child-caregiver interactions in over 40 different languages thanks to the efforts of hundreds of contributors over the last 40 years. 

CHILDES is an extremely valuable resource for research on child language development and has led to many thousands of published articles since its release. Due to the commitment to open-data sharing, the influence of these resources has also extended to other fields of research. For instance, in a recent assessment on the impact of the TalkBank project, \citet{bernstein_ratner_augmenting_2024} noted that a corpus she had contributed to CHILDES --- originally collected to study the acoustic features of child-directed speech \citep{Ratner_1984} --- was still leading to new insights forty years later:
\zeb{Probaby worth mentioning that this became the BR corpus for word segmenation.}
\begin{quote}
The corpus has been further used to test models of unsupervised induction of grammar in machine language learning \citep{glushchenko_programmatic_2019}, a prospect not remotely envisioned during the original study, when data were collected on reel-to-reel analog tapes, and acoustically analyzed using a dedicated mainframe computer that had to be booted with punched paper tape.
\end{quote}

For researchers seeking developmentally-plausible corpora to use as pre-training data for language models, CHILDES is a natural fit. For instance, BabyBERTa was pre-trained on the AO-CHILDES corpus (Age-Ordered CHILDES, \citep{huebner2021using}). AO-CHILDES contains approximately 5M words from the North American English sections of CHILDES. Specifically, it contains all utterances involving children aged 0 to 6, sorted by child age, with non-adult speech removed (thus simulating the theoretical input received by a child). In their study, \citet{huebner-etal-2021-babyberta} were interested in the outcome of pre-training BabyBERTa on AO-CHILDES compared to Wikipedia (which was considered a more representative dataset for NLP at the time). Using a linguistic benchmark based on BLiMP (see \cref{sec:12-blimp} below), they found that BabyBERTa achieved a higher accuracy when trained on AO-CHILDES, that the choice of corpora had an effect across the pre-training corpus and that ordering multiple corpora by grammatical complexity could `scaffold' learning. These findings validated the importance of using developmentally-plausible corpora for BabyLM research and the scaffolding results were a precursor of many of the curriculum learning approaches taken in the first edition of the BabyLM challenge \addcites.

\Zeb{Insert other examples of language modelling with CHILDES}

\Zeb{Insert a few phoneme examples of language modelling with CHILDES, discuss PhonBank}

\subsubsection{The BabyLM dataset}

\Zeb{BabyLM dataset description. Note that they wanted to reach 100M words, which they couldn't just do with CHILDES. Can slightly criticise, not quite developmentally plausible but more so than web-scraped corpora (maybe show figure comparing CHILDES, BabyLM and Pile). Maybe briefly mention other cognitively plausible datasets that have sprung up recently, like German BabyLM, KidLM, storybooks, chat-gpt generated data, future BabyLM multilingual.}

\subsection{Evaluation metrics}

Below are a description of the main benchmarks used in this thesis to evaluate LMs. 

\subsubsection{BLiMP}\label{sec:12-blimp}

\subsubsection{GLUE}

\subsubsection{BabySLM}

\section{Summary}

\Zeb{Go back to research questions. Need to establish whether resources exist. Need to do a thorough comparison of input representations and determine how to do language modeling with phonemes. Need to see what insights can be gained from phonological experimentation with such models. BabyLM framework gives good start to use for this stuff, but still limited ways to evaluate phoneme LMs.}