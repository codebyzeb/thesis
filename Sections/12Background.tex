\chapter{Background}\label{chapter:background}

\section{Input representations in language models}\label{sec:12-tokenization}

\Zeb{Formally define what I mean by input representation. Give the broad definition of tokenizer without going into too much detail. Discuss orthographic text more.}

\subsection{The default input representation}

\Zeb{Give background on what LMs use by default. Define as subwords + orthographic + pre-tokenization.}

%\subsubsection{Subword tokenizers}

\Zeb{Define tokenizers more formally and discuss BPE and WordPiece etc and pointing to some surveys about these methods.}

%\subsubsection{Criticisms of the default input representation}

\Zeb{Plausibility of subwords, issues in certain languages, misalignment etc. Superword paper as an alternative to word boundary-based pretokenization. Morpheme-aligned alternatives that never caught on. Many alternative approaches. Character-level models and byte-level models. Attempts to go `token-free' but byte-level still an arbitrary token choice.}

\subsection{The phonemic input representation}

\Zeb{Formal definition of what I mean by the phonemic input representation. This is where the three transformations are described. Similar to character-level.}

%\subsubsection{Practical uses of the phonemic input representation}

\Zeb{Survey of methods using phonemes. Briefly discuss how important phonemes were for speech recognition technology but now a lot of that is end-to-end. Discuss more recent uses in LMs. End by stating maybe we're resource limited, as discussed in \cref{chapter:resources}}

\subsection{Alternative input representations}

\Zeb{Briefly survey speech models, pixel tokenisation etc. but probably don't go into too much detail.}

\subsection{Related work comparing input representations}

\Zeb{Prior work, `tabula rasa', the study of Bunzeck done in parallel to mine. Concluding thoughts for this section.}

\section{Phonological experimentation using language models}\label{sec:12-phoneval}

\Zeb{Phoneme representation has been used in various contexts. First three sections below are all examples of trying to reach some conclusion about language by using a language model's predictions to justify some kind of argument. Final section is more about language models themselves, what kind of linguistic knowledge they acquire.}

\subsection{Connectionist models of language processing}

\Zeb{Brief survey of old connectionist models looking at language processing}

\subsection{Computational models of word segmentation}

\Zeb{Discuss word segmentation in detail here and discuss its relationship with subword tokenisation, forward referencing \cref{chapter:infotokenization}.}

\subsection{Cross-lingual studies of phonology}

\Zeb{More recent studies that use LMs to study phonology by using tiny LMs that train just over word types rather than tokens in context. Conclude that there have been few attempts at training cross-lingual LMs on naturalistic running text to get a model of phonology for each language.}

\subsection{Phonological evaluation of language models}

\Zeb{Discuss prior evaluation used to evaluate language models for phonological knowledge. Possibly start with wider background looking at how people evaluate LMs for knowledge of linguistic structure. Discuss phonologybench as the opposite of what we're looking for. BabySLM is more relevant, discussed in the next section.}

\section{Pre-training on developmentally-plausible data}\label{sec:12-babylm}

\Zeb{Idea that ``simulations must closely emulate real-life situations by training on developmentally plausible corpora'' in order to gain insights both about what language models can learn, and improve understanding about how infants learn language. Clearly could be valuable insights for phonology in this area.}

\Zeb{Early example is BabyBERTa paper, who trained on a version of AOCHILDES. Briefly discuss findings and criticisms.}

\Zeb{Later, BabyLM challenge created a framework for training and evaluting such models. Still motivated by developmentally plausible data, but still using default input representation. Findings more about architectures for low-data than specifically for insights into human learning.}

\Zeb{Finally, BabySLM paper, which focused more on speech models. They also trained models on portions of CHILDES, including AOCHILDES, as well as Seedlings, an audio dataset, and compare speech, phonemes and text as input representations.}

\Zeb{Besides BabySLM and Bunzeck papers, very few examples of phoneme-based training on developmentally-plausible data, possible due to resource limitations (see \cref{chapter:resources}). However, these papers provide a useful starting point for establishing whether phoneme-based training is plausible and the datasets and evaluation criteria described below are leveraged in this thesis..}

\subsection{Datasets}

\subsubsection{The CHILDES database}

\Zeb{Describe CHILDES. Discuss people who have done language modeling with CHILDES (e.g. BabyBERTa).}

\subsubsection{The BabyLM dataset}

\Zeb{BabyLM dataset description. Can slightly criticise.}

% \subsubsection{Other datasets}

% \Zeb{KidLM, storybooks, terrible dataset that is chat-gpt generated.}

\subsection{Evaluation metrics}

\subsubsection{BLiMP}

\subsubsection{GLUE}

\subsubsection{BabySLM}

\section{Summary}

\Zeb{Go back to research questions. Need to establish whether resources exist. Need to do a thorough comparison of input representations and determine how to do language modeling with phonemes. Need to see what insights can be gained from phonological experimentation with such models. BabyLM framework gives good start to use for this stuff.}