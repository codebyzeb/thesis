\chapter{Conclusion}\label{chapter:conclusion}

This chapter concludes the work presented in this thesis. First, \cref{sec:17-summary} provides a summary of the contributions of this thesis, returning to the four research questions presented in \cref{chapter:intro}. \cref{sec:17-audiomodels} then provides a discussion comparing this work to recent advances in models that learn directly from audio. Finally, \cref{sec:17-futurework} discusses open questions and directions for future work.

\section{Phonological representations in language models}
\label{sec:17-summary}

The field of Natural Language Processing (NLP) is currently dominated by research into large language models (LLMs) --- massive neural networks which are pre-trained on natural language data represented as discrete tokens, referred to in this thesis as the \emph{input representation}.

The standard input representation in LLMs is subword-based. Tokens are derived from orthographic text and segmented into units ranging in granularity from individual bytes to entire words. Subword tokenisation algorithms are used to learn these units prior to pre-training, typically through data-driven procedures that construct a vocabulary from large text corpora. These methods are designed primarily to meet the computational demands of LLM pre-training, which is increasingly resource-intensive. Consequently, subword-based input representations are often optimised for efficiency rather than guided by linguistic or cognitive principles.

Understanding and interpreting LLMs is crucial for improving model architectures and gaining insights into the structure of language itself. As LLMs have grown larger, the cost of pre-training has become prohibitive for most academic settings. This has led to a shift in focus towards training smaller models (LMs) within more developmentally plausible frameworks, often drawing from theories of language acquisition. These smaller-scale efforts not only enable research in low-resource settings but also contribute to our understanding of large-scale pre-training due to neural scaling laws. Importantly, they also offer a valuable lens for exploring linguistic phenomena, especially in cross-linguistic contexts.

The analysis and interpretation of LLMs plays a crucial role in developing better architectures and can yield useful insights into the structure of language. In recent years, LLMs have grown so large that pre-training is no longer feasible on academic budgets, leading to a shift towards pre-training smaller models, and using a developmentally-plausible framework to draw inspiration from linguistic theories of acquisition. In this smaller setting, insights can not only be made for low-resource language modelling but also large-scale pre-training due to neural scaling laws, and this work can also yield useful linguistic insights, particularly cross-lingually.

This thesis has focused on the insights that can be gained by pre-training LMs using a phoneme-based input representation, seeing answers to the following research questions:

\begin{question*}{Research Question 1}
    To what extent do sufficient resources exist for pre-training and evaluating language models using a phoneme-based input representation?
\end{question*}

\begin{question*}{Research Question 2}
    To what extent can phoneme-based pre-training facilitate cross-lingual phonological experimentation and analysis?
\end{question*}

\begin{question*}{Research Question 3}
    To what extent can phoneme-based pre-training provide insights into language model pre-training?
\end{question*}

I explored these questions by training phoneme LMs within a developmentally plausible framework and analysing the phonological representations they acquire. I examined what these representations can tell us about the distributional phonological properties of languages, whether they can provide insights into how language models internalise linguistic structure and using these insights, developed a novel method for subword tokenisation. % grounded in theories of acquisition 

In \Cref{chapter:resources}, I began by reviewing existing phoneme-based datasets and found that few are suitable for cross-lingual phoneme-based training. In particular, datasets containing spontaneous or child-directed speech are rare. Most phoneme-based corpora are derived by applying grapheme-to-phoneme (G2P) conversion to orthographic text, suggesting that the CHILDES corpus --- a database of child-centred speech transcripts --- could be similarly converted. However, many G2P tools lack consistency with established phonological inventories, such as those in \phoible. To address this, I introduce \gpp, an improved G2P tool that supports over \integer{100} languages by leveraging existing G2P systems as backends and mapping their outputs to phoneme inventories defined in \phoible. Using \gpp, I convert transcripts from \integer{31} CHILDES languages into phonemic modality to create \ipachildes --- the first cross-lingual dataset of child-directed speech represented as phonemes. Both \gpp and \ipachildes are open-sourced to support broader phonological and multilingual NLP research.

In \cref{chapter:modelling}, I apply \gpp to adapt the BabyLM Challenge framework to a phoneme-based representation and evaluate the impact of this shift compared to the standard subword-based input. Phoneme-based pre-training leads to decreased performance across benchmarks assessing grammatical and language understanding, highlighting key representational differences. To analyse this further, I define three binary transformations distinguishing phoneme and subword representations and isolate their effects. I find that character-level granularity harms performance primarily due to truncation effects, and that part of the degradation in the phoneme setting is due to the absence of punctuation. Nonetheless, transformer-based language models remain largely robust to these representational differences. Additionally, I use subsets of the North American English portion of \ipachildes to study scaling laws for phoneme-based models, finding that as few as \q{80}{\thousand} tokens enable models to distinguish words from non-words, though more data is required to capture syntactic regularities.

In \Cref{chapter:phonology}, I use the best-performing model configurations identified in the scaling experiments to train phoneme-based LMs on all 31 languages in \ipachildes, demonstrating their utility for phonological experimentation. Inspired by the word segmentation task, I propose novel methods for unsupervised word segmentation that rely on extracting model uncertainty and utterance boundary predictions. These methods identifying peaks, relative increases, or threshold crossings in these signals to posit word boundaries and successfully segment held-out utterances across all \integer{31} languages, illustrating the cross-linguistic learnability of word-like units from phoneme distributions. I also employ linear probes to investigate whether the phonological representations learned by the models encode boundary information, finding that all probes perform significantly above chance. However, performance varies by language, influenced by priors from word-final phoneme distributions. Further probing reveals that while core features such as vowel-consonant distinctions are clearly encoded across \integer{11} languages, other articulatory features show weaker representation, suggesting limited distributional relevance. Overall, I argue that phoneme-based LMs are effective tools for phonological analysis and serve as diagnostic instruments for understanding how language models acquire linguistic structure.

Finally, in \Cref{chapter:infotokenisation}, I draw connections between my segmentation strategies and recent developments in tokenisation --- particularly methods that merge predictable byte sequences using a byte-level LM. I explore whether the underlying principle shared by both --- that units within lexical items tend to be more predictable than those spanning boundaries --- can be applied to learning a vocabulary for a subword tokeniser. I introduce \bytespan, a method that uses a byte-level LM to group predictable bytes using a global, monotonic, or combined constraint, using similar cues to those previously explored in unsupervised word segmentation. I show that \bytespan produces subword vocabularies that are both efficient and linguistically informed, achieving similar compression to \bpe while offering improved morphological alignment in English. Language models trained with \bytespan perform comparably to those using \bpe. In multilingual experiments, \bytespan also matches \bpe in performance and can be adapted to yield better compression for under-represented scripts.

In summary, this thesis has introduced new phonological resources, explored phoneme-based pre-training within a developmentally plausible framework, and shown that cross-lingual phoneme language models can yield valuable insights into both phonological typology and language model training. Together, these contributions address the three core research questions and highlight the analytical potential of alternative training paradigms --- offering new tools for linguistic inquiry and meaningfully shaping the development of more interpretable and linguistically informed NLP models.

%offering new tools for linguistic inquiry and informing future directions in the design of NLP systems.

% \subsection{Comparing Human Acquisition to Language Model Learning}
% \label{sec:14-acquisition}

% % Here we could go into the BabyLM challenge as related work that looks into reducing the advantages language models have over humans, say that our work here contributes to the BabyLM challenge, and finally briefly mention the literature regarding 'advantages' and point to our discussion section on audio-based models. 

% The capacity of LMs to learn language from text alone has spurred interest in using such models for acquisition and psychology studies, such as comparing model learning trends to child learning behaviour \citep{evanson-etal-2023-language} and using model outputs to predict human reading times \citep{hollenstein-etal-2021-multilingual}.

% To push this research further, recent efforts aim to make language modelling more cognitively plausible \citep{beinborn2024cognitive} by reducing the advantages that typical language models have over humans during the learning process \citep{warstadt-2022-artificial}. One approach is to limit and curate the dataset to that which a typical human may be exposed to, such as is done in the BabyLM challenge \citep{warstadt-2023-babylm-findings}. Another approach is to use an input representation that more closely mimics speech rather than written text \citep{dupoux-2018-cognitive}. Finally, we must consider whether the architectures themselves are suitable linguistic theories, given that they were developed for downstream tasks \citep{baroni-2022-proper}.

% In this work we contribute to all three approaches by training a language model with streams of phonemes and assess whether the language model architecture used is advantaged or disadvantaged by these changes according to a wide variety of benchmarks. We hope that this leads to further work studying acquisition using phoneme streams as an input representation. However, while streams of phonemes may seem more cognitively plausible than written text, many studies go further than we do and seek to train directly on raw audio.

% Previously "learning directly from audio"
\section{Considering audio-based input representations}
\label{sec:17-audiomodels}

This thesis has primarily explored the insights that can be gained by pre-training language models using a phoneme-based input representation. However, a growing body of work focuses on training models directly from raw audio. Just as the architectures of text-based language models have evolved in recent years (as discussed in \cref{sec:12-architectures}), so too have models that operate on audio-based input representations.

A central development in this area is self-supervised learning (SSL) from speech, which enables models to learn useful representations from raw audio without relying on hand-crafted features. In SSL, models learn by predicting information extracted from the input itself, without the need for explicit labels --- a form of unsupervised learning. \citet{mohamed2022self} provide a wider overview of such techniques. Notably, \myemph{wav2vec 2.0}  \citep{baevski2020wav2vec} is a family of self-supervised models, some of which employ transformer architectures trained via masked time-step prediction and can be fine-tuned for automatic speech recognition (ASR). Successive models have built upon this approach: \myemph{HuBERT} \citep{hsu-2021-hubert} introduces hidden unit clustering to guide learning, while \myemph{WavLM} \citep{chen2022wavlm} incorporates both masked speech prediction and multi-task objectives.

Audio-based representations have also driven advances in \emph{end-to-end} (E2E) ASR systems, which map raw audio directly to text using a single neural network and a unified objective function. This contrasts with traditional hybrid systems that require the separate optimization of multiple components. \citet{li2022recent} review the performance of E2E models, which now achieve state-of-the-art results. A broader overview is provided by \citet{prabhavalkar2023end}. More recently, audio-based input representations have been used in efforts to develop large language models (LLMs) capable of handling spoken language in both input and output. \citet{arora2025landscapespokenlanguagemodels} propose the term spoken language model (SLM) to describe such systems. They frame SLM development as taking steps toward a universal speech processing model capable of handling arbitrary tasks given natural language instructions.

While this thesis has focused on the \emph{analytical} value of phoneme-based input representations, it is important to consider their \emph{practical} relevance, especially given that state-of-the-art systems increasingly train directly on audio and may seem to render phonemes unnecessary. However, E2E models are highly data-intensive \citep{li2022recent} and the linguistic performance of SLMs scales up to three orders of magnitude more slowly than their text-based counterparts \citep{cuervo2024scaling}, making them impractical for low-resource settings. In such contexts, phoneme-based representations remain valuable. For instance, \citet{feng-2023-language-universal-phonetic} show that phonemes can serve as a universal representation for multilingual speech pre-training. Furthermore, \citet{poli2024improving} demonstrate that fine-tuning with phoneme classification can help to mitigate the slow scaling of SLMs. These findings underscore the continued relevance of phoneme-based representations for low-resource language modelling.

%However, such models are extremely data-hungry \citep{li2022recent}, making them impractical for the 7000+ human languages which are considered under-resourced \citep{scharenborg2020speech}.

%These models are also used to study acquisition, regarding raw audio as an input representation that is more cognitively plausible than phonemes; a continuous signal full of noise and non-linguistic information that children must learn to filter. 

The use of audio-based input representations in low-resource settings has been pioneered by the Zero Resource Speech Challenge (ZRC), which for many years has focused on developing models that learn directly from raw speech without labelled data \citep[see][for an overview]{dunbar2022self}. The challenge is motivated by the need to develop speech technologies for the thousands of languages that are primarily or entirely unwritten. It is also driven by the goal of modelling language acquisition in a developmentally plausible framework, aiming to produce predictive models of how children learn language. These motivations closely parallel those behind the BabyLM Challenge, as discussed in \cref{sec:12-plausiblepretraining}. In particular, \citet{dupoux-2018-cognitive} argues that simulations of child language acquisition should (i) use audio as the input modality, (ii) rely on unsupervised learning algorithms, and (iii) be evaluated using psycholinguistic tasks that can be administered to both humans and models.

Over the years, ZRC has introduced a series of tasks within this framework. Early editions focused on representation learning, with two tasks grounded in child language acquisition \citep{versteegh2015zero, dunbar2017zero}. The first, \emph{acoustic unit discovery}, assesses whether learned representations support phonemic contrast using the ABX discrimination test \citep{schatz2016abx}, a task inspired by infants' perceptual tuning to native-language phonemes during the first year of life \citep{werker1984cross, kuhl1991human}. The second, \emph{spoken word discovery}, resembles the word segmentation task but operates directly on raw audio. Systems must first induce phoneme-like units from raw audio, then use these to identify word onsets and offsets from speech fragments. While these tasks do not explicitly require language models, later editions introduced a spoken language modelling task \citep{dunbar2021zero}, in which models are evaluated for lexical and syntactic sensitivity by assigning probabilities to minimal utterance pairs differing in grammaticality.

A notable model developed in this context is \stela (STatistical Learning of Early Language Acquisition), designed to simulate early phonetic, lexical and syntactic learning \citep{lavechin2022can, lavechin2025simulating}. Unlike many ZRC systems that rely on general-purpose encoders, \stela uses a sequential prediction approach inspired by \citet{elman-1990-finding}. Its pipeline consists of two stages: an acoustic model encodes \q{10}{ms} audio segments into discrete units, which are then fed into an LSTM language model. The acoustic model uses contrastive predictive coding followed by k-means clustering, based on the baseline model for ZRC 2021 \citep{nguyen2020zero}, and the LSTM is trained on the next-token prediction task. \stela is trained using \integer{3,200} hours of English and French audiobooks and evaluated on tasks similar to the acoustic unit discovery and spoken language modelling tasks in ZRC.

There are clear parallels between the ZRC evaluation tasks and the analytical methods used in this thesis. For example, the BabySLM benchmark employed in \cref{chapter:modelling} was inspired by the lexical and syntactic components of the ZRC spoken language modelling task \citep{lavechin}. Compared to the ZRC minimal pairs, BabySLM uses a vocabulary derived from child-directed speech and evaluates syntactic knowledge using constructions better aligned with spontaneous speech. In their study, \citet{lavechin} found that \stela performed worse on BabySLM benchmarks than models trained on phonemes or orthographic text, arguing that neither phoneme-based or subword-based representations are developmentally plausible.

Parallels also exist between the phonological probing experiments in \cref{chapter:phonology} and the acoustic unit discovery and word discovery tasks in ZRC. Audio-based models may be particularly suited to phonological probing because they naturally encode prosodic cues, which are known to play a crucial role in early lexical acquisition \citep{Cutler1987, Jusczyk1993stress, jusczyk-1999-stress-voice}. The status of phonemes in early processing remains debated: it is unclear whether phonemic categories are learned before or after word segmentation \citep{kazanina2018phonemes, matusevych2023infant}. Both \stela and the encoder-based model from \citet{schatz2021early} show that the discrete units induced from unsupervised clustering tend to be much shorter than phonemes and often span multiple phonemic categories. Nonetheless, these models still succeed on discrimination tasks. Further, \citet{lavechin2025simulating} show that such units encode distinctive features—including sonority, voicing, and place of articulation—even when trained on just \q{50}{\hour} of speech. They take this as evidence that linguistic categories are not required during the early stages of language acquisition.

Although results from audio-based models suggest that there may be limited value in using phoneme-based input representations, the motivations of this thesis differ substantially. \stela is intended as a cognitively plausible model of language acquisition, whereas the phoneme LMs in this thesis are not proposed as developmental simulations. Instead, they serve as a tool for analysing learned phonological representations and investigating how training paradigms influence model behaviour. While word segmentation is traditionally studied to understand how children learn, here it is used to probe the structure of the representations learned by phoneme LMs. Although an unsupervised segmentation strategy is proposed, the results are not meant to claim that children definitively use similar learning mechanisms, but rather to illustrate which statistical patterns are available for learning. These results also motivated the development of a new subword tokenisation method, demonstrating the practical utility of research into input representations.

Models like \stela may offer practical insights in the future, but a substantial performance gap remains between text-based and audio-based language models, particularly for higher-level linguistic tasks. Moreover, achieving full developmental plausibility remains elusive. Even though \stela uses audio input, it follows the ZRC framework and is trained on audiobooks. As discussed in \cref{chapter:resources}, read speech differs structurally and grammatically from naturalistic, spontaneous speech. Additionally, speech itself is often noisy, fragmented, and far less clearly articulated than the speech found in audio books. When learning language, children also need to distinguish language from ambient noise, overlapping speakers, music, and a variety of other non-linguistic sounds.

Recognising this, \citet{lavechin} and \citet{lavechin2024modeling} trained \stela on long-form recordings collected via child-worn devices to better simulate infant language input. They found that models trained on this data could perform above chance on the lexical BabySLM metric and discriminate native-language phonetic contrasts with sufficient data. However, \stela failed to perform above chance on syntactic benchmarks, highlighting the significant challenges that remain in building systems capable of learning language from raw signals in a manner comparable to human infants.

% Within this framework, \myemph{wav2vec 2.0} is a contrastive model, leveraging a transformer-based architectures trained with masked time-step prediction, which can later be fine-tuned for ASR \citep{baevski2020wav2vec}. Later models include \myemph{HuBERT} \citep{hsu2021hubert}, which builds on \myemph{wav2vec} by using hidden unit clustering, and \myemph{WavLM} \citep{chen2022wavlm}, which trains with masked speech prediction and multi-task objectives. In their survey, \citet{arora2025landscapespokenlanguagemodels} label these as speech encoders --- producing representations from speech that can then be used in other models or for specific speech processing tasks.   

% \begin{itemize}
%     \item \citet{mohamed2022self} review \emph{self-supervised learning} (SSL) as a growing subcategory of unsupervised learning approaches being used to learn representations of speech. They define SSL techniques as those that utilise information extracted from the input data as the label to learn useful representations. Their taxonomy distinguishes three classes of SSL models; generative models, contrastive models and predictive models. They conclude that SSL techniques such as \myemph{wav2vec 2.0} \citep{baevski2020wav2vec} and \myemph{HuBERT} \citep{hsu2021hubert} are state-of-the-art for creating useful representations that act as a starting point for downstream speech tasks, particularly when labelled data is scarce or difficult to source. 
%     \item \citet{prabhavalkar2023end} survey techniques that leverage neural architectures for ASR, calling these \emph{end-to-end} (E2E) models. Within their taxonomy, E2E ASR models use acoustic frames as an input-representation and output word or label sequences using an encoder-decoder framework. Under this view, SSL techniques like \myemph{wav2vec 2.0} and \myemph{HuBERT} are possible options for the encoder, but other techniques like semi-supervised learning could also be used.
%     \item \citet{arora2025landscapespokenlanguagemodels} use the term \emph{spoken language model} (SLM) to broadly refer to language models that are in principle capable of performing arbitrary speech tasks given natural language instructions. Their survey depicts the goal of SLM development to be to create a universal speech processing system that supports audio-based input and output, analogously to chat-based LLMs but for the speech domain. In their taxonomy, SSL systems like \myemph{wav2vec 2.0} and \myemph{HuBERT} are called \emph{speech encoders} with SLMs being categorised as either \emph{pure SLMs} (those that model speech directly using audio-based inputs), \emph{speech+text SLMs} (those that model the joint distribution of speech and corresponding text) and \emph{speech aware text LMS} (models that combine LLMs with separately pre-trained speech encoders).
% \end{itemize}

% There exist a range of techniques for learning representations directly from audio without relying on hand-crafted speech features. For example, \myemph{wav2vec} is a family of self-supervised models, some of which leverage transformer-based architectures trained with masked time-step prediction, which can later be fine-tuned for ASR \citep{baevski2020wav2vec}. Later models include \myemph{HuBERT} \citep{hsu2021hubert}, which builds on \myemph{wav2vec} by using hidden unit clustering, and \myemph{WavLM} \citep{chen2022wavlm}, which trains with masked speech prediction and multi-task objectives. In their survey, \citet{arora2025landscapespokenlanguagemodels} label these as speech encoders --- producing representations from speech that can then be used in other models or for specific speech processing tasks.  

% The symbolic representations learned by the model have a duration four times shorter than phonemes, challenging the assumption that phonetic categories are precursors to later stages of acquisition. 

% These models are also used to study acquisition, regarding raw audio as an input representation that is more cognitively plausible than phonemes; a continuous signal full of noise and non-linguistic information that children must learn to filter. Whether adults even use phonemes as a core linguistic representation, and whether children learn phonemic categories before other stages of acquisition both continue to be a matter of debate \citep{kazanina2018phonemes, matusevych2023infant} and the symbolic representations learned by models such as STELA have a duration four times shorter than phonemes, challenging the assumption that phonemic categories are precursors to later stages of acquisition. 


%\citet{lavechin} developed the BabySLM evaluation metrics to allow text-based LMs to be compared to speech-based LMs, listing text-based models (including models that train on phoneme strings) as equally low in terms of plausibility compared to audio-based models and highlighting a large gap in performance between text-based and audio-based models. They also note that even speech-based models may not always train on plausible input, many often using audiobooks as their training data \citep{kahn2020libri}. When training the STELA model on 1024 hours of ecological long-form child-centred audio compared to 1024 hours of audiobooks, \citet{lavechin} found that the model trained on long-form audio achieved chance-level syntactic and lexical capabilities, highlighting how far we are from producing architectures that can learn from the same signals as human children.

%\subsection{Lingering advantages}

%[Despite us approaching human-like input, sequences of phonemes are still an abstraction from the true nature of input we receive as children and in our day-to-day lives, which in reality is continuous audio, and full of noise and non-linguistic information. Furthermore, phonemes are themselves an abstraction of phones and are learned representations that may even be learned after some initial stages of acquisition, raising questions as to whether they should be used as a primary representation of the input. Arguably we do get to a point where we process audio into streams of phonemes, but that might happen later, and phonemes are also language-dependent.]

%[Some studies do try to learn from true audio input, but data is very limited. Some do show performance (e.g. the audio book one) but even those have advantages, and once removed (using true raw audio from a child's environment) those same models seem to learn nothing. Perhaps we just need more data, as those represent very little, but perhaps we are further than we thought in terms of producing a model that is able to learn language in exactly the same conditions as our amazing brains.]

% \paragraph{Limitations of phonemic data:} Using phonemic data for the word segmentation task is the typical framework for exploring relevant acquisition theories. However, the phonemic transcriptions in \ipachildes do have limitations. Having been generated using grapheme-to-phoneme (G2P) conversion, they may have been subject to conversion error, and the original transcriptions may also contain errors. The G2P process also removes natural variation in speech, such as accents and allophonic variation. The symbolic nature of phonemes may also be an unrealistic starting point for acquisition; it is unclear if infants have access to phonetic categories at this stage of acquisition \citep{feldman_infants_2021, mcmurray_myth_2022}. Researchers who advocate for using language models as cognitive models argue that the training data should be as developmentally plausible as possible \citep{dupoux-2018-cognitive, warstadt-2022-artificial}, and that phonemes may be as implausible as text for simulating early acquisition \citep{lavechin}.

% From this perspective, a more appropriate framework is to learn segmentation directly from raw audio, as pursued in the Zero Resource Speech Challenge \citep{nguyen2020zero, dunbar2021zero}. Audio-based models naturally incorporate prosodic cues, which play a key role in language acquisition \citep{Cutler1987, Jusczyk1993stress, jusczyk-1999-stress-voice}. Unsupervised models have demonstrated the ability to perform statistical learning directly from raw speech \citep{lavechin2022can, seyssel-2023-realistic}, and have found that the resulting units tend to be shorter than phonemes, consistent with early perceptual categories \citep{schatz2021early}. While such models show promising signs of early phonetic learning and perform well on word-level tasks, they currently require significantly more data to match the performance of text-based models \citep{lavechin}. Moreover, training on curated audiobook datasets gives these models a considerable advantage over learning from noisier, long-form audio that better resembles real-world input—but ongoing work is making such realistic simulations increasingly viable \citep{lavechin2024modeling}.

% Our resources could also support the training of self-supervised speech models \citep[e.g.][]{hsu2021hubert}. These models are trained directly on audio and lag behind phoneme or text-based models, often requiring several orders of magnitude more data to learn semantic representations \citep{cuervo2024scaling}, but recent work has found that fine-tuning on phoneme classification can reduce this gap \citep{feng-2023-language-universal-phonetic, poli2024improving}. 


\section{Future directions}
\label{sec:17-futurework}

Finally, I outline several promising directions for future research.

Multilingual phoneme LMs and the BabyLM Challenge:
In \cref{chapter:modelling}, I used the BabyLM challenge framework to investigate the effects of phoneme-based pre-training using English data and evaluated performance using BLiMP and GLUE. While \cref{chapter:phonology} extended the analysis to 31 languages for phonological evaluation, the modelling experiments remained monolingual. The recently announced fourth edition of the BabyLM Challenge will include a multilingual track, with a new multilingual dataset and evaluation via MultiBLiMP. Future entries could adapt the techniques developed here to this multilingual setting, evaluating phoneme LMs cross-linguistically and testing whether the three identified input transformations have comparable effects beyond English.

\begin{itemize}
    \item \textbf{Multilingual phoneme LMs and the BabyLM Challenge:} In \cref{chapter:modelling} I used the BabyLM challenge framework to investigate the effects of phoneme-based pre-training using English data and evaluated performance using BLiMP and GLUE. While \cref{chapter:phonology} explored phoneme LMs in 31 languages for phonological evaluation, the modelling experiments remained monolingual. The recently announced fourth edition of the BabyLM challenge will include a multilingual track, with a new multilingual dataset and evaluation using MultiBLiMP. releasing a multilingual dataset and using MultiBLiMP for evaluation. Future entries could adapt the techniques developed here to this multilingual setting, evaluating phoneme LMs cross-linguistically and testing whether the three identified input transformations have comparable effects beyond English.
    \item \textbf{Expanding language coverage for phonological analysis:} The phoneme LMs introduced in \cref{chapter:phonology} were trained on 31 languages and support cross-lingual studies of distributional phonology. However, these languages are still relatively high-resource and concentrated in Europe and Asia. This limitation reflects the language distribution in CHILDES. Relaxing the constraints of training on child-directed speech could enable training on broader datasets, such as the CMU Wilderness corpus \citep{8683536}, which contains bible readings in 699 languages. Furthermore, \gpp could be applied to orthographic datasets, as was demonstrated when converting the BabyLM dataset in \cref{chapter:modelling}. This opens the door to phoneme-based variants of large-scale resources such as the Goldfish suite, which includes 350 monolingual models \citep{chang2024goldfish}. While phoneme-based benchmarks remain limited, \cref{chapter:phonology} demonstrated that unsupervised word segmentation and distinctive feature probes can serve as effective language-agnostic tools for cross-lingual phonological analysis.
    \item \textbf{Bilingual and multilingual phoneme LMs:} This thesis focused on monolingual phoneme LMs. However, recent work in low-resource language modelling has begun exploring orthographic bilingual LMs to assess the effects of multilingual pre-training \citep{arnett2025acquisition}. Extending this approach to phoneme-based models could offer a powerful method for investigating phonological priming, cross-lingual transfer, and bilingual acquisition. A key challenge in this setting is that phonemes are language-specific. One possible solution is to adopt the approach of \citet{li2020universal}, who learn both language-specific and universal phoneme representations. Such an approach could provide novel insights into the structure of cross-linguistic phonology and bilingual learning dynamics.
    \item \textbf{Further development of \bytespan :} In \cref{chapter:infotokenisation}, I introduced and evaluated several variants of \bytespan, each using different strategies for grouping predictable byte spans and forming a subword vocabulary. There remains considerable room for extension. Future work could experiment with a broader range of hyper-parameters, inference strategies during tokenisation, or alternative byte-level models, such as lightweight \ngram LMs. Additionally, the cross-linguistic potential of \bytespan warrants further investigation, particularly in languages with rich morphology, where linguistic alignment may provide greater benefits than in English.
    \item \textbf{Comparative studies across input modalities:}
    The central focus of this thesis has been on phoneme-based input representations, but recent work on audio-based models reviewed in \cref{sec:17-audiomodels} offers exciting possibilities. These models, often developed within developmentally plausible frameworks, aim to simulate human acquisition from raw audio. Future research could compare phoneme-based, subword-based, and audio-based input representations more systematically, extending the experiments in \cref{chapter:modelling} to a tri-modal setting. Additionally, experiments in \cref{chapter:phonology} could be extended, for instance by evaluating \stela alongside phoneme LMs to compare their cross-linguistic representations. Just as word segmentation inspired the creation of \bytespan, comparative studies involving audio-based models may inspire new practical tokenisation techniques and improve our understanding of how models learn language from different modalities.
\end{itemize}
