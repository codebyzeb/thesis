\chapter{Conclusion}\label{chapter:conclusion}

This chapter concludes the work presented in this thesis. First, \cref{sec:17-summary} provides a summary of the contributions of this thesis, returning to the four research questions presented in \cref{chapter:intro}. \cref{sec:17-audiomodels} then provides a discussion comparing this work to recent advances in models that learn directly from audio. Finally, \cref{sec:17-futurework} discusses open questions and directions for future work.

\section{Phonological representations in language models}
\label{sec:17-summary}

This thesis has explored phonological representations in language models through experimentation with a phoneme-based input representation. 


% \subsection{Comparing Human Acquisition to Language Model Learning}
% \label{sec:14-acquisition}

% % Here we could go into the BabyLM challenge as related work that looks into reducing the advantages language models have over humans, say that our work here contributes to the BabyLM challenge, and finally briefly mention the literature regarding 'advantages' and point to our discussion section on audio-based models. 

% The capacity of LMs to learn language from text alone has spurred interest in using such models for acquisition and psychology studies, such as comparing model learning trends to child learning behaviour \citep{evanson-etal-2023-language} and using model outputs to predict human reading times \citep{hollenstein-etal-2021-multilingual}.

% To push this research further, recent efforts aim to make language modeling more cognitively plausible \citep{beinborn2024cognitive} by reducing the advantages that typical language models have over humans during the learning process \citep{warstadt-2022-artificial}. One approach is to limit and curate the dataset to that which a typical human may be exposed to, such as is done in the BabyLM challenge \citep{warstadt-2023-babylm-findings}. Another approach is to use an input representation that more closely mimics speech rather than written text \citep{dupoux-2018-cognitive}. Finally, we must consider whether the architectures themselves are suitable linguistic theories, given that they were developed for downstream tasks \citep{baroni-2022-proper}.

% In this work we contribute to all three approaches by training a language model with streams of phonemes and assess whether the language model architecture used is advantaged or disadvantaged by these changes according to a wide variety of benchmarks. We hope that this leads to further work studying acquisition using phoneme streams as an input representation. However, while streams of phonemes may seem more cognitively plausible than written text, many studies go further than we do and seek to train directly on raw audio.

\section{Learning directly from audio}
\label{sec:17-audiomodels}

This thesis has focused on exploring insights that can be gained from pre-training language models with a phoneme-based input representation, but there is also a field of work dedicated to training models directly from raw audio. Just as the architectures of language models used in NLP has evolved in recent years (as discussed in \cref{sec:12-architectures}), so too has the development of language models that use audio-based input representations.

There exist a range of techniques for learning representations directly from speech audio without relying on hand-crafted speech features. These techniques are known as self-supervised learning (SSL), a subcategory of unsupervised learning where information extracted from the input data is used as labels by a model for learning representations. A wider review of these techniques is provided by \citet{mohamed2022self}. Notably, \myemph{wav2vec 2.0} is a family of self-supervised models, some of which leverage transformer-based architectures trained with masked time-step prediction, which can later be fine-tuned for ASR \citep{baevski2020wav2vec}. Later models include \myemph{HuBERT} \citep{hsu-2021-hubert}, which builds on \myemph{wav2vec} by using hidden unit clustering, and \myemph{WavLM} \citep{chen2022wavlm}, which trains with masked speech prediction and multi-task objectives.

Audio-based input representations have also played a major role in the development of \emph{end-to-end} (E2E) ASR models, which use a single network to translate an audio-based input representation to an output token sequence. \citet{li2022recent} explore how E2E models --- which use a single objective function consistent with ASR --- achieve state-of-the-art results compared to hybrid systems that must separately optimise multiple components. A wider review of E2E ASR is provided by \citet{prabhavalkar2023end}. More recently, audio-based input representations have been explored to create LLMs compatible with speech-based input and output. \citet{arora2025landscapespokenlanguagemodels} provide a survey of these techniques, seeking to establish the term \emph{spoken language model} (SLM) as referring to language models that are in principle capable of performing arbitrary speech tasks given natural language instructions. They frame SLM development as ultimately aiming to create a universal speech processing system that supports audio-based input and output.

Although this work has primarily focused on the analytical value of phoneme-based input representations, it is worth considering the practical value of this representation given that state-of-the-art speech systems train directly from audio and so do not require an intermediary phoneme-level representation. However, as discussed in \cref{sec:12-practicalphoneme}, E2E systems are very data hungry, making them impractical for low-resource speech technology \citet{li2022recent}. Hence, the phoneme-based input representation continues to be useful for speech technology. For example, \citet{feng-2023-language-universal-phonetic} found that phonemes can act as a useful universal representation for multilingual speech pre-training. Separately, noting that SLMs can require three orders of magnitude more data than text-based counterparts to learn semantic representations, \citet{poli2024improving} demonstrate that fine-tuning SLMs with phoneme classification can address this gap --- demonstrating how research into phoneme-based representations can contribute to low-resource language modelling. The work in this thesis has further demonstrated the grammatical capabilities of phoneme LMs trained with next-phoneme prediction, which can support future work in low-resource speech modelling.

%However, such models are extremely data-hungry \citep{li2022recent}, making them impractical for the 7000+ human languages which are considered under-resourced \citep{scharenborg2020speech}.

The use of audio-based input representations in low-resource settings has been especially pioneered by the Zero Resource Speech Challenge (ZRC), which for many years has explored the development of models that learn representations from raw speech with no labelled data \citep[see][for an overview]{dunbar2022self}. The challenge is motivated by developing speech processing systems particularly for the many thousands of languages that are mostly or entirely unwritten, but is also motivated by exploring representation learning in a developmentally-plausible framework, aiming to provide predictive models of language development. These motivations are very similar to those driving the BabyLM challenge, as discussed in \cref{sec:12-plausiblepretraining}. In particular, \citet{dupoux-2018-cognitive} argues that simulations of child language acquisition should use an audio-based input representation, use an unsupervised learning algorithm and be evaluated using psycholinguistic tests that can be passed by humans and models alike \citet{dupoux-2018-cognitive}.

%These models are also used to study acquisition, regarding raw audio as an input representation that is more cognitively plausible than phonemes; a continuous signal full of noise and non-linguistic information that children must learn to filter. 

The challenge has evaluated models using a number of tasks over the years. Early editions were focused on representation learning, using two tasks primarily motivated by child language acquisition \citep{versteegh2015zero, dunbar2017zero}. The first task is \defn{acoustic unit discovery}, testing whether learned representations support phonemic contrast using an ABX discrimination test \citep{schatz2016abx}. This relates to the perceptual tuning that occurs during the first year of life to enable humans to distinguish between phonemes in their native language \citep{werker1984cross, kuhl1991human}. The second task is \defn{spoken word discovery} and is based on the word segmentation task, but instead of operating over phonemes, systems must first learn phoneme clusters from raw audio and used these to identify word onsets and offsets of speech fragments. Whereas systems for these two tasks did not need to be able to model language, later editions of the challenge did introduce a \defn{spoken language modelling} task \citep{dunbar2021zero} --- where models are tested for lexical and syntactic ability by using them to assign probabilities to minimal pairs of utterances, one legal and the other illegal.

A notable mode that has emerged from ZRC is \stela (STatistical Learning of Early Language Acquisition), designed to simulate early phonetic learning and language acquisition \citep{lavechin2022can, lavechin2025simulating}. Unlike many of the ZRC entries which primarily use speech encoders, \stela is based on sequential prediction, citing the influence of \citet{elman-1990-finding}. Raw audio is processed two stages. Firstly, an acoustic model encodes sequences of \q{10}{ms} of audio into discrete tokens using a contrastive predictive coding algorithm followed by k-means, based on the baseline model for ZRC 2021 \citep{nguyen2020zero}. Secondly, these discrete tokens are fed into an LSTM LM, which is trained on the next-token prediction task. \stela is trained using 3200 hours of English and French audio books and is evaluated using tasks similar to the acoustic unit discovery and language modelling tasks in ZRC.

There are clear parallels between the ZRC tasks and the analytical work presented in this thesis. The BabySLM benchmark used in \cref{chapter:modelling}, for instance, was based on the lexical and syntactic measures for the spoken language modelling task. Where these benchmarks differ is that the vocabulary used in BabySLM is designed to be compatible with child-directed speech, and the syntactic benchmark uses constructions that align more closely with spoken language \citep{lavechin}. In their study, \citet{lavechin} found that \stela did not perform as well on the benchmark as models trained with phonemes or orthographic text, stating that both neither phonemes and nor orthographic text are developmentally plausible input representations.

There are also parallels between the phonological probes used in \cref{chapter:phonology} and the ZRC acoustic unit discovery and spoken word discovery tasks. Arguably, models developed for these tasks are more suited for phonological probing, since audio-based input representations naturally incorporate prosodic cues. If the aim is to simulate acquisition, it is important to include these cues, as they play a key role in early lexical development \citep{Cutler1987, Jusczyk1993stress, jusczyk-1999-stress-voice}. The very nature of phonemes in the processing system, and whether children learn phonemic categories before word segmentation both continue to be a matter of debate \citep{kazanina2018phonemes, matusevych2023infant}. Both \stela and a similar acoustic encoding model by \citet{schatz2021early} found that discrete units learned from unsupervised clustering tend to be four times shorter than phonemes and the same units can encode multiple different phonemes, yet these models still succeed at discrimination tasks. \citet{lavechin2025simulating} also demonstrate that these units can be probed for distinctive features, similar to the experiments presented in \cref{chapter:phonology}, finding a clear emergence of sonority, voicing and place of articulation even in models trained on only \q{50}{\hour} of speech. They take these results as evidence that linguistic categories are not required during the early stages of language acquisition.

Although the results from these studies suggest that there is limited value in using phoneme-based input representations, the motivations are very separate. \stela is designed to simulate child language acquisition, whereas the phoneme LMs trained in this thesis are not claimed to be realistic simulations. Instead, this thesis has demonstrated that training LMs with a phoneme-based input representation can provide valuable insights into how LMs train, and the distributed representations of phonology across languages. The word segmentation task has traditionally been used to argue for mechanisms children may use to learn, but here it is primarily used to investigate the representations learned by phoneme LMs. Although an unsupervised word segmentation strategy is presented, the positive results are used to indicate what statistical features exist in language that can be learned, rather than stating that children definitely use these. These results additionally inspired a new method for subword tokenisation, demonstrating the practical value of research into input representations.

Audio-based models like \stela could also provide practical insights, but the gap in linguistic performance between text-based models and audio-based models continues to be substantial. Additionally, it is near-impossible to reach the perfect developmental plausibility recommended by \citet{dupoux-2018-cognitive} when it comes to simulating acquisition. For instance, also \stela uses an audio-based input representation, it still follows the ZRC challenge framework in training on audio books. As discussed in \cref{chapter:resources}, audio books consist of written text read aloud, which has very different grammatical constructions compared to naturalistic speech. Additionally, children primarily hear ambient sounds, noise, music and other non-linguistic audio, and even when they do listen to speech, it can be noisy, concurrent with other competing sources, and not as clearly articulated as audio books. Considering this, \citet{lavechin} and \citet{lavechin2024modeling} explore training \stela on long-form recordings collected from devices worn by language-learning infants, finding that although these models achieve higher than chance on the lexical BabySLM metric, and are able to discriminate native-language phonetic contrasts with sufficient training data, they do not beat chance in the syntactic test, highlighting how far there is to go to produce architectures that can learn from the same signals as human children.

% Within this framework, \myemph{wav2vec 2.0} is a contrastive model, leveraging a transformer-based architectures trained with masked time-step prediction, which can later be fine-tuned for ASR \citep{baevski2020wav2vec}. Later models include \myemph{HuBERT} \citep{hsu2021hubert}, which builds on \myemph{wav2vec} by using hidden unit clustering, and \myemph{WavLM} \citep{chen2022wavlm}, which trains with masked speech prediction and multi-task objectives. In their survey, \citet{arora2025landscapespokenlanguagemodels} label these as speech encoders --- producing representations from speech that can then be used in other models or for specific speech processing tasks.   

% \begin{itemize}
%     \item \citet{mohamed2022self} review \emph{self-supervised learning} (SSL) as a growing subcategory of unsupervised learning approaches being used to learn representations of speech. They define SSL techniques as those that utilise information extracted from the input data as the label to learn useful representations. Their taxonomy distinguishes three classes of SSL models; generative models, contrastive models and predictive models. They conclude that SSL techniques such as \myemph{wav2vec 2.0} \citep{baevski2020wav2vec} and \myemph{HuBERT} \citep{hsu2021hubert} are state-of-the-art for creating useful representations that act as a starting point for downstream speech tasks, particularly when labelled data is scarce or difficult to source. 
%     \item \citet{prabhavalkar2023end} survey techniques that leverage neural architectures for ASR, calling these \emph{end-to-end} (E2E) models. Within their taxonomy, E2E ASR models use acoustic frames as an input-representation and output word or label sequences using an encoder-decoder framework. Under this view, SSL techniques like \myemph{wav2vec 2.0} and \myemph{HuBERT} are possible options for the encoder, but other techniques like semi-supervised learning could also be used.
%     \item \citet{arora2025landscapespokenlanguagemodels} use the term \emph{spoken language model} (SLM) to broadly refer to language models that are in principle capable of performing arbitrary speech tasks given natural language instructions. Their survey depicts the goal of SLM development to be to create a universal speech processing system that supports audio-based input and output, analogously to chat-based LLMs but for the speech domain. In their taxonomy, SSL systems like \myemph{wav2vec 2.0} and \myemph{HuBERT} are called \emph{speech encoders} with SLMs being categorised as either \emph{pure SLMs} (those that model speech directly using audio-based inputs), \emph{speech+text SLMs} (those that model the joint distribution of speech and corresponding text) and \emph{speech aware text LMS} (models that combine LLMs with separately pre-trained speech encoders).
% \end{itemize}

% There exist a range of techniques for learning representations directly from audio without relying on hand-crafted speech features. For example, \myemph{wav2vec} is a family of self-supervised models, some of which leverage transformer-based architectures trained with masked time-step prediction, which can later be fine-tuned for ASR \citep{baevski2020wav2vec}. Later models include \myemph{HuBERT} \citep{hsu2021hubert}, which builds on \myemph{wav2vec} by using hidden unit clustering, and \myemph{WavLM} \citep{chen2022wavlm}, which trains with masked speech prediction and multi-task objectives. In their survey, \citet{arora2025landscapespokenlanguagemodels} label these as speech encoders --- producing representations from speech that can then be used in other models or for specific speech processing tasks.  

% The symbolic representations learned by the model have a duration four times shorter than phonemes, challenging the assumption that phonetic categories are precursors to later stages of acquisition. 

% These models are also used to study acquisition, regarding raw audio as an input representation that is more cognitively plausible than phonemes; a continuous signal full of noise and non-linguistic information that children must learn to filter. Whether adults even use phonemes as a core linguistic representation, and whether children learn phonemic categories before other stages of acquisition both continue to be a matter of debate \citep{kazanina2018phonemes, matusevych2023infant} and the symbolic representations learned by models such as STELA have a duration four times shorter than phonemes, challenging the assumption that phonemic categories are precursors to later stages of acquisition. 


%\citet{lavechin} developed the BabySLM evaluation metrics to allow text-based LMs to be compared to speech-based LMs, listing text-based models (including models that train on phoneme strings) as equally low in terms of plausibility compared to audio-based models and highlighting a large gap in performance between text-based and audio-based models. They also note that even speech-based models may not always train on plausible input, many often using audiobooks as their training data \citep{kahn2020libri}. When training the STELA model on 1024 hours of ecological long-form child-centred audio compared to 1024 hours of audiobooks, \citet{lavechin} found that the model trained on long-form audio achieved chance-level syntactic and lexical capabilities, highlighting how far we are from producing architectures that can learn from the same signals as human children.

%\subsection{Lingering advantages}

%[Despite us approaching human-like input, sequences of phonemes are still an abstraction from the true nature of input we receive as children and in our day-to-day lives, which in reality is continuous audio, and full of noise and non-linguistic information. Furthermore, phonemes are themselves an abstraction of phones and are learned representations that may even be learned after some initial stages of acquisition, raising questions as to whether they should be used as a primary representation of the input. Arguably we do get to a point where we process audio into streams of phonemes, but that might happen later, and phonemes are also language-dependent.]

%[Some studies do try to learn from true audio input, but data is very limited. Some do show performance (e.g. the audio book one) but even those have advantages, and once removed (using true raw audio from a child's environment) those same models seem to learn nothing. Perhaps we just need more data, as those represent very little, but perhaps we are further than we thought in terms of producing a model that is able to learn language in exactly the same conditions as our amazing brains.]

% \paragraph{Limitations of phonemic data:} Using phonemic data for the word segmentation task is the typical framework for exploring relevant acquisition theories. However, the phonemic transcriptions in \ipachildes do have limitations. Having been generated using grapheme-to-phoneme (G2P) conversion, they may have been subject to conversion error, and the original transcriptions may also contain errors. The G2P process also removes natural variation in speech, such as accents and allophonic variation. The symbolic nature of phonemes may also be an unrealistic starting point for acquisition; it is unclear if infants have access to phonetic categories at this stage of acquisition \citep{feldman_infants_2021, mcmurray_myth_2022}. Researchers who advocate for using language models as cognitive models argue that the training data should be as developmentally plausible as possible \citep{dupoux-2018-cognitive, warstadt-2022-artificial}, and that phonemes may be as implausible as text for simulating early acquisition \citep{lavechin}.

% From this perspective, a more appropriate framework is to learn segmentation directly from raw audio, as pursued in the Zero Resource Speech Challenge \citep{nguyen2020zero, dunbar2021zero}. Audio-based models naturally incorporate prosodic cues, which play a key role in language acquisition \citep{Cutler1987, Jusczyk1993stress, jusczyk-1999-stress-voice}. Unsupervised models have demonstrated the ability to perform statistical learning directly from raw speech \citep{lavechin2022can, seyssel-2023-realistic}, and have found that the resulting units tend to be shorter than phonemes, consistent with early perceptual categories \citep{schatz2021early}. While such models show promising signs of early phonetic learning and perform well on word-level tasks, they currently require significantly more data to match the performance of text-based models \citep{lavechin}. Moreover, training on curated audiobook datasets gives these models a considerable advantage over learning from noisier, long-form audio that better resembles real-world inputâ€”but ongoing work is making such realistic simulations increasingly viable \citep{lavechin2024modeling}.

% Our resources could also support the training of self-supervised speech models \citep[e.g.][]{hsu2021hubert}. These models are trained directly on audio and lag behind phoneme or text-based models, often requiring several orders of magnitude more data to learn semantic representations \citep{cuervo2024scaling}, but recent work has found that fine-tuning on phoneme classification can reduce this gap \citep{feng-2023-language-universal-phonetic, poli2024improving}. 


\section{Future directions}
\label{sec:17-futurework}

Wider limitations of work in this thesis?

