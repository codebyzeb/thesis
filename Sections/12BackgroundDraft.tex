
\section{Tokenization in Language Models}\label{sec:12-tokenization}


%Large language models are typically trained on written text using tokenizers that split the text into subword units. By contrast, studies that train models using a phoneme-based input representation tend to use tokenizers that produce a token for each individual phoneme, akin to character-based language models. Often, phonemes are fed to a model in a sequence that does not contain word boundaries, as utterances tend to be produced continuously, with no pauses between individual words. 

The standard input representation for training large language models consists of written text split into subword units. By contrast, studies that train models using a phonemic input representation tend to split words into individual phonemes, without word boundaries (as spoken utterances are produced continuously, without clear pauses between words).

We identify three key transformations that bring us from the standard input representation used by language models to this alternative \textbf{phoneme stream} representation:

% \begin{itemize}
%     \item Using a character-based tokenization instead of using subwords.
%     \item Removing word boundaries from the input.
%     \item Using phonemic transcriptions instead of written text.
% \end{itemize}

\begin{itemize}
\setlength\itemsep{0.1em}
    \item \characterhighlight{Character tokenization} Treating each phoneme or grapheme as a token, rather than using subwords.
    \item \spacehighlight{Word boundary removal} Removing whitespace or other word boundary cues from the input.
    \item \phonemehighlight{Phonemic transcription} Converting words to a phonemic representation. 
\end{itemize}

% \vspace{1em}
% \noindent\characterhighlight{Character tokenization} Treating each phoneme or grapheme as a token, rather than using subwords.

% \vspace{0.5em}
% \noindent\spacehighlight{Word boundary removal} Removing whitespace or other word boundary cues from the input.

% \vspace{0.5em}
% \noindent\phonemehighlight{Phonemic transcription} Converting words to a phonemic representation. 
% \vspace{1em}

\noindent
Each transformation can be made independently or in combination, as illustrated in \cref{fix:14-example}. %By implementing every possible combination, we can identify the impact of each transformation on language learning by LLMs. 
% We can also create novel representations of the input which may not have been considered (such as learning subword units from text stripped of word boundaries). 
% The three input transformations have mostly been studied separately. 

Previous literature has extensively explored these three transformations but they have typically been studied independently and been used for different downstream purposes. 

\subsection{Training with Phonemes}

Several language models have been trained with phonemic input \citep{sundararaman-2021-phonemebert, gale-etal-2023-bort} but it remains a challenge to do so due to the lack of large phonemic corpora. While a number of well-known speech-based datasets include phonemic transcriptions, such as Switchboard \citep{godfrey1992switchboard} and TIMIT \citep{garofolo1993darpa}, these datasets are small compared to the trillions of tokens contained in standard language model pre-training corpora \citep{elazar-2024-redpajama}. The majority of works that use phonemic representations typically rely on grapheme to phoneme conversion tools \citep{bisani-2008-g2p, hasegawa-2020-g2pmultilingual} to generate coarse phonemic transliterations of text data.

It is also a challenge to evaluate the broad capabilities of language models trained with phonemes, as most benchmarks assume a graphemic representation, even some that assess phonological knowledge \citep{suvarna-etal-2024-phonologybench}. One benchmark that assesses both the syntactic and phonological capabilities of language models is BabySLM \citep{lavechin}. %One distinguishing advantage of this benchmark is that it can be applied to models trained on either graphemes or phonemes.
We discuss this benchmark further in \cref{sec:14-effect}.

\subsection{Character-based Language Models}

The use of characters as the input representation, rather than words or subwords, has been extensively explored. Character-level language models offer a simplified input stream compared to the standard approach of training on learned susword tokens. Many studies have developed specialised architectures to train language models on characters \citep{jozefowicz2016exploringlimitslanguagemodeling, kim2016character, ma-etal-2020-charbert, al-rfou_character-level_2019} while other approaches seek to establish `token-free' training regimes to eliminate the need for subwords entirely \citep{clark-etal-2022-canine, xue-2022-byt5}.

Another alternative input representation is to split words into morphemes, which provide theoretical benefits over subwords and have their own analytical and practical benefits particularly for morphologically rich languages \citep{ustun-etal-2018-characters, nzeyimana-niyongabo-rubungo-2022-kinyabert, fan-sun-2023-constructivist}. Mapping orthographic text to morphemes continues to be a challenging task, requiring dedicated systems trained on labeled corpora \citep{batsuren-etal-2022-sigmorphon} and we do not consider morphemes in this work.

\subsection{Removal of Word Boundaries}

When using a phonemic input representation to model speech, word boundaries are not typically included, as word boundaries are not explicitly marked in the speech stream. The phoneme stream representation (i.e.,\ the combination of all three transformations) is the typical representation for word segmentation studies, where the task is to learn word boundaries without supervision \citep{Brent1999}. A wide variety of statistical, dynamic programming and neural approaches have been applied to the task, with consequences for acquisition research and low-resource language modeling \citep{Blanchard2010, Coltekin2017, algayres_dp-parse_2022, goriely2023word}.

\subsection{Input Representation Comparisons}

To the best of our knowledge, a full systematic comparison of the three input transformations has not yet been conducted.  \citet{hahn-baroni-2019-tabula} investigated the effect of removing word boundaries and using a word-level or character-level tokenization, evaluating on several psycholinguistic benchmarks. However, they only used graphemic text from Wikipedia and did not ablate the two transformations, only comparing a word-level model (with word boundaries) to a character-level model (without word boundaries). \citet{nguyen-2022-word-boundaries} extend this work, comparing character-level graphemic input (with and without word boundaries) to character-level phonemic input (with and without word boundaries) by training on the Librispeech corpus \citep{panayotov2015librispeech}. They also compare larger units of tokenization (BPE and word-level) for both graphemic and phonemic text, but only with word boundaries included, missing out on several key combinations. 

In our work, we provide a complete comparison of these three input representation transformations by considering all combinations, leading to new input representations that have not been studied before (such as subword tokenization trained without word boundaries). We also use a larger model than previous work, a 12-layer transformer rather than a 3-layer LSTM.

% Discuss word boundaries and how the removal has been done before. 

%Another simple heuristic for processing text to be more speech-like is to remove word boundaries. At the simplest level of approximation, some past efforts have studied the effects of removing word boundaries from language models \citep{hahn-baroni-2019-tabula, nguyen-2022-word-boundaries}, finding that LSTM-based models see a marked decrease in performance on several psycholingustic benchmarks. 

% However, these architectures were developed for downstream tasks and not for linguistic theories \citep{baroni-2022-proper}. Before using such models, we must establish the extent to which using standard orthographic text gives language models an advantage over input representations that more closely mimic human speech. 

%However, underlying these approaches is a concern that language models trained under common paradigms may not be scientifically sound for providing insights into human language acquisition. This is because the development of language models is driven by downstream performance on popular tasks rather than being grounded in linguistic theories \citep{baroni-2022-proper}. The recent explosion in the size of language models has further complicated comparisons between human and neural network language acquisition. \citet{warstadt-2022-artificial} argue that to draw valid conclusions about human language acquisition from neural networks, the trained models should not possess any advantages over humans. Similarly, \citet{dupoux-2018-cognitive} proposes that machine learning models should use raw sensory signals perceived by humans (e.g., sound waves) as inputs to remove confounding variables that make interpreting their behaviour as models of language acquisition challenging. Similarly to how the BabyLM challenge reduces the training set to a reasonable size, the Zero Resource Speech Challenge involves developing unsupervised methods that learn language directly from audio \citep{dunbar_self-supervised_2022}. We discuss this paradigm further in \cref{sec:14-audiomodels}.

\section{Phonological Evaluation}\label{sec:12-phoneval}


Since their inception, language models have been used to study the structures of language and explore mechanisms that humans may use to learn them. 

%Since their inception, LLMs have been used to study the various structures and components of language. However, benchmarks that evaluate phonology are relatively scarce compared to established benchmarks for syntax and semantics.

Early ``connectionist'' language models were trained on sequences of letters or phonemes, often using developmentally plausible data in order to explore theories of word learning and phonology \citep{seidenberg1989distributed, norris1994shortlist, coltheart2001drc}. Modern \emph{large} language models (LLMs) are still probed for grammatical information, but standard benchmarks are generally based on higher-order structures: syntax and semantics rather than morphology and phonology. This due to LLM design being optimised for downstream tasks, not linguistic analysis. For instance, LLMs are typically trained on graphemic text using subword tokens. While this representation is practical for large-scale training, these tokens are not very cognitively plausible \citep{beinborn-pinter-2023-analyzing}, are less effective than character-based tokens for learning word structure \citep{bunzeck2025subwordmodelsstruggleword} and cannot be used to explore representations of phonological units. Additionally, modern LLMs are inappropriate for theories of acquisition, due to the scales of data they are trained on \citep{warstadt-2023-babylm-findings}.

%In recent years, the BabyLM challenge has explored the training of smaller LLMs (so called ``BabyLMs'') on developmentally plausible data, leading to new advancements across pre-training strategies, architectures and tools for linguistic analysis \citet{hu2024findings}. Within this framework, \citet{goriely2024babble, bunzeck2024graphemes} demonstrated that these models could be trained with phonemes


%In part, this can be attributed to the fact that most state-of-the-art LLMs are trained on graphemic text with a subword tokenization scheme. While this representation has proven to be useful for downstream tasks and allows for linguistic probing at the syntactic and semantic levels, these tokens are not cognitively plausible \citep{beinborn-pinter-2023-analyzing}, and prevent models from learning the structure of words compared to using character-based tokenizers \citep{bunzeck2025subwordmodelsstruggleword}.

Here, we are interested in evaluating models that train directly on individual phonemes, without word boundaries. When trained on individual words, phoneme LMs have been used to study the acquisition of morphological rules \citep{kirov-2018-recurrent} and compare phonotactic complexity across languages \citep{pimentel2020phonotactic}. When trained on running text, phoneme LMs have been used for text-to-speech \citep{li-2023-phoneme-level-bert} and lyric generation \citep{ding-2024-songcomposer}. When compared to grapheme-based models on standard linguistic benchmarks, phoneme models slightly under-perform \citep{nguyen-2022-word-boundaries, bunzeck2024graphemes} but this could be attributed to pre-processing, punctuation and the fact that LLM architectures and evaluation sets have been optimised for written text \citep{goriely2024babble}. Despite the benefits of phoneme-based training, phonological evaluation is limited, and few phoneme LMs exist beyond English. \citet{goriely2025} trained phoneme LMs on child-directed speech across 11 languages, but were only able to use an English benchmark for studying how phonological and syntactic knowledge scales in phoneme LMs. 

%When trained on developmentally plausible corpora (see the BabyLM challenge, \citep{hu2024findings}), these models have demonstrated comparable performance to their graphemic counterparts \citep{bunzeck2024graphemes, goriely2024babble}.
%However, the majority of this work only considers English. Recently, \citet{goriely2025} trained phoneme LMs on child-directed speech across 11 languages, but were only able to use an English benchmark for studying how phonological and syntactic knowledge scales in phoneme LMs. 

In this work, we propose the word segmentation task as a language-independent method for probing the representations learned by phoneme LMs. Below, we summarise past approaches for investigating the phonological capabilities of language models. We then give historical background on the word segmentation task. Finally, we discuss past examples of word segmentation being used as a probing task.

%noting that the observation that \emph{prediction-errors in sequential models indicate lexical boundaries} has spanned from early connectionism to modern-day NLP.  

%Below we first summarize past work using phonemes for language modeling. We then give historical background on the word segmentation task and past work using LLMs for word segmentation. Finally, we discuss whether word boundaries should be treated as ground truth labels for lexical units and ...

%Discuss uses of this model for languages without labeled word boundaries (maybe in discussion). 


%Probing phoneme LMs trained on developmentally plausible data for phonological knowledge could lead to new insights, especially cross-lingually. ,  such as on the 31 languages in \ipachildes 

%When compared to grapheme-based models on standard linguistic benchmarks, phoneme models slightly under-perform \citep{nguyen-2022-word-boundaries, bunzeck2024graphemes}% but this could be attributed to pre-processing differences, punctuation and the fact that LLM architectures and evaluation sets have been optimized for written text \citep{goriely2024babble}. 


% \subsection{Phoneme Language Modeling}

% \emph{Discuss the few studies looking at training with phonemes, how some have been for downstream tasks and others have been used to study particular linguistic theories. Then discuss mine and the other BabyLM entry about training with phonemes and end with a brief summary of the \ipachildes paper as the first attempt to train BabyLMs on phonemic data beyond English. Say that in this paper, we extend their work by exploring all 31 languages in the dataset and finding new ways to evaluate for phonological knowledge.}

% Compared to models trained on orthographic text or raw audio, few studies training with phonemes. Models based on BERT have shown that training with phonemes can be useful for downstream tasks. In comparison studies, grapheme models slightly outperform phoneme models \citep{nguyen-2022-word-boundaries, bunzeck2024graphemes} but this could be attributed to preprocessing differences, punctuation and the fact that typical LLM architectures have been optimized for written text \citep{goriely2024babble}. As with phonological benchmarks, few phoneme LMs exist for languages other than English. Here, we release phoneme LMs trained on the 31 languages in \ipachildes...

% - phoneme LMs for low resource work

\subsection{Phonological Evaluation of LLMs}

% \emph{Note relatively few benchmarks that exist for phonological knowledge. Note the ones that evaluate orthographic models, which makes no sense. Mention rhyme prediction and age prediction as recent work. Discuss BabySLM, and note historical studies based on head-turn paradigm for children. Conclude by noting that real-word identification and word segmentation capture a general phonological capability and can be applied to many languages, particularly word segmentation, as the Wuggy pipeline only supports so many languages.}

While many studies have explored the representations learned by phoneme LMs trained on individual words, there are very few benchmarks for phoneme LMs trained on running text.

One method for testing phonology is to use minimal pairs of words and pseudowords as a lexical decision task. One benchmark that uses this approach is BabySLM \citep{lavechin}, which provides a lexical decision metric for phoneme LMs or speech LMs (which learn directly from audio) using a vocabulary based on child-directed speech. \citet{bunzeck-etal-2025-small} use a similar approach in order to compare grapheme LMs to phoneme LMs. They also use two probing tasks to examine the representations of sentences; age prediction and rhyme prediction. %These studies only test English models, in part due to the lack of phoneme LMs in other languages, but also due to a lack of resources for constructing phonological tasks. For example, they both use \texttt{wuggy} \citep{keuleers2010wuggy} to generate pseudowords, which only supports three languages for phonetic pseudoword generation.

PhonologyBench \citep{suvarna-etal-2024-phonologybench} is a benchmark that uses prompts to test chat-based English LLMs. However, by using prompts, they treat phonology as an emergent ability tested through metalinguistic judgment, an evaluation strategy which \citet{hu2023prompting} argues is inferior to using quantities directly derived from a model's representations.  

These benchmarks also only test English models, in part due to the lack of phoneme LMs in other languages, but also due to a lack of resources for constructing phonological tasks. For example, pseudowords are typically generated using \texttt{wuggy} \citep{keuleers2010wuggy}, which only supports three languages for phonetic pseudoword generation. An example of language-independent evaluation of phoneme LMs is the phonetic feature probe used in \citet{goriely2025}, which only requires feature vectors for each IPA symbol. The word segmentation task requires no language-specific data, only utterances labeled with word boundaries. 

%seeming to correspond to a higher-level understanding of phonology based on semantics, rather than relating to representations learned from individual phonemes. 

%Very few methods for measuring phonological evaluation of LLMs. PhonologyBench is one example but is based on prompting for very large language models trained on orthographic text, limiting analysis of representations learned for individual phonemes. BabySLM provides a lexical decision task, more closely relating language models to psycholingustic studies of human preference. \citet{bunzeck-etal-2025-small} use a similar test, also exploring rhyme prediction and age prediction as phonological tasks. These studies all only explore English phonology due in part to the lack of phonemic data in other language \citep{goriely2023word} as well as a lack of resources for constructing phonological tasks. For example, BabySLM uses Wuggy to generate pseudowords but Wuggy only supports three languages for phonetic pseudoword generation.

\subsection{Computational Models of Segmentation}\label{sec:15-wordseg}

% \emph{Talk about word segmentation literature and the computational approaches to studying it over the last 25 years, with a focus on statistical learning experiments based on `uncertainty' / surprisal. Mention how most studies only do English. Discuss \citet{hahn-baroni-2019-tabula} who used word segmentation to explore the `tabula rasa' nature of LLMs, but critique their work for only examining orthographic models. Mention word segmentation of low-resource languages as a use-case of our work.}

Unlike in written text, where lexical units are separated by spaces and punctuation, spoken communication consists of continuous utterances with no clear demarcation of words (see e.g. \citet{cole1980model}). Somehow, without a lexicon to consult, children are able to segment speech into words and phrasal units by the age of six months \citep{Jusczyk1999infants}. How children learn to segment words and bootstrap their lexicon is known in psycholinguistics as the \emph{word segmentation problem}, and statistical learning experiments have established a wide variety of statistical cues which children may use to segment speech \citep{Cutler1987, gleitman1988learning, Jusczyk1993stress, Saffran1996distributional, Jusczyk1999allophonic, Suomi1997}.

Particularly influential were the experiments of \citet{Saffran1996learning}, who established that 8-month-old children use distributional information to segment speech, specifically noting that low conditional probability %\footnote{In their study, the calculation is called ``transitional probability''.}
between two adjacent syllables often indicated a word boundary. These experiments inspired the development of computational models proposing cognitively plausible learning mechanisms for word segmentation, most of which are based on the principle that units within words are far more predictable than units across word boundaries \citep{harris1955}. Many models draw on \citet{Brent1999}, who used unigram statistics to segment speech, with later models using higher-order n-grams \citep{Venkataraman2001}, incorporating phonological constraints \citep{Blanchard2010} or leveraging prior distributions over word frequencies and phonological shapes \citep{Goldwater2009}. Other models explicitly calculate several statistical cues at each potential word boundary and combine cues using a majority voting framework \citep{ccoltekin2014explicit, Coltekin2017, goriely2023word}. Each cue provides a signal over the utterance (as illustrated in \cref{fig:15-example}) with peaks in each cue indicating a potential boundary. 

Peaks in predictability can also be observed in neural language models. In the foundational work of \citet{elman-1990-finding}, a simple recurrent network (SRN) is trained to predict letters in an unsegmented sequence (one of the first examples of autoregressive language modeling). Elman observes that the prediction-error increases at the onset of each new word, concluding that ``there is information in the signal that could serve as a cue to the boundaries of linguistic units which must be learned''.

\citet{christiansen1998learning} later used an SRN to segment speech by using the probability of an \emph{utterance} boundary, rather than prediction-error, to place word boundaries. This followed previous work suggesting that children could use utterance boundaries to bootstrap their lexicon \citep{aslin1996models} and is a cue used in the models of \citet{ccoltekin2014explicit, goriely2023word}.

In this study, we combine ideas from past computational models for word segmentation. Rather than explicitly calculate n-gram statistics, our cues are based on prediction-error and utterance boundary probability extracted from LLMs trained on the next-phoneme prediction task. As these cues are based on the language model's prediction of phonemes, successful segmentation indicates that implicit phonological knowledge of word-like units in these models. 

% Some models leverage n-gram statistics \citep{Brent1999, Venkataraman2001}, some  Blanchard2010, Elsner2017, Coltekin2017}. Many of these models were evaluated on 10,000 utterances from the Bernstein-Ratner \citeyear{bernstein1987phonology} corpus in CHILDES, transcribed phonemically by \citet{Brent1999}, who also established the evaluation metrics for word segmentation performance (see \cref{sec:15-evaluation}).
%Brent also established the typical metrics used to evaluate word segmentation performance (see \cref{sec:15-evaluation}) and phonemically transcribed 10,000 utterances from the Bernstein-Ratner \citeyear{bernstein1987phonology} corpus in CHILDES which continues to be used to evaluate segmentation models. 

%In this study, we draw inspiration from Elman's observations, using cues extracted from a modern LLM trained to predict phoneme sequences, and finding that these cues indicate implicit knowledge of word boundaries. 

\subsection{Probing for Word Boundaries}

% Soon after word2vec, \citet{ma2016learning} presented a model that learned phoneme embeddings using the  a word segmentation task

Previous work has explored the representations of word boundaries in LLMs. \citet{sanabria2021difficulty} explored methods for extracting word boundaries from attention weights in an LSTM, finding that attention had limited value for segmentation. \citet{hahn-baroni-2019-tabula} trained character-level RNNs and LSTMs without word boundaries, finding that individual activations correlated with word boundaries and that a linear probe trained on all activations also identified boundaries. They claimed that removing word boundaries resulted in a `near tabula rasa' training paradigm but trained on billions of graphemic words Wikipedia, which is not developmentally plausible. Here, we use this probe on the final layer of phoneme LMs trained on developmentally plausible data, a more `tabula rasa' paradigm. 

Other studies have verified \citeauthor{elman-1990-finding}'s observations that prediction-error corresponds with word boundaries. For instance, \citet{al-rfou_character-level_2019} train a 64-layer character-level transformer and in qualitative analysis note that three measures of prediction-error sharply increase at the start of words. However, their model is trained on graphemic text from Wikipedia without removing the word boundaries and they do not explicitly use these measures to evaluate word segmentation performance. Here, we use their three measures to propose an unsupervised word segmentation algorithm using phoneme LMs trained without word boundaries.

% Maybe extend this with more general work looking at error in LMs
% Maybe worth talking about Speech LM word segmentation challenge from zerospeech.

% SOMEWHERE NOTE DIFFERENCE TO CHINESE WORD SEGMENTATION?

\section{Pre-training on Developmentally Plausible Data}\label{sec:12-babylm}

\subsection{Phoneme LMs and Child-Centered Data}\label{sec:13-babylm}

% Modelling of child-directed speech (focus on BabyLM challenge, note that smaller than typical work but has led to new architectures developed from low-data scenario, but difficulty in analysis phonological information due to orthographic subword representation. Conclude noting that a few have addressed this by converting the BabyLM dataset to phonemes but is only English and lacks reproducability due to lack of datasets. In this study we extend that work by extending phoneme-based modelling of BabyLMs to 11 languages and probing them for phonological feature information, demonstrating what can be done with the datasets which would not have been possible before.

In this work, we illustrate one use of our dataset by training small monolingual LMs on 11 languages and examining the representations they learn for individual phonemes. 

Training models on such little data (here, only 500 thousand words) may be considered atypical in the modern NLP landscape, but questions of developmental plausibility have led to an increased interest in pretraining with limited data. For instance, the BabyLM workshop series challenges participants to train smaller models on data that is limited by both scale, 10--100 million words, and by domain, with the pre-training corpus including data from CHILDES, among other child-centered corpora \citep{warstadt-2023-babylm-findings, hu-etal-2024-findings}. Such limitations have led to the development of new architectures \citep{georges-gabriel-charpentier-samuel-2023-layers, charpentier2024gpt}, motivated cognitively-inspired pre-training strategies \citep{huebner-etal-2021-babyberta, martinez-etal-2023-climb} and allowed for gaining insights into human learning \citep{yedetore-etal-2023-poor}. The majority of this work has centered on English. Exceptions include \citet{capone2024babies, shen2024bambino}, who train Italian monolingual and bilingual models, respectively, \citet{yadavalli2023slabert} who use data from five language in CHILDES to explore second language acquisition theories (but only train an English LM) and \citet{salhan-etal-2024-less}, who use age-ordered data from four languages in CHILDES to explore fine-grained curricula inspired by language acquisition.

%train BabyLMs on several languages CHILDES to explore curricula inspired by first language acquisition theories. 

However, these BabyLMs are typically trained on orthographic text, limiting their ability to be studied at the phonological level, and generally use subword tokens, which do not generally correspond to cognitively plausible units \citep{beinborn-pinter-2023-analyzing} limiting their value for psycholinguistic research \citep{giulianelli-etal-2024-proper}. \citet{bunzeck2024graphemes} and \citet{goriely2024babble} both establish phoneme-based training of BabyLMs (where tokens consist of individual phonemes, with word boundaries removed) but only train on English text. Here, we use \ipachildes to demonstrate phoneme-based training for 11 languages and leverage the fact that \gpp maintains a correspondence to \phoible in order to probe our BabyLMs for knowledge of distinctive features. 

%Phoneme LMs have recently demonstrated comparable performance to LMs trained on orthographic text \citep{nguyen-2022-word-boundaries,bunzeck2024graphemes,goriely2024babble} but so far this work has only trained on English data. 
