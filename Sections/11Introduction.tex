\chapter{Introduction}\label{chapter:intro}

% Language models are ubiquitous tools in the field of Natural Language Processing (NLP). Originally consisting of \ngram counts and now implemented using deep neural networks with billions of parameters, they have demonstrated a striking ability to process natural language, leading to their use in downstream applications but also to gain new insights into the statistical properties and structure of language. 

Language models have long been foundational tools in Natural Language Processing (NLP). By providing probability distributions over sequences of discrete linguistic units, they have supported a wide range of downstream applications and enabled the quantitative study of the statistical structure of language.

From early \ngram models to today's large neural architectures, language models have undergone significant evolution. Modern models, particularly \defn{large language models} (LLMs), contain millions or even billions of parameters and are trained on trillions of words of text. These models now exhibit remarkable language processing abilities and have become ubiquitous --- both as objects of linguistic study and as integral components of commercial NLP systems. This widespread adoption has, in turn, driven substantial research into understanding how LLMs learn, often drawing on linguistic theory to investigate whether their training behaviours resembles human language acquisition \citep{evanson-2023-language}, whether their predictions align with human language processing, and whether the internal representations they develop capture generalisable grammatical patterns.

Alongside efforts to analyse how LLMs learn, there has also been growing interest in improving language models by incorporating insights from linguistics. Initiatives such as the Zero Resource Speech Challenge \citep{dunbar2022self} and the BabyLM Challenge \citep{warstadt2023findings} have motivated new architectures and learning strategies designed to function in low-resource settings --- reflecting how humans acquire language from far less linguistic input than current LLMs receive. Within this framework, linguistic principles have shaped new approaches to architecture design, curriculum learning, and training methodologies.

Despite these advances, one critical factor in language modelling remains relatively under-explored: the \defn{input representation} --- the way raw language data is transformed into the discrete tokens processed by the model. While contemporary LLMs overwhelmingly rely on \emph{subword}-based representations derived from orthographic text, earlier \ngram models often used tokens corresponding to individual \emph{phonemes}, extracted from speech transcripts. Although phoneme-based representations continue to appear in certain speech processing systems and phonological studies, they remain largely neglected in the context of modern, large-scale language modelling.

This thesis investigates the potential of phoneme-based input representations in modern language model architectures, with a particular emphasis on developmentally plausible training scenarios. I refer to language models trained in this way as \defn{phoneme LMs} in this thesis. Confronting a lack of suitable tools and datasets for training phoneme LMs, I first develop and release two open-source resources, also designed to support broader phonological analysis. I then conduct a series of experiments leveraging these resources to evaluate the feasibility of phoneme-based input for training neural language models. By comparing phoneme-based inputs to subword-based inputs, these experiments reveal the subtle impact of input representation on how these neural language models acquire phonological and syntactic capabilities, and further experiments reveal how these capabilities scale with data and model size.

Building on these results, I demonstrate how phoneme-based input enables cross-linguistic phonological analysis by training phoneme LMs on child-directed speech from 31 languages and probing the learned representations for phonological knowledge. Distinctive phonological feature probes reveal that certain features are encoded by these models, implying they have a role in distributional phonology cross-lingually. Using a word segmentation task inspired by the language acquisition literature, I show that these models implicitly track word boundaries --- even though no explicit boundary information was provided during training.

Finally, drawing on parallels between the word segmentation problem and common subword tokenisation techniques, I propose a novel, unsupervised method for learning subword vocabularies based on grouping predictable bytes. This final contribution illustrates how linguistic insights can inspire new techniques for application-driven language modelling, and the broader value of studying the nature of input representations in language models.

\section{Research questions and contributions}

The research conducted in this thesis explores insights that can be gained by training language models with a phoneme-based input representation. This broad exploration is split into three research questions, the first of which concerns the need for dedicated resources to support phoneme-based training:

\begin{question}{}{rq1}
    To what extent do sufficient resources exist for training and evaluating language models using a phoneme-based input representation?
\end{question}

This question is addressed through the following contributions:

\begin{itemize}
    \item I conduct a thorough analysis of existing datasets and grapheme-to-phoneme tools and find crucial gaps in existing resources (\cref{chapter:resources}).
    \item I develop and release two resources that fill these gaps; the \ipachildes dataset and the \gpp tool for grapheme-to-phoneme conversion (\cref{chapter:resources}).
    \item I demonstrate that the BabyLM challenge framework can be used to train and evaluate phoneme-based language models using standard language modelling benchmarks (\cref{chapter:modelling}).
    \item I demonstrate that distinctive feature probes and the word segmentation task can be used to test phoneme-based language models for phonological knowledge (\cref{chapter:phonology}).
\end{itemize}

With these resources established, the second research questions concerns the linguistic insights that can be gained from phoneme-based training:

\begin{question}{}{rq2}
    To what extent can phoneme-based training facilitate cross-lingual phonological experimentation and analysis?
\end{question}

This question is addressed through the following contributions:

\begin{itemize}
    \item I probe models trained on 11 languages from \ipachildes for phonological distinctive features, possible due to the coherence that \gpp maintains with a pre-existing phonological database (\cref{chapter:phonology}).
    \item I investigate whether phoneme LMs implicitly track word boundaries by using the word segmentation task, proposing a novel unsupervised method based on models in the acquisition literature which I use to use alongside linear probes to evaluate models across 31 languages (\cref{chapter:phonology}).
\end{itemize}

Finally, the final research question explores the technical insights that can be gained from phoneme-based training:

\begin{question}{}{rq3}
    To what extent can phoneme-based training provide insights into language model training?
\end{question}

This question is addressed through the following contributions:

\begin{itemize}
    \item I compare models trained with a phoneme-based input representation to models trained with a standard subword-based input representation to explore the technical effects of three identified transformations that separate the two (\cref{chapter:modelling}).
    \item Using the North American English section of \ipachildes, I compare the effect of model size and data size on syntactic and phonological capabilities of phoneme models (\cref{chapter:modelling}).
    \item Using word boundary probes, I identify that phoneme models implicitly track word boundaries (\cref{chapter:phonology}).
    \item Noting the parallels between information-driven word segmentation strategies and subword tokenisation, I propose and evaluate a novel tokenisation strategy that groups bytes using prediction error, rather than frequency (\cref{chapter:infotokenisation}).
\end{itemize}

\section{Thesis outline}

A summary of the subsequent chapters of this thesis is given below.

\begin{itemize}
    \item In \cref{chapter:background}, I provide the necessary background for the research presented in this thesis. I discuss how input representations have evolved alongside language model architectures, defining the phoneme-based input representation in contrast to the subword-based input representations. I then focus on subword-based input representations, discussing typical tokenisation methods used to create them, their limitations, and reviewing alternative representations that have been explored. I then focus on phoneme-based input representations, reviewing prior work that has utilised them in \ngram models as well as modern architectures. Finally, I discuss research that uses a developmentally plausible training framework to study smaller neural language models. This research has led to numerous advancements and insights but has predominantly focused on English and mostly used the subword-based input representation, despite the relative implausibility of orthographic text split into subwords. I conclude that this framework is useful for studying the impact of phoneme-based input representations and describe the datasets and evaluation benchmarks utilised in this thesis.
    \item In \cref{chapter:resources}, I conduct a thorough review of existing phonemic datasets and the methods used to create them. I identify a particular lack of data for cross-lingual studies of spontaneous or child-centred speech. I find that the majority of phonemic datasets are created using grapheme-to-phoneme conversion tools but that these tools lack coherence with established phonological inventories and databases. I address both of these issues by developing two resources which are released open-source; \gpp, an improved grapheme-to-phoneme conversion tool which ensures that phoneme inventories correspond to inventories from an established phonological database, and \ipachildes, a phonemic dataset of child-centred, spontaneous speech across 31 languages, created using \gpp.
    \item In \cref{chapter:modelling}, I explore the impact of phoneme-based training with modern neural language architectures. I begin by defining the architecture used for the experimental work in this thesis. I then present eight tokenisers that cover all combinations of the three input transformations and compare models trained with these tokenisers using the BabyLM challenge dataset and evaluation pipeline, finding that phoneme LMs only lead to a minor decrease in performance on these traditional benchmarks but are beneficial for phonological benchmarks. I then establish the scaling laws of phoneme LMs by training models of various sizes on subsets of North American English portion of \ipachildes and evaluating their syntactic and phonological capabilities.
    \item In \cref{chapter:phonology}, using the scaling laws established in the previous chapter, I train and release a suite of phoneme LMs across the 31 languages of \ipachildes to support phonological experimentation. I demonstrate how the correspondence that \gpp maintains with a phonological database allows for these models to be probed for implicit knowledge of distinctive features and find that the distributional properties of phonemes are sufficient to learn major class and place features cross-lingually. I also use the word segmentation task from the acquisition literature, proposing unsupervised methods for extracting word boundaries from phoneme LMs as well as using linear probes, finding that these models implicitly track word boundaries and noting priors inherit in the distributional properties of phonemes in different languages.
    \item In \cref{chapter:infotokenisation}, I draw parallels between the unsupervised word segmentation methods introduced in the previous chapter and recent tokenisation methods utilised in LLMs. Using this insight, I introduce \bytespan, a novel method for learning subwords using the predictions of a byte-level language model. I find that this cognitively-motivated method learns subwords that align more closely with morphemes without sacrificing the benefits of popular subword tokenisation methods for compression. In a multilingual setup, I find that the method has the potential to be biased towards dominant scripts, but explore a method for balancing the vocabulary across languages. 
    \item In \cref{chapter:conclusion}, I summarise the contributions of this thesis and discuss the wider implications of phoneme-based input representations in contrast to recent advances in audio-based training. Finally, I conclude with ideas for future directions.
\end{itemize}

\section{Publications}

The research presented in this thesis has been the basis for the following publications.

\begin{itemize}
\item The work presented in \cref{chapter:resources} was published as:
\begin{mdframed}[linewidth=1pt]
Z\'ebulon Goriely and Paula Buttery. 2025. \href{https://arxiv.org/abs/2504.03036}{\myemph{IPA-CHILDES \& G2P+: Feature-Rich Resources for Cross-Lingual Phonology and Phonemic Language modelling}}. Accepted to CoNLL 2025.
\end{mdframed}

\item The work presented in \cref{chapter:modelling} won the \enquote{Outstanding Paper} award at the second edition of the BabyLM challenge and was published as:
\begin{mdframed}[linewidth=1pt]
Z\'ebulon Goriely, Richard Diehl Martinez, Andrew Caines, Paula Buttery, and Lisa Beinborn. 2024. \href{https://aclanthology.org/2024.conll-babylm.4/#}{\myemph{From Babble to Words: Pre-Training Language Models on Continuous Streams of Phonemes}}. In The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning, pages 37--53, Miami, FL, USA. Association for Computational Linguistics.
\end{mdframed}

\item The work presented in \cref{chapter:phonology} was published as:
\begin{mdframed}[linewidth=1pt]
Z\'ebulon Goriely and Paula Buttery. 2025. \href{https://arxiv.org/abs/2504.03338}{\myemph{BabyLM's First Words: Word Segmentation as a Phonological Probing Task}}. Accepted to CoNLL 2025.
\end{mdframed}

\item The work presented in \cref{chapter:infotokenisation} was published as:
\begin{mdframed}[linewidth=1pt]
Z\'ebulon Goriely, Suchir Salhan, Pietro Lesci, Julius Cheng, and Paula Buttery. 2025. \href{https://arxiv.org/abs/2506.18639}{\myemph{ByteSpan: Information-Driven Subword Tokenisation}}. Accepted to TokShop 2025.
\end{mdframed}
\end{itemize}

Note that some experiments have been moved from their original papers to other chapters in this thesis to provide a more coherent narrative. Specifically, these are the scaling experiments using the \ipachildes dataset in \cref{chapter:modelling} and the distinctive feature probing experiments in \cref{chapter:phonology}, both of which were originally published in the first paper above.

In addition to the publications above, several related publications were produced during the course of this research. These are as follows:
%These are listed below due to being highly related to this research but are not included in this thesis.

\begin{itemize}
\item The winner of the \enquote{Most Interesting Paper} award at the first edition of the BabyLM challenge, exploring the use of curriculum learning to train language models in a developmentally plausible setting:
\begin{mdframed}[linewidth=1pt]
    Richard Diehl Martinez, Z\'ebulon Goriely, Hope McGovern, Christopher Davis, Andrew Caines, Paula Buttery, and Lisa Beinborn. 2023. \href{https://aclanthology.org/2023.conll-babylm.10/}{\myemph{CLIMB --- Curriculum Learning for Infant-inspired Model Building}}. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning, pages 112--127, Singapore. Association for Computational Linguistics.
\end{mdframed}

\item Introducing a measure of \defn{Frequency Bias} to explore how the grammatical capabilities of language models are affected by token frequency and introducing \defn{Syntactic Smoothing}, using a prior based on part-of-speech classes during training, to mitigate this bias:
\begin{mdframed}[linewidth=1pt]
    Z\'ebulon Goriely, Richard Diehl Martinez, Andrew Caines, Paula Buttery, and Lisa Beinborn. 2024. \href{https://aclanthology.org/2024.emnlp-main.344/}{\myemph{Mitigating Frequency Bias and Anisotropy in Language Model Pre-Training with Syntactic Smoothing}}. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 5999–6011, Miami, Florida, USA. Association for Computational Linguistics.
\end{mdframed}

\item Exploring the use of fine-grained curriculum learning strategies based on linguistic acquisition theories across several languages:
\begin{mdframed}[linewidth=1pt]
    Suchir Salhan, Richard Diehl Martinez, Z\'ebulon Goriely, and Paula Buttery. 2024. \href{https://aclanthology.org/2024.conll-babylm.15/}{\myemph{Less is More: Pre-Training Cross-Lingual Small-Scale Language Models with Cognitively-Plausible Curriculum Learning Strategies}}. In The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning, pages 174--188, Miami, FL, USA. Association for Computational Linguistics.
\end{mdframed}

\item My master's thesis exploring a cue-based algorithm for word segmentation, which was augmented with additional experiments:
\begin{mdframed}[linewidth=1pt]
    Z\'ebulon Goriely, Andrew Caines and Paula Buttery. 2023. \href{https://www.cambridge.org/core/journals/journal-of-child-language/article/word-segmentation-from-transcriptions-of-childdirected-speech-using-lexical-and-sublexical-cues/5D206536C878ADA80026ED8E2E72F848}{\myemph{Word segmentation from transcriptions of child-directed speech using lexical and sub-lexical cues}}. Journal of Child Language. 2025;52(1):1-41.
\end{mdframed}
\end{itemize}

\section{Open-source contributions and research artefacts}

The two resources presented in \cref{chapter:resources} have been released as open-source:

\begin{itemize}
\item \gpp is a pip-installable tool using an MIT license. The source is available on \href{https://github.com/codebyzeb/g2p-plus}{\myemph{GitHub}}. 
\item The \ipachildes dataset is hosted on \href{https://huggingface.co/datasets/phonemetransformers/IPA-CHILDES}{\myemph{Huggingface}}.
\end{itemize}

Additionally, the following research artefacts have been released for reproducibility and dissemination:

\begin{itemize}
    \item \href{https://github.com/codebyzeb/childes-processor}{\myemph{github.com/codebyzeb/childes-processor}}: The source code for \gpp, a pip-installable tool used to download and process the CHILDES database for the creation of the \ipachildes in \cref{chapter:resources}. It is open-sourced with an MIT license.
    \item \href{https://github.com/codebyzeb/babylm-ipa}{\myemph{github.com/codebyzeb/babylm-ipa}}: The scripts used for leveraging \gpp to convert the BabyLM training data and evaluation data to an IPA representation in \cref{chapter:modelling}.
    \item \href{https://github.com/codebyzeb/PhonemeTransformers}{\myemph{github.com/codebyzeb/PhonemeTransformers}}: The source code and scripts used to train language models using phonemes, compare input representations and evaluate models using word segmentation and distinctive phonological features probes in \cref{chapter:modelling,chapter:phonology}. 
    \item \href{https://huggingface.co/phonemetransformers}{\myemph{huggingface.co/phonemetransformers}}: A Huggingface organisation containing all datasets, models and tokenisers trained for the experiments in \cref{chapter:modelling,chapter:phonology}, grouped into collections for convenience.
    \item \href{https://github.com/codebyzeb/infotokenization}{\myemph{github.com/codebyzeb/infotokenization}}: The scripts used to train byte-level language models, extract their predictions to create subword tokenisers, train subword-level models using these tokenisers and evaluate them for the experiments in \cref{chapter:infotokenisation}.
    \item \href{https://huggingface.co/Infotokenizers}{\myemph{huggingface.co/Infotokenizers}}: A Huggingface organisation containing all datasets, models and tokenisers trained for the experiments in \cref{chapter:infotokenisation}.
\end{itemize}
